@book{1390c,
  title = {Earthquakes: {{Risk}} Factors, Seismic Effects and Economic Consequences},
  author = {Quinn, Albert P.},
  year = {2014},
  journal = {Earthquakes: Risk Factors, Seismic Effects and Economic Consequences},
  publisher = {{Nova}},
  abstract = {An earthquake is a natural disaster that causes damage worldwide. Not only earthquakes of high magnitude, but also those of small magnitude that strike unprepared regions can cause economic and social consequences, and many casualties. Unlike other natural disasters, the exact time of an earthquake cannot be estimated; scientists can only predict the timeline and magnitude based on the history of earthquakes in a region. Even though current technology cannot predict the precise time, location or magnitude, public awareness about the estimations allows both individuals and government to be ready for their devastating effects. This book begins by discussing how public awareness about the effects of earthquakes and how to prepare for a possible earthquake which can potentially save lives. The book then continues with topics that include seismic PRA; seismic safety assessments of existing buildings; psychiatric reactions of individuals to earthquakes; possible relation between an intense earthquake and the voltage signal generated by atmospheric ionic currents and/or suddent change of the electric field in the air; and others.},
  isbn = {978-1-63117-518-3},
  file = {D\:\\Zotero\\storage\\2DVSD2WM\\شریفی - 1390 - No Titleبررسی تاثیر(2).pdf}
}

@book{2544a,
  title = {Nonlinear {{Time Series Analysis}}},
  author = {Kantz, Holger and Shreiber, Thomas},
  year = {2003},
  publisher = {{Cambridge}},
  isbn = {978-0-521-82150-6},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\IVZT5VWC\\pdf}
}

@book{2544d,
  title = {Inverse {{Eigenvalue Problems}}},
  author = {Chu, Moody and Golub, Gene},
  publisher = {{Oxford Science Publications}},
  isbn = {978-0-19-856664-9},
  file = {D\:\\Zotero\\storage\\LQVR6A9Y\\Chu, Golub - Unknown - Inverse Eigenvalue Problems.pdf}
}

@book{2544e,
  title = {Parameter {{Estimation}} and {{Inverse Problems}}},
  author = {Aster, Richard and Borchers, Brian and Thurber, Clifford},
  isbn = {0-12-065604-3},
  file = {D\:\\Zotero\\storage\\RC6774YW\\Aster, Borchers, Thurber - Unknown - Parameter Estimation and Inverse Problems.pdf}
}

@techreport{A.S.Cakmak,
  title = {Parametric {{Time Series Models}} for {{Earthquake Strong Ground Motions}} and {{Their Relationship}} to {{Site Parameters}}},
  author = {Cakmak, A. S. and Sherif, R. I.},
  file = {D\:\\Zotero\\storage\\VTFLCZ3T\\Cakmak, Sherif - Unknown - Parametric Time Series Models for Earthquake Strong Ground Motions and Their Relationship to Site Parameters.pdf}
}

@book{Aggarwal2017a,
  title = {Outlier {{Analysis}}},
  author = {Aggarwal, Charu C. and Aggarwal, Charu C.},
  year = {2017},
  doi = {10.1007/978-3-319-47578-3_3},
  abstract = {The attributes in real data are usually highly correlated. Such dependencies provide the ability to predict attributes from one another. The notions of prediction and anomaly detection are intimately related. Outliers are, after all, values that deviate from expected (or predicted) values on the basis of a particular model. Linear models focus on the use of interattribute dependencies to achieve this goal. In the classical statistics literature, this process is referred to as regression modeling.},
  isbn = {978-3-319-47577-6},
  file = {D\:\\Zotero\\storage\\SEHMY8B2\\Aggarwal, Aggarwal - 2017 - Linear Models for Outlier Detection.pdf}
}

@article{Al,
  title = {Appendix 1 {{Vector}} Algebra},
  author = {Al, Figure},
  file = {D\:\\Zotero\\storage\\GC2Q9W52\\Al - Unknown - Appendix 1 Vector algebra.pdf}
}

@book{Alder1997,
  title = {Introduction to {{Complex Analysis}} for {{Engineers}}},
  author = {Alder, Michael D},
  year = {1997},
  file = {D\:\\Zotero\\storage\\C8U7QYW2\\Alder - 1997 - Introduction to Complex Analysis Engineers.pdf}
}

@article{Alessandro2019a,
  title = {Seismic Microzonation by Means of Cluster Analysis},
  author = {Alessandro, A D and Scudero, S and Capizzi, P},
  year = {2019},
  pages = {1911--1918},
  isbn = {9780367143282},
  file = {D\:\\Zotero\\storage\\GQI6HRTM\\Alessandro, Scudero, Capizzi - 2019 - Seismic microzonation by means of cluster analysis.pdf}
}

@book{Allison2002,
  title = {Missing Data},
  author = {Allison, Paul D.},
  year = {2002},
  volume = {11},
  pmid = {12068851},
  file = {D\:\\Zotero\\storage\\XEVQFB7T\\Allison - 2002 - Missing data.pdf}
}

@inproceedings{Alonso,
  title = {Time {{Series Clustering}}},
  booktitle = {{{ASDM}} - {{C02}}},
  author = {Alonso, Andr{\'e}s M.},
  year = {2019},
  file = {D\:\\Zotero\\storage\\LH7HXIY6\\Alonso - 2019 - Time Series Clustering.pdf}
}

@book{Analysisc,
  title = {Spectral Analysis Parametric and Non-Parametric Digital Methods},
  author = {Castani{\'e}, Francis},
  isbn = {978-1-905209-05-7},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\DSVA6C96\\Castanié - Unknown - Spectral analysis parametric and non-parametric digital methods.pdf}
}

@book{Andr1387,
  title = {Data {{Analysis}} Using Regression and Multilevel-Hierarchical Models},
  author = {Gelman, Andrew and Hill, Jennifer},
  publisher = {{Cambridge University Press}},
  isbn = {0-521-86706-1},
  file = {D\:\\Zotero\\storage\\PFKR24QX\\Gelman, Hill - Unknown - Data Analysis using regression and multilevel-hierarchical models.pdf}
}

@article{Apostol1969,
  title = {{{CALCULUS}} - {{Volume II}}, {{Multi Variable Calculus}} and {{Linear Algebra}}, with {{Applications}} to {{Differential Equations}} and {{Probability}}},
  author = {Apostol, Tom M.},
  year = {1969},
  volume = {II},
  pages = {696},
  abstract = {This book is a continuation of the author's Calculus, Volume I, Second Edition. The present volume has been written with the same underlying philosophy that prevailed in the first. Sound training in technique is combined with a strong theoretical development. Every effort has been made to convey the spirit of modern mathematics without undue emphasis on formalization. As in Volume I, historical remarks are included to give the student a sense of participation in the evolution of ideas. The second volume is divided into three parts, entitled Linear Analysis, Nonlinear Ana!ysis, and Special Topics. The last two chapters of Volume I have been repeated as the first two chapters of Volume II so that all the material on linear algebra will be complete in one volume. Part 1 contains an introduction to linear algebra, including linear transformations, matrices, determinants, eigenvalues, and quadratic forms. Applications are given to analysis, in particular to the study of linear differential equations. Systems of differential equations are treated with the help of matrix calculus. Existence and uniqueness theorems are proved by Picard's method of successive approximations, which is also cast in the language of contraction operators. Part 2 discusses the calculus of functions of several variables. Differential calculus is unified and simplified with the aid of linear algebra. It includes chain rules for scalar and vector fields, and applications to partial differential equations and extremum problems. Integral calculus includes line integrals, multiple integrals, and surface integrals, with applications to vector analysis. Here the treatment is along more or less classical lines and does not include a formal development of differential forms. The special topics treated in Part 3 are Probability and Numerical Analysis. The material on probability is divided into two chapters, one dealing with finite or countably infinite sample spaces; the other with uncountable sample spaces, random variables, and dis- tribution functions. The use of the calculus is illustrated in the study of both one- and two-dimensional random variables. The last chapter contains an introduction to numerical analysis, the chief emphasis being on different kinds of polynomial approximation. Here again the ideas are unified by the notation and terminology of linear algebra. The book concludes with a treatment of approximate integration formulas, such as Simpson's rule, and a discussion of Euler's summation formula. There is ample material in this volume for a full year's course meeting three or four times per week. It presupposes a knowledge of one-variable calculus as covered in most first-year calculus courses. The author has taught this material in a course with two lectures and two recitation periods per week, allowing about ten weeks for each part and omitting the starred sections. This second volume has been planned so that many chapters can be omitted for a variety of shorter courses. For example, the last chapter of each part can be skipped without disrupting the continuity of the presentation. Part 1 by itself provides material for a com- bined course in linear algebra and ordinary differential equations. The individual instructor can choose topics to suit his needs and preferences by consulting the diagram on the next page which shows the logical interdependence of the chapters. Once again I acknowledge with pleasure the assistance of many friends and colleagues. In preparing the second edition I received valuable help from Professors Herbert S. Zuckerman of the University of Washington, and Basil Gordon of the University of California, Los Angeles, each of whom suggested a number of improvements. Thanks are also due to the staff of Blaisdell Publishing Company for their assistance and cooperation. As before, it gives me special pleasure to express my gratitude to my wife for the many ways in which she has contributed. In grateful acknowledgement I happily dedicate this book to her.},
  isbn = {0471000078},
  file = {D\:\\Zotero\\storage\\SDV5Q36J\\Apostol - 1969 - CALCULUS - Volume II, Multi Variable Calculus and Linear Algebra, with Applications to Differential Equations and Proba.pdf}
}

@book{Atkinson1981a,
  title = {Identification of {{Outliers}}.},
  author = {Atkinson, A. C. and Hawkins, D. M.},
  year = {1981},
  journal = {Biometrics},
  volume = {37},
  issn = {0006341X},
  doi = {10.2307/2530182},
  abstract = {Hawkins, D. M. (1980). Identification of outliers (Vol. 11). London: Chapman and Hall.},
  isbn = {978-94-015-3994-4},
  file = {D\:\\Zotero\\storage\\C4S3QE6U\\Atkinson, Hawkins - 1981 - Identification of Outliers.pdf}
}

@book{Au2017a,
  title = {Operational {{Modal Analysis}}},
  author = {Au, Siu-Kui},
  year = {2017},
  journal = {Operational Modal Analysis},
  doi = {10.1007/978-981-10-4118-1},
  abstract = {O artigo d iscute a no\c{c}\~ao sociol\'ogica de espa\c{c}o, na concep\c{c}\~ao de Jean R\'emy , identificando formas de apropria\c{c}\~ao e transa\c{c}\~ao social em cidade intermedi\'aria, no caso Montes Claros/Brasil. Problematiza as condi\c{c}\~oes de moradia, tendo a pol\'itica habitacional no Brasil como refer\^encia de investiga\c{c}\~ao. Examina o fen\^omeno da mobilidade social a partir do conceito de ``transa\c{c}\~ao social''.},
  isbn = {978-981-10-4117-4},
  file = {D\:\\Zotero\\storage\\HS5BSR39\\Au - 2017 - Operational Modal Analysis.pdf}
}

@book{Augmentation,
  title = {Bayesian {{Missing Data Problems EM}} , {{Data Augmentation}} And},
  author = {Augmentation, Data and Computation, Noniterative},
  isbn = {978-1-4200-7749-0},
  file = {D\:\\Zotero\\storage\\YJGTJ98V\\Augmentation, Computation - Unknown - Bayesian Missing Data Problems EM , Data Augmentation and.pdf}
}

@book{Ayyub2006a,
  title = {Uncertainty Modeling and Analysis in Engineering and the Sciences},
  author = {Ayyub, Bilal M. and Klir, George J.},
  year = {2006},
  journal = {Uncertainty Modeling and Analysis in Engineering and the Sciences},
  doi = {10.1201/9781420011456},
  abstract = {Engineers and scientists often need to solve complex problems with incomplete information resources, necessitating a proper treatment of uncertainty and a reliance on expert opinions. Uncertainty Modeling and Analysis in Engineering and the Sciences prepares current and future analysts and practitioners to understand the fundamentals of knowledge and ignorance, how to model and analyze uncertainty, and how to select appropriate analytical tools for particular problems. This volume covers primary components of ignorance and their impact on practice and decision making. It provides an overview of the current state of uncertainty modeling and analysis, and reviews emerging theories while emphasizing practical applications in science and engineering. The book introduces fundamental concepts of classical, fuzzy, and rough sets, probability, Bayesian methods, interval analysis, fuzzy arithmetic, interval probabilities, evidence theory, open-world models, sequences, and possibility theory. The authors present these methods to meet the needs of practitioners in many fields, emphasizing the practical use, limitations, advantages, and disadvantages of the methods.},
  isbn = {978-1-4200-1145-6},
  file = {D\:\\Zotero\\storage\\APWI5YW8\\Ayyub, Klir - 2006 - Uncertainty modeling and analysis in engineering and the sciences.pdf}
}

@article{Baker2003,
  title = {Uncertainty {{Specification}} and {{Propagation}} for {{Loss Estimation Using FOSM Methods}}},
  author = {Baker, Jack W and Cornell, C Allin},
  year = {2003},
  number = {September},
  pages = {89},
  abstract = {Probabilistic prediction of structural and nonstructural damage costs due to future earthquakes is one component of loss estimation currently being developed for use in performance-based earthquake engi- neering. Sources of uncertainty in this prediction include epistemic and aleatory uncertainty in the site ground motion hazard, the building response, the damage measures of each of the many building elements, and repair cost of each of the elements. These are inter- and cross-correlated random variables. Two desired results are the total uncertainty in annual losses, and the contribution of each uncertainty source to the total uncertainty. Monte Carlo simulation is a simple solution, but it can be computationally expensive. This study proposes an alternative approach using First-Order Second-Moment (FOSM) methods for all but the (dominant) ground motion intensity variable. Suggestions for characterization of correlations are presented. A procedure for ap- plying FOSM methods in the calculation of total uncertainty is outlined. The proposed technique is very effi- cient, and easily used for sensitivity studies.},
  keywords = {abstract,and aleatory uncertainty in,and nonstructural damage costs,correlation,developed for use in,due to future earthquakes,first-order second-moment,is one component of,loss estimation,loss estimation currently being,neering,performance-based earthquake engi-,probabilistic prediction of structural,seismic,sources of uncertainty in,the site ground,this prediction include epistemic},
  file = {D\:\\Zotero\\storage\\HPXUM8DE\\m-api-5e13a847-6c6e-a72f-2733-adf280749425.pdf;D\:\\Zotero\\storage\\XQB3VGQU\\m-api-712090cc-c5c4-2da9-4fd8-f43960247b89.pdf}
}

@article{Baker2008a,
  title = {Uncertainty Propagation in Probabilistic Seismic Loss Estimation},
  author = {Baker, Jack W. and Cornell, C. Allin},
  year = {2008},
  journal = {Structural Safety},
  volume = {30},
  number = {3},
  pages = {236--252},
  issn = {01674730},
  doi = {10.1016/j.strusafe.2006.11.003},
  abstract = {Probabilistic estimation of losses in a building due to earthquake damage is a topic of interest to decision makers and an area of active research. One promising approach to the problem, proposed by the Pacific Earthquake Engineering Research (PEER) Center, involves breaking the analysis into separate components associated with ground motion hazard, structural response, damage to components and repair costs. Each stage of this method has both inherent (aleatory) randomness and (epistemic) model uncertainty, and these two sources of uncertainty must be propagated through the analysis in order to determine the total uncertainty in the resulting loss estimates. In this paper, the PEER framework for seismic loss estimation is reviewed and options for both characterizing and propagating the various sources of uncertainty are proposed. Models for correlations (among, e.g., element repair costs) are proposed that may be useful when empirical data is lacking. Several options are discussed for propagating uncertainty, ranging from flexible but expensive Monte Carlo simulation to closed form solutions requiring specific functional forms for relationships between variables to be assumed. A procedure that falls between these two extremes is proposed, which integrates over the discrete element damage states, and uses the first-order second-oment method to collapse several conditional random variables into a single conditional random variable representing total repair cost given the ground motion intensity. Numerical integration is then used to incorporate the ground motion hazard. Studies attempting to characterize epistemic uncertainty or develop specific elements of the framework are referenced as an aid for users wishing to implement this loss-estimation procedure. \textcopyright{} 2006 Elsevier Ltd. All rights reserved.},
  keywords = {Aleatory,Epistemic,First-order second-moment,Generalized equi-correlated model,Propagation of uncertainty,Seismic loss estimation},
  file = {D\:\\Zotero\\storage\\8LM4R2XU\\Baker, Cornell - 2008 - Uncertainty propagation in probabilistic seismic loss estimation.pdf}
}

@article{Baker2008b,
  title = {Introducing Correlation among Fragility Functions for Multiple Components},
  author = {Baker, Jack Wesley},
  year = {2008},
  journal = {14th World Conference on Earthquake Engineering},
  abstract = {Correlations are known to be an important consideration when obtaining accurate probabilistic seismic loss estimation results. Correlations among structural response values are straightforward to measure and incorporate, but it is difficult to incorporate correlations in the element damage predicted by fragility functions. In the most widely used component-based loss estimation procedures (e.g., [1-5]), structural response correlations are incorporated, but typically only the special cases of independent or perfectly-correlated component damage states are allowed. A simple method for considering partially correlated damage is proposed here. Whereas traditional calculations first compute a probability of damage and then consider occurrence of that damage to be random, the method proposed here instead considers the ``capacity'' of the components to be random, which allows partial correlation of capacities to be easily introduced (because the random component capacity is continuous, unlike the discrete damage states). The approach relies on Monte Carlo simulation, making it compatible with the Monte Carlo analysis approaches used within some existing loss assessment procedures, although purely analytic implementations are also feasible.},
  keywords = {correlation,Fragility function,loss estimation,performance-based engineering,uncertainty},
  file = {D\:\\Zotero\\storage\\UT7FUUQZ\\Baker - 2008 - Introducing correlation among fragility functions for multiple components.pdf}
}

@article{Baker2011,
  title = {Conditional Mean Spectrum: {{Tool}} for Ground-Motion Selection},
  author = {Baker, Jack W.},
  year = {2011},
  journal = {Journal of Structural Engineering},
  volume = {137},
  number = {3},
  pages = {322--331},
  issn = {07339445},
  doi = {10.1061/(ASCE)ST.1943-541X.0000215},
  abstract = {A common goal of dynamic structural analysis is to predict the response of a structure subjected to ground motions having a specified spectral acceleration at a given period. This is important, for example, when coupling ground-motion hazard curves from probabilistic seismic hazard analysis (PSHA) with results from dynamic structural analysis. The prediction is often obtained by selecting ground motions that match a target response spectrum and using those ground motions as input to dynamic analysis. The commonly used uniform hazard spectrum (UHS) is shown here to be an unsuitable target for this purpose, as it conservatively implies that large-amplitude spectral values will occur at all periods within a single ground motion. An alternative, termed a conditional mean spectrum (CMS), is presented here. The CMS provides the expected (i.e., mean) response spectrum, conditioned on occurrence of a target spectral acceleration value at the period of interest. It is argued that this is the appropriate target response spectrum for the goal described above and is thus a useful tool for selecting ground motions as input to dynamic analysis. The CMS is described, its advantages relative to the UHS are explained, and practical guidelines for use in ground-motion selection are presented. Recent work illustrating the impact of this change in target spectrum on resulting structural response is briefly summarized. \textcopyright{} 2011 American Society of Civil Engineers.},
  keywords = {Conditional mean spectrum,Epsilon,Ground motions,Record selection,Uniform hazard spectrum},
  file = {D\:\\Zotero\\storage\\QZW5YFDC\\Baker - 2011 - Conditional mean spectrum Tool for ground-motion selection.pdf}
}

@article{bakonDataMiningApproach2017,
  title = {A {{Data Mining Approach}} for {{Multivariate Outlier Detection}} in {{Postprocessing}} of {{Multitemporal InSAR Results}}},
  author = {Bakon, Matus and Oliveira, Irene and Perissin, Daniele and Sousa, Joaquim Joao and Papco, Juraj},
  year = {2017},
  month = jun,
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume = {10},
  number = {6},
  pages = {2791--2798},
  issn = {1939-1404, 2151-1535},
  doi = {10.1109/JSTARS.2017.2686646},
  urldate = {2023-07-24},
  langid = {english},
  file = {D\:\\Zotero\\storage\\VDMW7YRL\\Bakon et al. - 2017 - A Data Mining Approach for Multivariate Outlier De.pdf}
}

@article{barbaraParameterEstimationStable,
  title = {Parameter {{Estimation}} for {{Stable Distributions}}: {{Spacings-based}} and {{Indirect Inference}}},
  author = {Barbara, Santa},
  langid = {english},
  file = {D\:\\Zotero\\storage\\HLT8LYKW\\Barbara - Parameter Estimation for Stable Distributions Spa.pdf}
}

@misc{BarrickRiskRecording,
  title = {Barrick\_{{Risk}}\_{{Recording}}\_template\_{{VeladeroClosure}}\_{{2017Oct}}\_{{Rev05}}},
  file = {D\:\\Zotero\\storage\\9FMX7CU2\\Unknown - Unknown - Barrick_Risk_Recording_template_VeladeroClosure_2017Oct_Rev05.xlsm}
}

@book{Bartholomew2010,
  title = {Principal Components Analysis},
  author = {Bartholomew, D. J.},
  year = {2010},
  abstract = {The main purpose of principal-components analysis is to reduce the dimensionality of multivariate data to make its structure clearer. It does this by looking for the linear combination of the variables which accounts for as much as possible of the total variation in the data. It then goes on to look for a second combination, uncorrelated with the first, which accounts for as much of the remaining variation as possible - and so on. If the greater part of the variation is accounted for by a small number of components, they may be used in place of the original variables. \textcopyright{} 2010 Elsevier Ltd. All rights reserved.},
  isbn = {9780080448947},
  keywords = {Correlation matrix,Dimensionality,Factor analysis,Maximum likelihood,Standardized variables},
  file = {D\:\\Zotero\\storage\\RIKHRW7J\\Bartholomew - 2010 - Principal components analysis.pdf}
}

@article{Bayraktarli2011,
  title = {Uncertainty Treatment in Earthquake Modelling Using Bayesian Probabilistic Networks},
  author = {Bayraktarli, Yahya Y. and Baker, Jack W. and Faber, Michael H.},
  year = {2011},
  journal = {Georisk},
  volume = {5},
  number = {1},
  pages = {44--58},
  issn = {17499526},
  doi = {10.1080/17499511003679931},
  abstract = {A probabilistic description of potential ground motion intensity is computed using a Bayesian probabilistic network (BPN) representing the standard probabilistic seismic hazard analysis (PSHA). Two earthquake ground motion intensity parameters are used: Response spectral values for structural failures and peak ground acceleration for geotechnical failures. The correlation of these parameters is also considered within a BPN. It is further shown how deaggregation of the seismic hazard could be easily performed using BPNs. A systematic consideration of uncertainty in the values of the parameters of a particular seismic hazard model can be described by PSHA. But the correct choices for elements of the seismic hazard model are uncertain. Logic trees provide a convenient framework for the treatment of model uncertainty. The paper illustrates an alternative way of incorporating the model uncertainty by extending the developed BPN. Incorporation of time-dependent seismic hazard using a BPN is also illustrated. Finally, the uncertainty treatment in earthquake modelling using BPNs is illustrated on the region Adapazari, which is located close to the western part of the North Anatolian Fault in Turkey. \textcopyright{} 2011 Taylor \& Francis.},
  keywords = {Ground motion intensity parameter correlation,Psha,Time-dependent seismic hazard},
  file = {D\:\\Zotero\\storage\\PAMYLBVW\\Bayraktarli, Baker, Faber - 2011 - Uncertainty treatment in earthquake modelling using bayesian probabilistic networks.pdf}
}

@book{Beh,
  title = {Correspondence {{Analysis}}: {{Theory}}, {{Practice}} and {{New Strategies}}},
  author = {Beh, Eric J. and Lombardo, Rosaria},
  year = {2014},
  journal = {Correspondence Analysis: Theory, Practice and New Strategies},
  doi = {10.1002/9781118762875},
  abstract = {A comprehensive overview of the internationalisation of correspondence analysis Correspondence Analysis: Theory, Practice and New Strategies examines the key issues of correspondence analysis, and discusses the new advances that have been made over the last 20 years. The main focus of this book is to provide a comprehensive discussion of some of the key technical and practical aspects of correspondence analysis, and to demonstrate how they may be put to use. Particular attention is given to the history and mathematical links of the developments made. These links include not just those major contributions made by researchers in Europe (which is where much of the attention surrounding correspondence analysis has focused) but also the important contributions made by researchers in other parts of the world. Key features include: \textbullet{} A comprehensive international perspective on the key developments of correspondence analysis. \textbullet{} Discussion of correspondence analysis for nominal and ordinal categorical data. \textbullet{} Discussion of correspondence analysis of contingency tables with varying association structures (symmetric and non-symmetric relationship between two or more categorical variables). \textbullet{} Extensive treatment of many of the members of the correspondence analysis family for two-way, three-way and multiple contingency tables. Correspondence Analysis offers a comprehensive and detailed overview of this topic which will be of value to academics, postgraduate students and researchers wanting a better understanding of correspondence analysis. Readers interested in the historical development, internationalisation and diverse applicability of correspondence analysis will also find much to enjoy in this book.},
  isbn = {978-1-118-76287-5},
  file = {D\:\\Zotero\\storage\\PJV7GX44\\Beh, Lombardo - 2014 - Correspondence Analysis Theory, Practice and New Strategies.pdf}
}

@book{Benedetto2013a,
  title = {Applied and {{Numerical Harmonic Analysis Series Editor Editorial Advisory Board}}},
  author = {Benedetto, John J and Nih, Akram Aldroubi and Engineering, Biomedical and Daubechies, Ingrid and Heil, Christopher and Mcclellan, James and Unser, Michael and Wickerhauser, Victor and Cochran, Douglas and Feichtinger, Hans G and Kunt, Murat and Sweldens, Wim and Vetterli, Martin},
  year = {2013},
  isbn = {0-8176-3967-5},
  file = {D\:\\Zotero\\storage\\ZDUD34XS\\m-api-0dcf64a3-2e4c-4ac5-045f-cfcef5a1aa51.pdf}
}

@article{Bogart2002,
  title = {Discrete {{Math}} in {{Computer Science}}},
  author = {Bogart, Ken and Stein, Cliff},
  year = {2002},
  journal = {Science},
  abstract = {This is a working draft of a textbook for a discrete mathematics course. This course is designed to be taken by computer science students. The prerequisites are first semester calculus (Math 3) and the introductory computer science course (CS 5). The class is meant to be taken concurrently with or after the second computer science course, Data Structures and Computer Programming (CS 15). This class is a prerequite to Algorithms (CS 25) and it is recommended that it be taken before all CS courses other than 5 and 15.},
  file = {D\:\\Zotero\\storage\\SWLXQJM3\\Bogart, Stein - 2002 - Discrete Math in Computer Science.pdf}
}

@article{Bommer2003,
  title = {Uncertainty about the Uncertainty in Seismic Hazard Analysis},
  author = {Bommer, Julian J.},
  year = {2003},
  journal = {Engineering Geology},
  volume = {70},
  number = {1-2},
  pages = {165--168},
  issn = {00137952},
  doi = {10.1016/S0013-7952(02)00278-8},
  abstract = {In recent years, practitioners of probabilistic seismic hazard analysis have adopted the use of the terms aleatory and epistemic uncertainty. This new terminology has been criticised as a new weapon in the probabilist's arsenal, whereas, in fact, nothing has actually changed: These are merely new words for the existing concepts of randomness and uncertainty, whose usage has become confused and ambiguous. The debate regarding the relative merits and shortcomings of deterministic and probabilistic approaches will no doubt continue for many years. However, this debate could become more productive if clear definitions of concepts, such as uncertainty, are first established to avoid discussions at cross purposes. \textcopyright{} 2003 Elsevier Science B.V. All rights reserved.},
  keywords = {Aleatory uncertainty,Epistemic uncertainty,Recurrence interval,Return period,Seismic hazard assessment},
  file = {D\:\\Zotero\\storage\\PZYN6SUA\\Bommer - 2003 - Uncertainty about the uncertainty in seismic hazard analysis.pdf}
}

@book{bonniniNonparametricHypothesisTesting,
  title = {Nonparametric {{Hypothesis Testing}}},
  author = {Bonnini, Stefano},
  langid = {english},
  file = {D\:\\Zotero\\storage\\AFZ994DI\\Bonnini - Nonparametric Hypothesis Testing.pdf}
}

@article{borakStableDistributions,
  title = {Stable {{Distributions}}},
  author = {Borak, Szymon and H{\"a}rdle and Weron},
  file = {D\:\\Zotero\\storage\\U9KYWMDD\\8.pdf}
}

@book{Bricmont2009,
  title = {Probabilities in {{Physics}}},
  author = {Beisbart, Claus and Hartmann, Stephan},
  year = {2011},
  journal = {Probabilities in Physics},
  doi = {10.1093/acprof:oso/9780199577439.001.0001},
  abstract = {Many theories and models from physics are probabilistic. This observation raises several philosophical questions: What are probabilities in physics? Do they reflect objective chances which exist independently of the human mind? Or do they only express subjective credences and thus capture our own uncertainty about the world? Finally, which metaphysical lessons, if at all, can one draw from the largely probabilistic character of physics? The chapters collected in this volume address these questions and provide a detailed philosophical appraisal of the status of probabilities in all of physics. Particular emphasis is laid upon statistical physics and quantum mechanics. Many chapters reflect a desire to understand probabilities from physics as objective chances. These chances are characterized, e.g., as time-averages, as probabilities from a best system in the terms of David Lewis, or using the Boltzmannian typicality approach. Other chapters are sympathetic to a Bayesian view of probabilities in physics. The chapters about quantum mechanics elucidate the peculiar characteristics of quantum correlations and discuss strategies to justify the Born Rule. Finally, the chapters of this volume demonstrate how closely interpretive issues about probabilities are entangled with other foundational problems of physics such as the Reversibility Paradox, the ontology of the quantum world and the question whether the world is deterministic.},
  isbn = {978-0-19-173060-3},
  keywords = {Chances,Credences,Determinism,Models,Probabilities,Quantum mechanics,Statistical physics},
  file = {D\:\\Zotero\\storage\\SFBGCEYI\\Unknown - 2009 - Bricmont_Probabilities_in_Physics_01.djvu}
}

@book{broemelingBayesianInferenceStochastic,
  title = {Bayesian {{Inference}} for {{Stochastic Processes}}},
  author = {Broemeling, Lyle D},
  langid = {english},
  file = {D\:\\Zotero\\storage\\FMVEJ65K\\Broemeling - Bayesian Inference for Stochastic Processes.pdf}
}

@book{buneaMONOGRAPHSSTATISTICSAPPLIED,
  title = {{{MONOGRAPHS ON STATISTICS AND APPLIED PROBABILITY}}},
  author = {Bunea, F and Isham, V and Keiding, N and Louis, T and Smith, R L and Tong, H},
  langid = {english},
  file = {D\:\\Zotero\\storage\\J4BF8EAF\\Bunea et al. - MONOGRAPHS ON STATISTICS AND APPLIED PROBABILITY.pdf}
}

@article{Burridge2006a,
  title = {Additive Outlier Detection via Extreme-Value Theory},
  author = {Burridge, Peter and Taylor, A. M.Robert},
  year = {2006},
  journal = {Journal of Time Series Analysis},
  volume = {27},
  number = {5},
  pages = {685--701},
  issn = {01439782},
  doi = {10.1111/j.1467-9892.2006.00483.x},
  abstract = {This article is concerned with detecting additive outliers using extreme value methods. The test recently proposed for use with possibly non-stationary time scries by Perron and Rodriguez [Journal of Time Series Analysis (2003) vol. 24, pp. 193-220], is, as they point out, extremely sensitive to departures from their assumption of Gaussianity, even asymptotically. As an alternative, we investigate the robustness to distributional form of a test based on weighted spacings of the sample order statistics. Difficulties arising from uncertainty about the number of potential outliers are discussed, and a simple algorithm requiring minimal distributional assumptions is proposed and its performance evaluated. The new algorithm has dramatically lower level-inflation in face of departures from Gaussianity than the Perron-Rodriguez test, yet retains good power in the presence of outliers. \textcopyright{} 2006 Blackwell Publishing Ltd.},
  keywords = {Additive outliers,Extreme order statistics,Standardized spacings},
  file = {D\:\\Zotero\\storage\\SZHNQMLB\\Burridge, Taylor - 2006 - Additive outlier detection via extreme-value theory.pdf}
}

@book{butcherFeatureEngineeringSelection2020,
  title = {Feature {{Engineering}} and {{Selection}}: {{A Practical Approach}} for {{Predictive Models}}},
  shorttitle = {Feature {{Engineering}} and {{Selection}}},
  author = {Butcher, Brandon and Smith, Brian J.},
  year = {2020},
  month = jul,
  volume = {74},
  urldate = {2023-06-27},
  langid = {english},
  keywords = {key book},
  file = {D\:\\Zotero\\storage\\8CBAMU7J\\Butcher and Smith - 2020 - Feature Engineering and Selection A Practical App.pdf}
}

@article{By1977,
  title = {The Probabilistic Significance of Earthquake Prediction},
  author = {Collins, J D},
  year = {1977},
  journal = {Bulletin - Seismological Society of America},
  volume = {67},
  number = {1},
  pages = {243-244.},
  isbn = {0670010243},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\CSAVMJXB\\By, Sprague - 1977 - F I E D.pdf}
}

@book{caiBayesianNetworksReliability2020,
  title = {Bayesian {{Networks}} for {{Reliability Engineering}}},
  author = {Cai, Baoping and Liu, Yonghong and Liu, Zengkai and Chang, Yuanjiang and Jiang, Lei},
  year = {2020},
  publisher = {{Springer Singapore}},
  address = {{Singapore}},
  doi = {10.1007/978-981-13-6516-4},
  urldate = {2023-06-19},
  isbn = {9789811365157 9789811365164},
  langid = {english},
  file = {D\:\\Zotero\\storage\\RAAYSP2Y\\Cai et al. - 2020 - Bayesian Networks for Reliability Engineering.pdf}
}

@book{candyModelbasedSignalProcessing2006,
  title = {Model-Based Signal Processing},
  author = {Candy, James V.},
  year = {2006},
  series = {Wiley Series in Adaptive and Learning Systems for Signal Processing, Communications, and Control},
  publisher = {{Wiley [u.a.]}},
  address = {{Hoboken, NJ}},
  isbn = {978-0-471-23632-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\PQPUSNMA\\Candy - 2006 - Model-based signal processing.pdf}
}

@article{Carvalho2020,
  title = {Application of Kernel K-Means and Kernel x-Means Clustering to Obtain Soil Classes from Cone Penetration Test Data},
  author = {Carvalho, L.O. and Ribeiro, D.B.},
  year = {2020},
  journal = {Soils and Rocks},
  volume = {43},
  number = {4},
  pages = {607--618},
  issn = {19809743},
  doi = {10.28927/sr.434607},
  keywords = {\ding{72},artificial neural network,cone penetration test},
  file = {D\:\\Zotero\\storage\\JYSREU94\\Carvalho, Ribeiro - 2020 - Application of kernel k-means and kernel x-means clustering to obtain soil classes from cone penetration test.pdf}
}

@techreport{Chang1979,
  title = {{{ARMA}} Models for Earthquake Ground Motions.},
  author = {Chang, M. K. and Kwiatkowski, J. W. and Nau, R. F. and Oliver, R. M. and Pister, K. S.},
  year = {1979},
  abstract = {Four major California earthquake records were analyzed by use of a class of discrete linear time-domain processes commonly referred to as ARMA (Autoregressive/Moving-Average) models. It was possible to analyze these different earthquakes, identify the order of the appropriate ARMA model(s), estimate parameters, and test the residuals generated by these models. It was also possible to show the connections, similarities, and differences between the traditional continuous models (with parameter estimates based on spectral analyses) and the discrete models with parameters estimated by various maximum-likelihood techniques applied to digitized acceleration data in the time domain. The methodology proposed is suitable for simulating earthquake ground motions in the time domain, and appears to be easily adapted to serve as inputs for nonlinear discrete time models of structural motions. -from US Govt Reports Announcements, 13, 1980},
  file = {D\:\\Zotero\\storage\\RKKFQUHR\\Chang et al. - 1979 - ARMA models for earthquake ground motions.pdf}
}

@book{Chatfield1988,
  title = {Statistical {{Analysis}} with {{Missing Data}}.},
  author = {Chatfield, Chris and Little, R. J. A. and Rubin, D. B.},
  year = {1988},
  issn = {09641998},
  doi = {10.2307/2982783},
  isbn = {978-1-118-59601-2},
  file = {D\:\\Zotero\\storage\\7YHIP7AG\\Chatfield, Little, Rubin - 1988 - Statistical Analysis with Missing Data.pdf}
}

@book{Chatterjee2013,
  title = {Handbook of {{Regression Analysis}}},
  author = {Chatterjee, Samprit and Simonoff, Jeffrey S.},
  year = {2013},
  journal = {Handbook of Regression Analysis},
  doi = {10.1002/9781118532843},
  abstract = {Written by two established experts in the field, the purpose of the Handbook of Regression Analysis is to provide a practical, one-stop reference on regression analysis. The focus is on the tools that both practitioners and researchers use in real life. It is intended to be a comprehensive collection of the theory, methods, and applications of regression methods, but it has been deliberately written at an accessible level. The handbook provides a quick and convenient reference or "refresher" on ideas and methods that are useful for the effective analysis of data and its resulting interpretations. Students can use the book as an introduction to and/or summary of key concepts in regression and related course work (including linear, binary logistic, multinomial logistic, count, and nonlinear regression models). Theory underlying the methodology is presented when it advances conceptual understanding and is always supplemented by hands-on examples. References are supplied for readers wanting more detailed material on the topics discussed in the book. R code and data for all of the analyses described in the book are available via an author-maintained website. \textcopyright{} 2013 John Wiley \& Sons, Inc. All rights reserved.},
  isbn = {978-0-470-88716-5},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\C3G5SVD5\\Chatterjee, Simonoff - 2013 - Handbook of Regression Analysis.pdf}
}

@article{Che-Hao1995a,
  title = {Evaluation of Probability Point Estimate Methods},
  author = {{Che-Hao}, Chang and {Yeou-Koung}, Tung and {Jinn-Chuang}, Yang},
  year = {1995},
  journal = {Applied Mathematical Modelling},
  volume = {19},
  number = {2},
  pages = {95--105},
  issn = {0307904X},
  doi = {10.1016/0307-904X(94)00018-2},
  abstract = {In modern engineering designs and analyses, computer models are frequently used. Due to the presence of uncertainties associated with the model inputs and parameters, which are treated as random variables, analysis is feasible if the methods employed do not require excessive computations yet produce reasonably accurate results. Point estimate methods are such schemes that are potentially capable of achieving the goals. Assuming normal distributions to the random variables, three point estimate methods (Rosenblueth's, Harr's, and a modified Harr's method) were evaluated in this paper for different numbers of random variables and different model types. Results of this evaluation indicated that the proposed modified Harr's method yielded comparable, if not better, performance than the other two methods. Also, performance evaluation indicated that additional statistical information, other than the commonly used first two moments, should be incorporated, if available, to enhance the accuracy of uncertainty analysis. \textcopyright{} 1995.},
  keywords = {computer model,equal probability,point estimates,probability,uncertainty analysis},
  file = {D\:\\Zotero\\storage\\MDY9L6TF\\Che-Hao, Yeou-Koung, Jinn-Chuang - 1995 - Evaluation of probability point estimate methods.pdf}
}

@book{Cheremisinoff1988a,
  title = {Statistics for {{Environmental Engineers}}.},
  author = {Mac Berthouex, Paul and Brown, Linfield C.},
  year = {2002},
  publisher = {{Lewis Publisher}},
  issn = {00323640},
  isbn = {1-56670-592-4},
  file = {D\:\\Zotero\\storage\\2TZFSR2Y\\Cheremisinoff - 1988 - Computer Statistics for Environmental Engineers.pdf}
}

@book{Chollet2017,
  title = {Deep {{Learning}} with {{R}}},
  author = {Chollet, Francois and Allaire, J. J.},
  year = {2017},
  file = {D\:\\Zotero\\storage\\6SHAXEGS\\Chollet, Allaire - 2017 - Deep Learning with R.pdf}
}

@book{Chorro2015,
  title = {A Time Series Approach to Option Pricing: {{Models}}, Methods and Empirical Performances},
  author = {Chorro, Christophe and Gu{\'e}gan, Dominique and Lelpo, Florian},
  year = {2015},
  journal = {A Time Series Approach to Option Pricing: Models, Methods and Empirical Performances},
  doi = {10.1007/978-3-662-45037-6},
  abstract = {The current world financial scene indicates at an intertwined and interdependent relationship between financial market activity and economic health. This book explains how the economic messages delivered by the dynamic evolution of financial asset returns are strongly related to option prices. The Black Scholes framework is introduced and by underlining its shortcomings, an alternative approach is presented that has emerged over the past ten years of academic research, an approach that is much more grounded on a realistic statistical analysis of data rather than on ad hoc tractable continuous time option pricing models. The reader then learns what it takes to understand and implement these option pricing models based on time series analysis in a self-contained way. The discussion covers modeling choices available to the quantitative analyst, as well as the tools to decide upon a particular model based on the historical datasets of financial returns. The reader is then guided into numerical deduction of option prices from these models and illustrations with real examples are used to reflect the accuracy of the approach using datasets of options on equity indices.},
  isbn = {978-3-662-45037-6},
  file = {D\:\\Zotero\\storage\\YV7SHHNK\\Chorro, Guégan, Lelpo - 2015 - A time series approach to option pricing Models, methods and empirical performances.pdf}
}

@book{Christensen2006,
  title = {Log-{{Linear Models}} and {{Logistic Regression}}},
  author = {Christensen, Ronald},
  year = {2006},
  journal = {Springer Texts in Statistics},
  issn = {01621459},
  abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a \textasciitilde without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
  isbn = {978-0-387-78188-4},
  pmid = {10911016},
  file = {D\:\\Zotero\\storage\\JIDP4PM2\\Christensen - 2006 - Log-Linear Models and Logistic Regression (Springer Texts in Statistics).pdf}
}

@article{Christian2002a,
  title = {The Point-Estimate Method with Large Numbers of Variables},
  author = {Christian, John T. and Baecher, Gregory B.},
  year = {2002},
  journal = {International Journal for Numerical and Analytical Methods in Geomechanics},
  volume = {26},
  number = {15},
  pages = {1515--1529},
  issn = {03639061},
  doi = {10.1002/nag.256},
  abstract = {Rosenbleuth's point-estimate method has become widely used in geotechnical practice for reliability calculations. Although the point-estimate method is a powerful and simple method for evaluating the moments of functions of random variables, it is limited by the need to make 2n evaluations when there are n random variables. Modifications of the method reduce this to 2n evaluations by using points on the diameters of a hypersphere instead of at the corners of the inscribed hypercube. However, these techniques force the co-ordinates of the evaluation points farther from the means of the variables; for a bounded variable, the points may easily fall outside the domain of definition of the variable. The problem can be avoided by using other techniques for some special cases or by reducing the number of random variables that must be considered. Copyright \textcopyright{} 2002 John Wiley and Sons, Ltd.},
  file = {D\:\\Zotero\\storage\\RK22YXM4\\Christian, Baecher - 2002 - The point-estimate method with large numbers of variables.pdf}
}

@misc{ClosureRiskWorkshop,
  title = {Closure\_{{Risk}}\_{{Workshop}}\_{{Presentation}}\_{{ARCA00}}},
  file = {D\:\\Zotero\\storage\\ID93MVTH\\Unknown - Unknown - Closure_Risk_Workshop_Presentation_ARCA00.pptx}
}

@techreport{COHODES1954a,
  title = {A New Method for Analyzing Extreme-Value Data},
  author = {Lieblein, J.},
  year = {1954},
  journal = {Hospital management},
  volume = {77},
  number = {5},
  pages = {6},
  issn = {00185744},
  pmid = {13162419},
  file = {D\:\\Zotero\\storage\\DK88ZZYI\\Lieblein - 1954 - A new method for analyzing extreme-value data.pdf}
}

@article{Conte1992,
  title = {Nonstationary {{ARMA}} Modeling of Seismic Motions},
  author = {Conte, J. P. and Pister, K. S. and Mahin, S. A.},
  year = {1992},
  journal = {Soil Dynamics and Earthquake Engineering},
  volume = {11},
  number = {7},
  pages = {411--426},
  issn = {02677261},
  doi = {10.1016/0267-7261(92)90005-X},
  abstract = {Discrete time-varying autoregressive - moving average (ARMA) models are used to describe realistic earthquake ground motion time histories. Both amplitude and frequency nonstationarities are incorporated in the model. An iterative Kalman filtering scheme is introduced to identify the time-varying parameters of an ARMA model from an actual earthquake record. Several model verification tests are performed on the identified model. Applications of these identification and verification procedures are given and show that the proposed models and identification algorithms are able to capture accurately the nonstationary features of real earthquake accelerograms, especially the time-variation of the frequency content. The well-known Kanai-Tajimi earthquake model is covariance equivalent with a subset of the low order ARMA(2,1) model. Using the results and methodology of this study, the parameters of a time-varying Kanai-Tajimi earthquake model can be estimated from a target earthquake record or they can be directly associated with characteristic earthquake features such as predominant frequency and frequency bandwidth. \textcopyright{} 1992.},
  keywords = {earthquake ground motion,identification,Kalman filtering,nonstationary,time-varying ARMA model,verification},
  file = {D\:\\Zotero\\storage\\QCHRXV7K\\Conte, Pister, Mahin - 1992 - Nonstationary ARMA modeling of seismic motions.pdf}
}

@article{Cornelis2014,
  title = {Online {{Bayesian}} Spike Removal Algorithms for Structural Health Monitoring of Vehicle Components},
  author = {Cornelis, Bram and Peeters, Bart},
  year = {2014},
  number = {July},
  pages = {2295--2302},
  isbn = {9789727521654},
  keywords = {auto-regressive modeling,bayesian,data preprocessing,estimation,spike removal,structural health monitoring,vehicle damper systems},
  file = {D\:\\Zotero\\storage\\2D88JELX\\Cornelis, Peeters - 2014 - Online Bayesian spike removal algorithms for structural health monitoring of vehicle components.pdf}
}

@article{Cornell1968,
  title = {Engineering Seismic Risk Analysis},
  author = {Cornell, Carl Allin},
  year = {1968},
  journal = {Bulletin of the Seismological Society of America},
  volume = {58},
  number = {5},
  pages = {1583--1606},
  issn = {0037-1106, 1943-3573},
  abstract = {This paper introduces a method for the evaluation of the seismic risk at the site of an engineering project. The results are in terms of a ground motion parameter (such as peak acceleration) versus average return period. The method incorporates the influence of all potential sources of earthquakes and the average activity rates assigned to them. Arbitrary geographical relationships between the site and po- tential point, line, or areal sources can be modeled with computational ease. In the range of interest, the derived distributions of maximum annual ground motions are in the form of Type I or Type II extreme value distributions, if the more com- monly assumed magnitude distribution and attenuation laws are used.},
  isbn = {0167-6105},
  file = {D\:\\Zotero\\storage\\CPC46I6K\\Cornell - 1968 - Engineering seismic risk analysis.pdf}
}

@article{cousineauOutliersDetectionTreatment2010,
  title = {Outliers Detection and Treatment: A Review.},
  author = {Cousineau, Denis and Chartier, Sylvain},
  year = {2010},
  journal = {International Journal of Psychological Research},
  volume = {3},
  number = {1},
  abstract = {Outliers are observations or measures that are suspicious because they are much smaller or much larger than the vast majority of the observations. These observations are problematic because they may not be caused by the mental process under scrutiny or may not reflect the ability under examination. The problem is that a few outliers is sometimes enough to distort the group results (by altering the mean performance, by increasing variability, etc.). In this paper, various techniques aimed at detecting potential outliers are reviewed. These techniques are subdivided into two classes, the ones regarding univariate data and those addressing multivariate data. Within these two classes, we consider the cases where the population distribution is known to be normal, the population is not normal but known, or the population is unknown. Recommendations will be put forward in each case.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\RI8AFUZI\\Cousineau and Chartier - 2010 - Outliers detection and treatment a review..pdf}
}

@book{coxTheoryDesignExperiments2000,
  title = {The {{Theory}} of the {{Design}} of {{Experiments}}},
  author = {Cox, D. R. and Reid, N},
  year = {2000},
  series = {Monographs on Statistics and Applied Probability},
  volume = {36},
  isbn = {1-58488-195-X},
  keywords = {read},
  file = {D\:\\Zotero\\storage\\34P3F5YN\\Pike, McNally - 1997 - Theory and design of photon correlation and light-scattering experiments.pdf}
}

@article{Daling1985,
  title = {Concrete {{Mathmatics}}},
  author = {Daling, J. R. and Weiss, N. S. and Voigt, L. and Spadoni, L. R. and Soderstrom, R. and Moore, D. E. and Stadel, B. V.},
  year = {1985},
  journal = {Fertility and Sterility},
  volume = {43},
  number = {3},
  pages = {389--394},
  issn = {00150282},
  abstract = {One hundred twenty-seven women who had been given diagnoses of tubal infertility between 1979 and 1981 in King County, Washington, yet previously had been pregnant, were interviewed to determine their prior history of legally induced abortion. Their responses were compared with those of 395 women who conceived a child at the same time the infertile women began their unsuccessful attempt to become pregnant. In making the comparison, we adjusted for the effects of variables that in this population were related both to having a induced abortion and to the occurrence of infertility, i.e., age, number of prior pregnancies, number of sexual partners, cigarette smoking habits, Dalkon Shield (A.H. Robins Company, Richmond, VA) use, and whether the woman worked outside the home. The risk of tubal infertility in women who had had an induced abortion was not increased above that of other women (relative risk, 1.15, 95\% confidence interval, 0.70 to 1.89). For women with two or more abortions, the relative risk was 1.29 (95\% confidence interval, 0.39 to 4.20). When only the most recent pregnancy was considered, the relative risk was 1.19 (95\% confidence interval, 0.72 to 1.97). Our results suggest that legal abortion, as performed during the past decade in the United States, does not carry an excess risk for future tubal infertility.},
  isbn = {0201558025},
  file = {D\:\\Zotero\\storage\\SBNBXMK4\\Daling et al. - 1985 - Concrete Mathmatics.pdf}
}

@misc{DanihelkaRISKANALYSISTAILING,
  title = {{{DanihelkaRISK}}\_{{ANALYSIS}}\_{{OF}}\_{{TAILING}}\_{{DAMS}}\_{{F}}},
  file = {D\:\\Zotero\\storage\\QCR65F7A\\Unknown - Unknown - DanihelkaRISK_ANALYSIS_OF_TAILING_DAMS_F.ppt}
}

@book{darwicheModelingReasoningBayesian2009,
  title = {Modeling and Reasoning with {{Bayesian}} Networks},
  author = {Darwiche, Adnan},
  year = {2009},
  edition = {Reprinted},
  publisher = {{Cambridge Univ. Press}},
  address = {{Cambridge}},
  isbn = {978-0-521-88438-9},
  langid = {english},
  file = {D\:\\Zotero\\storage\\VD875F5U\\Darwiche - 2009 - Modeling and reasoning with Bayesian networks.pdf}
}

@article{David1964b,
  title = {Basic {{Statistics}}},
  author = {David, H. A.},
  year = {1964},
  journal = {Army Institute for Proffessional Development},
  volume = {6},
  number = {1},
  pages = {111--112},
  issn = {15372723},
  doi = {10.1080/00401706.1964.10490153},
  file = {D\:\\Zotero\\storage\\GJ93I2S5\\David - 1964 - Basic Statistics.pdf}
}

@book{De2017,
  title = {Statistical {{Models}}. {{THEORY AND PRACTICE}}},
  author = {Freedman, David},
  year = {2017},
  file = {D\:\\Zotero\\storage\\NNJRE2KU\\Freedman - 2017 - Statistical Models.pdf}
}

@article{Deakin1999a,
  title = {A Note on Standard Deviation and {{RMS}}},
  author = {Deakin, R. E. and Kildea, D. G.},
  year = {1999},
  journal = {Australian Surveyor},
  volume = {44},
  number = {1},
  pages = {74--79},
  issn = {00050326},
  doi = {10.1080/00050351.1999.10558776},
  file = {D\:\\Zotero\\storage\\MGJB6BEU\\Deakin, Kildea - 1999 - A note on standard deviation and RMS.pdf}
}

@book{Dean1387,
  title = {Handbook of {{Design}} and {{Analysis}} of {{Experiments}}},
  author = {Dean, Angela and Morris, Max and Stufken, John and Bingham, Derek},
  year = {2016},
  isbn = {978-1-4665-5188-6},
  file = {D\:\\Zotero\\storage\\H46MKLE4\\Dean et al. - 2016 - Handbook of Design and Analysis of Experiments (Chapman & HallCRC Handbooks of Modern Statistical Methods).pdf}
}

@book{Debnath2007,
  title = {Transforms and {{Integral Transforms}} And},
  author = {Debnath, Lokenath and Bhatta, Dambaru},
  year = {2007},
  isbn = {978-1-58488-575-7},
  file = {D\:\\Zotero\\storage\\C79SM69E\\Debnath, Bhatta - 2007 - Transforms and Integral Transforms and.pdf}
}

@book{Devica,
  title = {Nonlinear {{Least Squares}} for {{Inverse Problems}} - {{Theoretical Foundations}} and {{Step-by-Step Guide}} for {{Applications}}},
  author = {Chavent, Guy},
  year = {2009},
  journal = {Scientific Computation},
  eprint = {1011.1669v3},
  issn = {1098-6596},
  doi = {10.1007/978-90-481-2785-6},
  abstract = {Presents an introduction into the least squares resolution of nonlinear inverse problems. This title intends to develop a geometrical theory to analyze nonlinear least square (NLS) problems with respect to their quadratic wellposedness, that is, both wellposedness and optimizability. Nonlinear Least Squares for Inverse Problems; Preface; I Nonlinear Least Squares; Nonlinear Inverse Problems: Examples and Difficulties; Computing Derivatives; Choosing a Parameterization; Output Least Squares Identifiability and QuadraticallyWellposed NLS Problems; Regularization of Nonlinear Least Squares Problems; II A Generalization of Convex Sets; Quasi-Convex Sets; Strictly Quasi-Convex Sets; Deflection Conditions for the Strict Quasi-convexityof Sets; Bibliography; Index.},
  archiveprefix = {arxiv},
  isbn = {978-90-481-2784-9},
  pmid = {25246403},
  file = {D\:\\Zotero\\storage\\XY7MIRSQ\\Chavent - 2009 - Nonlinear Least Squares for Inverse Problems - Theoretical Foundations and Step-by-Step Guide for Applications.pdf}
}

@book{DeWaal2007,
  title = {Statistical Data Editing and Imputation},
  author = {De Waal, Ton and Pannekoek, Jorden and Scholtus, Sander},
  year = {2007},
  journal = {Sample Surveys: Theory, Methods and Inference},
  volume = {29},
  abstract = {The Wiskott-Aldrich syndrome protein (WASP) and its relative neural WASP (N-WASP) regulate the nucleation of actin filaments through their interaction with the Arp2/3 complex and are regulated in turn by binding to GTP-bound Cdc42 and phosphatidylinositol 4,5-bisphosphate. The Nck Src homology (SH) 2/3 adaptor binds via its SH3 domains to a proline-rich region on WASP and N-WASP and has been implicated in recruitment of these proteins to sites of tyrosine phosphorylation. We show here that Nck SH3 domains dramatically stimulate the rate of nucleation of actin filaments by purified N-WASP in the presence of Arp2/3 in vitro. All three Nck SH3 domains are required for maximal activation. Nck-stimulated actin nucleation by N-WASP.Arp2/3 complexes is further stimulated by phosphatidylinositol 4,5-bisphosphate, but not by GTP-Cdc42, suggesting that Nck and Cdc42 activate N-WASP by redundant mechanisms. These results suggest the existence of an Nck-dependent, Cdc42-independent mechanism to induce actin polymerization at tyrosine-phosphorylated Nck binding sites.},
  isbn = {978-0-470-54280-4},
  keywords = {Editing,Imputation,Non-response,Survey data quality},
  file = {D\:\\Zotero\\storage\\A6UGKMAD\\De Waal, Pannekoek, Scholtus - 2007 - Statistical data editing and imputation.pdf}
}

@book{Douglas2017,
  title = {Experimental {{Design}}},
  author = {Montgomery, Douglas C.},
  year = {2017},
  publisher = {{Wiley}},
  isbn = {978-1-119-11347-8},
  file = {D\:\\Zotero\\storage\\VSQBJ9TR\\Douglas - 2017 - Experrimenta Design.pdf}
}

@book{Durbin2013,
  title = {Time {{Series Analysis}} by {{State Space Methods}}},
  author = {Durbin, James and Koopman, Siem Jan},
  year = {2013},
  abstract = {This new edition updates Durbin \& Koopman's important text on the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as made up of distinct components such as trend, seasonal, regression elements and disturbance terms, each of which is modelled separately. The techniques that emerge from this approach are very flexible and are capable of handling a much wider range of problems than the main analytical system currently in use for time series analysis, the Box-Jenkins ARIMA system. Additions to this second edition include the filtering of nonlinear and non-Gaussian series. Part I of the book obtains the mean and variance of the state, of a variable intended to measure the effect of an interaction and of regression coefficients, in terms of the observations. Part II extends the treatment to nonlinear and non-normal models. For these, analytical solutions are not available so methods are based on simulation.},
  file = {D\:\\Zotero\\storage\\484FE47R\\Durbin, Koopman - 2013 - Time Series Analysis by State Space Methods.pdf}
}

@book{EarthquakeScienceSeismic,
  title = {Earthquake {{Science}} and {{Seismic Risk Reduction NATO Science Series}}},
  isbn = {978-1-4020-1778-0},
  file = {D\:\\Zotero\\storage\\XL9HEFZH\\Unknown - Unknown - Earthquake Science and Seismic Risk Reduction NATO Science Series.pdf}
}

@book{Edition2015a,
  title = {Applied {{Regression Analysis}} and {{Generalized Linear Models}}},
  author = {Fox, John},
  year = {2015},
  isbn = {978-1-4522-0566-3},
  keywords = {\ding{72},all},
  file = {D\:\\Zotero\\storage\\N8XMRLCM\\Fox - 2015 - Applied Regression Analysis and Generalized Linear Models.pdf}
}

@book{Editionc,
  title = {Correspondence {{Analysis}} in {{Practice}}},
  author = {Greenacre, Michael},
  year = {2017},
  isbn = {978-1-4987-3177-5},
  file = {D\:\\Zotero\\storage\\YQZL5JUB\\Greenacre - 2017 - Correspondence Analysis in Practice.pdf}
}

@book{Efrromovicha,
  title = {Nonparametric {{Curve Estimation}} : {{Methods}} , {{Theory}} , and {{Applications Springer Series}} in {{Statistics}}},
  author = {Efrromovich, Sam},
  isbn = {0-387-98740-1},
  file = {D\:\\Zotero\\storage\\BN5P54A9\\Efrromovich - Unknown - Nonparametric Curve Estimation Methods , Theory , and Applications Springer Series in Statistics.pdf}
}

@article{El-Choum2014,
  title = {Utilization of Arma Models to Measure Damage Potential in Seismic Records},
  author = {{El-Choum}, M. K.},
  year = {2014},
  journal = {Proceedings of International Structural Engineering and Construction},
  volume = {1},
  number = {1},
  pages = {145--150},
  issn = {2644108X},
  doi = {10.14455/ISEC.res.2014.58},
  abstract = {ARMA parameters are developed to assess the damage potential in seismic records. The parameters are chosen to fit acceleration time series of particular earthquake records using the maximum-likelihood method. For each event, a set of random accelerograms is generated and used to establish statistically-valid structural response spectra. Since the number of earthquake accelerograms in any seismic region is limited, non-stationary stochastic models are used to characterize earthquake ground motion. From a sample of earthquakes, the mean and variance of response spectral ordinates are obtained for damage predictors. Structural design spectra for earthquakes are based on smoothed linear response spectra obtained from different events scaled by their peak values. Such an approach does not incorporate other characteristics of the excitation represented by measured data. Samples of acceleration records are generated for each event. In this study, individual records for an earthquake are treated as one realization of an underlying non-stationary process that actually characterizes the earthquake. This research provides a reliable description of the information contained within acceleration records, and can also provide a reasonable estimate of the average nonlinear demand spectra.},
  isbn = {9780996043700},
  keywords = {Autoregressive Moving Average (ARMA),Ductility,Earthquake,Hysteretic Energy and Stochastic Process,Modeling,Response Spectra,Time Series},
  file = {D\:\\Zotero\\storage\\RKU3J3T7\\El-Choum - 2014 - Utilization of arma models to measure damage potential in seismic records.pdf}
}

@book{Enochson1965,
  title = {Digital Spectral Analysis},
  author = {Enochson, L. D. and Otnes, R. K.},
  year = {1965},
  journal = {SAE Technical Papers},
  issn = {26883627},
  doi = {10.4271/650821},
  abstract = {Overall procedures to perform digital spectral analysis are described, including the necessary equations and analytical procedures to compute the power spectral density functions by the usually employed correlation transformation method. The details which must be considered to obtain proper statistical estimates of power spectral density functions from random data are discussed and statistical variability and bias of the spectrum estimates are reviewed. In addition to the basic statistical problems, a brief discussion reviewing measurement and data processing and reduction problems is presented. It is apparent from the discussion that statistical variability due to finite record lengths, as well as many other errors can occur in the actual data collection and analysis. The many programming problems which arise in addition to those that occur in preparing the computational procedures are discussed. Also, some of the considerations necessary to prepare proper input and output programs are mentioned. These considerations constitute a roughly complete review of the overall problem of digital spectral analysis.},
  isbn = {978-1-84821-277-0},
  file = {D\:\\Zotero\\storage\\7EX5B6PR\\Enochson, Otnes - 1965 - Digital spectral analysis.pdf}
}

@book{Ensor2002,
  title = {Time {{Series Analysis}} and {{Its Applications}}},
  author = {Shumway, Robert and Stoffer, David},
  year = {2017},
  journal = {Springer Texts in Statistics},
  isbn = {978-3-319-52452-8},
  file = {D\:\\Zotero\\storage\\WRZAN2PV\\Shumway, Stoffer - 2017 - Time Series Analysis and Its Applications.pdf}
}

@article{Eom2010,
  title = {Secant Stiffness Method for Inelastic Design of Strut-and-Tie Model},
  author = {Eom, Tae Sung and Park, Hong Gun},
  year = {2010},
  journal = {ACI Structural Journal},
  volume = {107},
  number = {6},
  pages = {689--698},
  issn = {08893241},
  doi = {10.14359/51664017},
  abstract = {A secant stiffness method was developed for the inelastic design of strut-and-tie models (STMs). According to the design strategy intended by the engineer, struts and ties are classified as elastic and inelastic elements. For the inelastic elements, secant stiffness is used to address the effect of inelastic deformations. By performing linear analysis for the STM, the forces and inelastic deformations of the struts and ties are directly determined. The safety of the struts and ties is evaluated by using deformationdependent failure criteria. The proposed method was applied to existing test specimens including a deep beam, a coupling beam, and a shear wall. The design results were compared with the properties and deformation capacities of the test specimens. Copyright \textcopyright{} 2010, American Concrete Institute. All rights reserved.},
  keywords = {Performance-based design,Reinforced concrete,Secant stiffness,Strut-and-tie model},
  file = {D\:\\Zotero\\storage\\SCW32HM8\\Eom, Park - 2010 - Secant stiffness method for inelastic design of strut-and-tie model.pdf}
}

@article{ernitsLIBRARYPOSTPROCESSINGMULTITEMPORAL2019,
  title = {R {{LIBRARY FOR POST-PROCESSING OF MULTI-TEMPORAL INSAR RESULTS USING MULTIVARIATE OUTLIER DETECTION}}},
  author = {Ernits, Juhan-Peep and Kiik, Andreas},
  year = {2019},
  langid = {english},
  file = {D\:\\Zotero\\storage\\Z8TLR3AQ\\Ernits and Kiik - 2019 - R LIBRARY FOR POST-PROCESSING OF MULTI-TEMPORAL IN.pdf}
}

@book{espositovinziHandbookPartialLeast2010,
  title = {Handbook of {{Partial Least Squares}}: {{Concepts}}, {{Methods}} and {{Applications}}},
  shorttitle = {Handbook of {{Partial Least Squares}}},
  editor = {Esposito Vinzi, Vincenzo and Chin, Wynne W. and Henseler, J{\"o}rg and Wang, Huiwen},
  year = {2010},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-32827-8},
  urldate = {2023-07-24},
  isbn = {978-3-540-32825-4 978-3-540-32827-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\YKRADJQQ\\Esposito Vinzi et al. - 2010 - Handbook of Partial Least Squares Concepts, Metho.pdf}
}

@article{Esteva1969a,
  title = {Seismicity {{Prediction}} , {{A Bayesian Approach}}},
  author = {Esteva, Luis},
  year = {1969},
  journal = {4th World Conference on Earthquake Engineering},
  number = {January},
  pages = {13},
  file = {D\:\\Zotero\\storage\\W6DMZB9B\\Esteva - 1969 - Seismicity Prediction , A Bayesian Approach.pdf}
}

@book{EuropeanEnvironmentAgencyEEA2019bl,
  title = {Environmental {{Statistics}}: {{Methods}} and {{Applications}}},
  author = {Ziegel, Eric R},
  year = {2004},
  journal = {Technometrics},
  volume = {46},
  publisher = {{VIC BARNETT}},
  issn = {0040-1706},
  doi = {10.1198/tech.2004.s249},
  abstract = {In modern society, we are ever more aware of the environmental issues we face, whether these relate to global warming, depletion of rivers and oceans, despoliation of forests, pollution of land, poor air quality, environmental health issues, etc. At the most fundamental level it is necessary to monitor what is happening in the environment - collecting data to describe the changing scene. More importantly, it is crucial to formally describe the environment with sound and validated models, and to analyse and interpret the data we obtain in order to take action. Environmental Statistics provides a broad overview of the statistical methodology used in the study of the environment, written in an accessible style by a leading authority on the subject. It serves as both a textbook for students of environmental statistics, as well as a comprehensive source of reference for anyone working in statistical investigation of environmental issues. {$\cdot$} Provides broad coverage of the methodology used in the statistical investigation of environmental issues. {$\cdot$} Covers a wide range of key topics, including sampling, methods for extreme data, outliers and robustness, relationship models and methods, time series, spatial analysis, and environmental standards. {$\cdot$} Includes many detailed practical and worked examples that illustrate the applications of statistical methods in environmental issues. {$\cdot$} Authored by a leading authority on environmental statistics.},
  isbn = {0-471-48971-9},
  keywords = {icle},
  file = {D\:\\Zotero\\storage\\DMCDBZW7\\European Environment Agency (EEA) - Unknown - Environmental Statistics - Methods and Applications.pdf}
}

@book{EuropeanEnvironmentAgencyEEA2019bm,
  title = {Probability {{Methods}} for {{Cost Uncertainty Analysis}}},
  author = {Garvey, P.},
  year = {2019},
  volume = {53},
  keywords = {icle},
  file = {D\:\\Zotero\\storage\\4ICGG8SQ\\Garvey - 2019 - Probability Methods for Cost Uncertainty Analysis.pdf}
}

@book{Farrell1971,
  title = {Solved Problems in Analysis- {{Betta}}, {{Gamma}}, {{Bessel}} Functions},
  author = {Farrell, Orin and Ross, Bertram},
  year = {1971},
  publisher = {{Dover}},
  isbn = {0-486-62713-6},
  file = {D\:\\Zotero\\storage\\EHDLR6PI\\Farrell, Ross - 1971 - Solved problems in analysis- Betta, Gamma, Bessel functions.pdf}
}

@article{Feyter2006a,
  title = {Extreme Value Analysis within a Parametric Outlier Detection Framework},
  author = {Cabras, S. and Morales, J.},
  year = {2006},
  journal = {Applied Stochastic Models in Business and Industry},
  volume = {2007},
  number = {23},
  pages = {157--164},
  doi = {10.1002/asmb},
  keywords = {homogeneity,manpower planning,markov models,stochastic models},
  file = {D\:\\Zotero\\storage\\BPYXDU56\\Feyter - 2006 - Modelling heterogeneity in manpower planning dividing the personnel system into more homogeneous subgroups.pdf}
}

@book{forbesStatisticalDistributions2011,
  title = {Statistical Distributions},
  editor = {Forbes, Catherine Scipione and Evans, Merran and Forbes, Catherine and Evans, Merran and Hastings, Nicholas and Peacock, Brian},
  year = {2011},
  edition = {4. ed},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  isbn = {978-0-470-39063-4},
  langid = {english},
  file = {D\:\\Zotero\\storage\\EST8NALR\\Forbes et al. - 2011 - Statistical distributions.pdf}
}

@misc{FormalRiskAssessment,
  title = {Formal {{Risk Assessment}} ({{FRA}})},
  pages = {4--5},
  file = {D\:\\Zotero\\storage\\RCBQDIS4\\Unknown - Unknown - Process for managing risk Risks & Controls When identifying risk , simply ask , “ what could happen ” , an.pdf}
}

@book{Fotopoulos2007,
  title = {All of {{Nonparametric Statistics}}},
  author = {Fotopoulos, Stergios B},
  year = {2007},
  journal = {Technometrics},
  volume = {49},
  issn = {0040-1706},
  doi = {10.1198/tech.2007.s454},
  abstract = {applicability for this approach.},
  isbn = {978-0-387-25145-5},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\2RZUGDES\\Fotopoulos - 2007 - All of Nonparametric Statistics.pdf}
}

@book{Francq2010,
  title = {{{GARCH Models}}},
  author = {Francq, Christian and Zako{\"i}an, Jean-Michel},
  year = {2010},
  journal = {GARCH Models},
  doi = {10.1002/9780470670057},
  isbn = {978-1-119-31356-4},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\FYAW2CZC\\Francq, Zakoïan - 2010 - GARCH Models.pdf}
}

@misc{FuncionesBesselSu,
  title = {Funciones de {{Bessel}} y Su {{Aplicaci\'on}} a La {{Resoluci\'on}} de {{EDPs}}},
  abstract = {En este cap\'itulo pretendemos estudiar EDPs que dependen de dos variables espaciales (adem\'as de la variable temporal) y veremos que el m\'etodo de separaci\'on de variables es tambi\'en aplicable para estudiar dichas ecuaciones. Ello nos permitir\'a poder estudiar problemas tales como el de la difusi\'on de calor en recintos circulares, la vibraci\'on de una membrana circular sujeta en el borde, o el problema de Dirichlet para la ecuaci\'on de Laplace en un cilindro. Desde un punto de vista matem\'atico, la diferencia fundamental con el caso unidimensional estriba en el hecho de que el problema de Sturm-Liouville al que conduce la aplicaci\'on del m\'etodo de separaci\'on de variables es lo que llamamos singular, lo cual b\'asicamente significa que la ecuaci\'on diferencial ordinaria que nos aparecer\'a es de coeficientes no constantes. Las autofunciones asociadas a estos nuevos problemas de Sturm-Liouville ser\'an bastante m\'as complicadas que las funciones seno y coseno, y son lo que se llaman funciones de Bessel. Este cap\'itulo nos servir\'a adem\'as para constatar un hecho bastante com\'un en ingenier\'ia: el hecho de que a medida que complicamos m\'as los modelos matem\'aticos para estudiar problemas de ingenier\'ia, tambi\'en se complican en igual o mayor medida las matem\'aticas involucradas en dichos problemas. Para ilustrar con un poco m\'as de detalle lo que estamos diciendo en esta introducci\'on, consideremos el problema de Dirichlet para la ecuaci\'on de Laplace en un cilindro y veamos qu\'e sucede si intentamos resolver dicho problema por el m\'etodo de separaci\'on de variables. Siendo {$\Omega$} = \textcopyright{} (x, y, z) {$\in$} R 3 : x 2 + y 2 {$<$} b 2 , 0 {$<$} z {$<$} a \textordfeminine{} el interior de un cilindro, se trata de encontrar una funci\'on u {$\in$} C 2 ({$\Omega$}) {$\cap$} C \textexclamdown{} {$\Omega$} \textcent{} de modo que {$\frac{1}{2}$} {$\increment$}u = 0 en {$\Omega$} u = f sobre {$\partial\Omega$} siendo f {$\in$} C ({$\partial\Omega$}) una funci\'on dada. Debido a la geometr\'ia del dominio en el que estudiamos este problema, parece razonable usar coordenadas cil\'indricas. Si suponemos adem\'as, para simplificar, que f se anula sobre la tapa inferior del cilindro y sobre su borde, entonces el problema anterior se convierte en        u rr + r -1 u r + r -2 u \texttheta\texttheta{} + u zz = 0, en {$\Omega$} u (b, \texttheta, z) = 0, 0 {$\leq$} \texttheta{} {$<$} 2{$\pi$} , 0 {$<$} z {$<$} a u (r, \texttheta, a) = g (r, \texttheta ) , 0 {$\leq$} r {$\leq$} b , 0 {$\leq$} \texttheta{} {$<$} 2{$\pi$} u (r, \texttheta, 0) = 0, 0 {$\leq$} r {$\leq$} b , 0 {$\leq$} \texttheta{} {$<$} 2{$\pi$} 85},
  file = {D\:\\Zotero\\storage\\ST6ASKPG\\Unknown - Unknown - Funciones de Bessel y su Aplicación a la Resolución de EDPs.pdf}
}

@book{genuerRandomForests2020,
  title = {Random {{Forests}} with {{R}}},
  author = {Genuer, Robin and Poggi, Jean-Michel},
  year = {2020},
  series = {Use {{R}}!},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-56485-8},
  urldate = {2023-07-23},
  isbn = {978-3-030-56484-1 978-3-030-56485-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\I4BPDNKY\\Genuer and Poggi - 2020 - Random Forests with R.pdf}
}

@book{ghosalFundamentalsNonparametricBayesian2017,
  title = {Fundamentals of {{Nonparametric Bayesian Inference}}},
  author = {Ghosal, Subhashis and Van Der Vaart, Aad},
  year = {2017},
  month = jun,
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781139029834},
  urldate = {2023-06-19},
  abstract = {Explosive growth in computing power has made Bayesian methods for infinite-dimensional models - Bayesian nonparametrics - a nearly universal framework for inference, finding practical use in numerous subject areas. Written by leading researchers, this authoritative text draws on theoretical advances of the past twenty years to synthesize all aspects of Bayesian nonparametrics, from prior construction to computation and large sample behavior of posteriors. Because understanding the behavior of posteriors is critical to selecting priors that work, the large sample theory is developed systematically, illustrated by various examples of model and prior combinations. Precise sufficient conditions are given, with complete proofs, that ensure desirable posterior properties and behavior. Each chapter ends with historical notes and numerous exercises to deepen and consolidate the reader's understanding, making the book valuable for both graduate students and researchers in statistics and machine learning, as well as in application areas such as econometrics and biostatistics.},
  isbn = {978-0-521-87826-5 978-1-139-02983-4},
  langid = {english},
  file = {D\:\\Zotero\\storage\\U2QQGJ6Y\\Ghosal and Van Der Vaart - 2017 - Fundamentals of Nonparametric Bayesian Inference.pdf}
}

@article{Giamouzi2008b,
  title = {Extreme {{Value Theory Filtering Techniques}} for {{Outlier Detection}}},
  author = {Olmo, Jose},
  year = {2008},
  volume = {34},
  number = {2019},
  pages = {51--79},
  doi = {10.1177/0896920507084623},
  abstract = {Background: Sentence Repetition (SR) is considered to be a good indicator of children's grammatical knowledge. Cross-linguistic evidence suggests that performance on SR improves with age, differentiates children with language difficulties, and shows relationships with other language assessments. However, there is debate about the underlying skills involved in SR with few studies directly investigating the impact of linguistic manipulation on SR performance. In the absence of standardized language assessments and lack of normative data, and building on evidence from typologically diverse languages, SR provides a potentially useful assessment tool in Arabic. Aims: (1) To examine the clinical utility of a novel SR test and an adapted Verbal Short Term Memory (VSTM) test by investigating the psychometric properties of the tests and their sensitivity to age and language ability. (2) To evaluate the contribution of established linguistic knowledge to immediate repetition by comparing the patterns of performance across different linguistic factors 3) To determine whether patterns of performance are similar or dissimilar across different age groups of Typically Developing children and different language ability groups. Methods: Three immediate repetition tests were developed or adapted: (1) a novel SR test targeting morphosyntactic structures of Arabic; (2) an adapted VSTM test based on the structure of the Working Memory Test Battery for Children (WMTB-C; Pickering \& Gathercole, 2001) with three subtests of Digit Recall, Word List Recall, and Nonword List Recall; and (3) an Anomalous Sentence Repetition (ASR) test including sets of Semantically Anomalous and Syntactically Anomalous sentences created from and matched to a subset of sentences in the SR test in target Lexical and Grammatical Morphemes as well as length. The SR and ASR tests were scored for the number of Lexical and Grammatical Morphemes repeated correctly. VSTM tests were scored based on the highest number of items repeated in correct order. The SR and VSTM tests were administered to Typically Developing Arabic-speaking children aged 2;6 to 5;11 (n = 140) and a Language Concerns group in the same age range (n = 16), matched on age and nonverbal IQ. The ASR test was only administered to participants older than 4 years. Results: The SR and VSTM tests were reliable, valid, and sensitive to age and language ability of participants. In the Typical sample a) Lexical Morphemes were easier to repeat than Grammatical Morphemes, (b) Digit span was higher than Word span and Word span was higher than Nonword span, and (c) Typical sentences were easier to repeat than Semantically Anomalous sentences followed by Syntactically Anomalous sentences. The gap between Digit and Word span, Grammatical and Lexical Morphemes in the SR test and Lexical Morphemes in Typical and Semantically Anomalous sentences showed a change with age. While performance was significantly reduced in the Language Concerns group, the profile of performance was largely similar. Like the younger children in the Typical sample, they showed a greater vulnerability in Grammatical Morphemes. Only four of 16 children in the clinical sample showed mismatches between their performance on the SR and VSTM tests. Conclusions: The study's results are consistent with cross-linguistic evidence demonstrating that SR and VSTM tests are sensitive to developmental change and language difficulties and are informative about children's language processing abilities. These findings lay the foundations for creating standardized assessments for Arabic-speaking preschool children.},
  isbn = {0896920507084},
  file = {D\:\\Zotero\\storage\\C8Q4Q3UV\\Giamouzi - 2008 - City , University of London Institutional Repository.pdf}
}

@book{gilyazovRegularizationIllPosedProblems2000,
  title = {Regularization of {{Ill-Posed Problems}} by {{Iteration Methods}}},
  author = {Gilyazov, S. F. and Gol'dman, N. L.},
  year = {2000},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-015-9482-0},
  urldate = {2023-06-27},
  isbn = {978-90-481-5382-4 978-94-015-9482-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\XWY7YHFF\\Gilyazov and Gol’dman - 2000 - Regularization of Ill-Posed Problems by Iteration .pdf}
}

@book{Ginevan2003a,
  title = {Statistical Tools for Environmental Quality Measurement},
  author = {Ginevan, Michael E. and Splitstone, Douglas E.},
  year = {2003},
  journal = {Statistical Tools for Environmental Quality Measurement},
  issn = {0040-1706},
  doi = {10.1198/tech.2004.s203},
  abstract = {When interpreting environmental data, scientists and engineers first must select the correct statistical tool to use for their analysis. By doing this they will be able to make sound decisions in their efforts to solve environmental problems. They need a detailed reference that points out the subtle differences between statistical procedures, makin.},
  isbn = {978-0-203-49689-3},
  file = {D\:\\Zotero\\storage\\66RIBFIE\\Ginevan, Splitstone - 2003 - Statistical tools for environmental quality measurement.pdf}
}

@book{Gladwell2005,
  title = {Inverse {{Problems}} in {{Vibration SOLID MECHANICS AND ITS APPLICATIONS Volume}} 119},
  author = {Gladwell, Graham M.L.},
  year = {2005},
  journal = {Civil Engineering},
  abstract = {In the first, 1986, edition of this book, inverse problems in vibration were interpreted strictly: problems concerning the reconstruction of a unique, undamped vibrating system, of a specified type, from specified vibratory behaviour, particularly specified natural frequencies and/or natural mode shapes. In this new edition the scope of the book has been widened to include topics such as isospectral systems- families of systems which all exhibit some specified behaviour; applications of the concept of Toda flow; new, non- classical approaches to inverse Sturm-Liouville problems; qualitative properties of the modes of some finite element models; damage identification. With its emphasis on analysis, on qualitative results, rather than on computation, the book will appeal to researchers in vibration theory, matrix analysis, differential and integral equations, matrix analysis, non- destructive testing, modal analysis, vibration isolation, etc.},
  isbn = {1-4020-2721-4},
  file = {D\:\\Zotero\\storage\\IC474RJS\\Gladwell - 2005 - Inverse Problems in Vibration SOLID MECHANICS AND ITS APPLICATIONS Volume 119.pdf}
}

@article{Gobierno,
  title = {Proceso Para {{Gestionar}} El {{Riesgo}} for Managing Risk {{Riesgos}} y {{Controles Matriz}} de {{Riesgo Efectividad}} de {{Controles Criterios}} de {{Consecuencia}}},
  author = {Gobierno, El and Ejecutivo, El},
  file = {D\:\\Zotero\\storage\\BEL2CDTL\\Gobierno, Ejecutivo - Unknown - Proceso para Gestionar el Riesgo for managing risk Riesgos y Controles Matriz de Riesgo Efectividad de C.PDF}
}

@article{Gonzalez2017,
  title = {Risk {{Assessment}} for {{Landslides Using Bayesian Networks}} and {{Remote Sensing Data}}},
  author = {Gonz{\'a}lez, Patricia Varela and {Medina-cetina}, Zenon and Ph, D},
  year = {2017},
  number = {2010},
  pages = {113--123},
  file = {D\:\\Zotero\\storage\\KX9C3C3U\\González, Medina-cetina, Ph - 2017 - Risk Assessment for Landslides Using Bayesian Networks and Remote Sensing Data.pdf}
}

@book{grabchakTemperedStableDistributions2016,
  title = {Tempered {{Stable Distributions}}},
  author = {Grabchak, Michael},
  year = {2016},
  series = {{{SpringerBriefs}} in {{Mathematics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24927-8},
  urldate = {2023-04-05},
  isbn = {978-3-319-24925-4 978-3-319-24927-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\H66X7FZS\\Grabchak - 2016 - Tempered Stable Distributions.pdf}
}

@book{greenbergIntroductionBayesianEconometrics2008,
  title = {Introduction to {{Bayesian}} Econometrics},
  author = {Greenberg, Edward},
  year = {2008},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  abstract = {This book introduces the increasingly popular Bayesian approach to statistics to graduates and advanced undergraduates. In contrast to the long-standing frequentist approach to statistics, the Bayesian approach makes explicit use of prior information and is based on the subjective view of probability. Bayesian econometrics takes probability theory as applying to all situations in which uncertainty exists, including uncertainty over the values of parameters. A distinguishing feature of this book is its emphasis on classical and Markov chain Monte Carlo (MCMC) methods of simulation. The book is concerned with applications of the theory to important models that are used in economics, political science, biostatistics, and other applied fields. These include the linear regression model and extensions to Tobit, probit, and logit models; time series models; and models involving endogenous variables},
  isbn = {978-1-139-12924-4},
  langid = {english},
  annotation = {OCLC: 774401974},
  file = {D\:\\Zotero\\storage\\QTBUJDFG\\Greenberg - 2008 - Introduction to Bayesian econometrics.pdf}
}

@book{Greenfield1980a,
  title = {Fundamentals of {{Probability}} and {{Statistics}} for {{Engineers}}},
  author = {Soong, T. T.},
  issn = {00359254},
  doi = {10.2307/2346424},
  abstract = {This book was written for an introductory one-semester or two-quarter course in probability and statistics for students in engineering and applied sciences. No previous knowledge of probability or statistics is presumed but a good under- standing of calculus is a prerequisite for the material.},
  isbn = {0-470-86813-9},
  file = {D\:\\Zotero\\storage\\BJL3I363\\Greenfield, Greer - 1980 - Statistics for Engineers.pdf}
}

@article{Grubbs1969a,
  title = {Procedures for {{Detecting Outlying Observations}} in {{Samples}}},
  author = {Grubbs, Frank E.},
  year = {1969},
  journal = {Technometrics},
  volume = {11},
  number = {1},
  pages = {1--21},
  issn = {15372723},
  doi = {10.1080/00401706.1969.10490657},
  abstract = {Procedures are given for determining statistically whether the highest observation, the lowest observation, the highest and lowest observations, the two highest observations, the two lowest observations, or more of the observations in the sample are statistical outliers. Both the statistical formulae and the application of the procedures to examples are given, thus representing a rather complete treatment of tests for outliers in single samples. This paper has been prepared primarily as an expository and tutorial article on the problem of detecting outlying observations in much experimental work. We cover only tests of significance in thii paper. \textcopyright{} 1969 Taylor \& Francis Group, LLC.},
  file = {D\:\\Zotero\\storage\\PP3DGRA2\\Grubbs - 1969 - Procedures for Detecting Outlying Observations in Samples.pdf}
}

@article{hadlockJohnsonQuantileParameterizedDistributions2017,
  title = {Johnson {{Quantile-Parameterized Distributions}}},
  author = {Hadlock, Christopher C. and Bickel, J. Eric},
  year = {2017},
  month = mar,
  journal = {Decision Analysis},
  volume = {14},
  number = {1},
  pages = {35--64},
  issn = {1545-8490, 1545-8504},
  doi = {10.1287/deca.2016.0343},
  urldate = {2023-04-09},
  abstract = {It is common decision analysis practice to elicit quantiles of continuous uncertainties and then fit a continuous probability distribution to the corresponding probabilityquantile pairs. This process often requires curve fitting and the best-fit distribution will often not honor the assessed points. By strategically extending the Johnson Distribution System, we develop a new distribution system that honors any symmetric percentile triplet of quantile assessments (e.g., the 10th-50th-90th) in conjunction with specified support bounds. Further, our new system is directly parameterized by the assessed quantiles and support bounds, eliminating the need to apply a fitting procedure. Our new system is practical, flexible, and, as we demonstrate, able to match the shapes of numerous commonly named distributions.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\57LIGP8L\\Hadlock and Bickel - 2017 - Johnson Quantile-Parameterized Distributions.pdf}
}

@book{hairPartialLeastSquares2021,
  title = {Partial {{Least Squares Structural Equation Modeling}} ({{PLS-SEM}}) {{Using R}}: {{A Workbook}}},
  shorttitle = {Partial {{Least Squares Structural Equation Modeling}} ({{PLS-SEM}}) {{Using R}}},
  author = {Hair, Joseph F. and Hult, G. Tomas M. and Ringle, Christian M. and Sarstedt, Marko and Danks, Nicholas P. and Ray, Soumya},
  year = {2021},
  series = {Classroom {{Companion}}: {{Business}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-80519-7},
  urldate = {2023-07-24},
  isbn = {978-3-030-80518-0 978-3-030-80519-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\BFGTTH3I\\Hair et al. - 2021 - Partial Least Squares Structural Equation Modeling.pdf}
}

@book{hamelKnowledgeDiscoverySupport2009,
  title = {Knowledge {{Discovery}} with {{Support Vector Machines}}},
  author = {Hamel, Lutz},
  year = {2009},
  month = jul,
  edition = {1},
  publisher = {{Wiley}},
  doi = {10.1002/9780470503065},
  urldate = {2023-06-27},
  isbn = {978-0-470-37192-3 978-0-470-50306-5},
  langid = {english},
  file = {D\:\\Zotero\\storage\\TWBCQSQB\\Hamel - 2009 - Knowledge Discovery with Support Vector Machines.pdf}
}

@article{Hammah2009a,
  title = {Numerical Modelling of Slope Uncertainty Due to Rock Mass Jointing},
  author = {Hammah, {\relax RE} and Yacoub, T.E. and Curran, J. H.},
  year = {2009},
  journal = {International Conference on Rock Joints and Jointed Rock Masses},
  pages = {1--8},
  abstract = {Analysis of the stability of slopes in jointed rock masses is not trivial and is fraught with uncertainty and risk. This paper evaluates the ability of different probabilistic methods to model slope stability uncertainty caused by randomness in the geometry of joint networks. The stability of slopes is modelled with a Finite Element-based, shear strength reduction method. The probabilistic techniques considered are the point estimate, response surface, reliability, Monte Carlo and Latin Hypercube methods. The intent of the probabilistic analysis is to calculate the statistical moments of the distribution of factors of safety.\textbackslash nThe paper establishes that Monte Carlo simulation is the most appropriate method for analyzing uncertainty caused by joint network randomness. Because of the diversity in factors of safety and failure modes that stem from joint network randomness, it is concluded that probabilistic analysis must be applied more regularly to improve understanding and the robustness of design.\textbackslash nKeywords: Rock Slopes; Probabilistic Analysis; Risk Analysis; Probability of Failure; Joint Networks; Discrete Fracture Networks; Jointed Rock Masses, Finite Element Method; Shear Strength Reduction Analysis;},
  keywords = {discrete fracture,finite element method,joint networks,jointed rock masses,networks,probabilistic analysis,probability of failure,risk analysis,rock slopes,shear strength reduction analysis},
  file = {D\:\\Zotero\\storage\\VGV8E7UQ\\m-api-6c9e2016-7e5c-5a21-69ae-fb8dac218e31.pdf}
}

@article{Hardle1993,
  title = {Comparing {{Nonparametric Versus Parametric Regression Fits}}},
  author = {Hardle, W. and Mammen, E.},
  year = {1993},
  journal = {The annals of Statistics},
  volume = {21},
  eprint = {1011.1669v3},
  archiveprefix = {arxiv},
  file = {D\:\\Zotero\\storage\\5HQ38TME\\Hardle, Mammen - 1993 - Comparing Nonparametric Versus Parametric Regression Fits.pdf}
}

@article{Hardle2003a,
  title = {Applied {{Multivariate Statistical Analysis}}},
  author = {H{\"a}rdle, Wolfgang and Simar, L{\'e}opold},
  year = {2003},
  journal = {Applied Multivariate Statistical Analysis},
  number = {April},
  doi = {10.1007/978-3-662-05802-2},
  abstract = {The second edition of this book widens the scope of the methods and applications of Applied Multivariate Statistical Analysis. We have introduced more up to date data sets in our examples. These give the text a higher degree of timeliness and add an even more applied flavour. Since multivariate statistical methods are heavily used in quantitative finance and risk management we have put more weight on the presentation of distributions and their densities. We discuss in detail different families of heavy tailed distributions (Laplace, Generalized Hy- perbolic). We also devoted a section on copulae, a new concept of dependency used in the financial risk management and credit scoring. In the chapter on computer intensive meth- ods we have added support vector machines, a new classification technique from statistical learning theory. We apply this method to bankruptcy and rating analysis of firms. The very important CART (Classification and Regression Tree) technique is also now inserted into this chapter. We give an application to rating of companies. The probably most important step towards readability and user friendliness of this book is that we have translated all Quantlets into the R and Matlab language. The algorithms can be downloaded from the authors' web sites. In the preparation of this 2nd edition, we received helpful output from Anton Andriyashin, Ying Chen, Song Song and Uwe Ziegenhagen. We would like to thank them.},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\SUJPHX43\\Härdle, Simar - 2003 - Applied Multivariate Statistical Analysis.pdf}
}

@book{Hardle2008,
  title = {Incorporating Parametric Components},
  author = {H{\"a}rdle, Wolfgang},
  year = {2008},
  doi = {10.1017/ccol0521382483.009},
  isbn = {978-1-139-05213-9},
  file = {D\:\\Zotero\\storage\\XXVH6N3H\\Härdle - 2008 - Incorporating parametric components.pdf}
}

@article{Harr1989a,
  title = {Probabilistic Estimates for Multivariate Analyses},
  author = {Harr, Milton E.},
  year = {1989},
  journal = {Applied Mathematical Modelling},
  volume = {13},
  number = {5},
  pages = {313--318},
  issn = {0307904X},
  doi = {10.1016/0307-904X(89)90075-9},
  abstract = {The ways in which engineering systems fail (i.e., their occurence and frequency) demonstrate considerable differences between hypothetical models assumed for design and actual performance. All materials contain imperfections. All systems are subject to complex interrelationships, material defects, structural deficiencies, human errors, ambient fluctuations, and, hence, to varying degrees of randomness (uncertainties). Various methods have been offered to accomodate uncertainty. The most common, by far, is to assign single-valued point estimates that reflect central tendencies or implied levels of conservatism. Analyses are then reduced to deterministic treatments. More direct probabilistic methods employ Monte Carlo simulations or truncated Taylor series. However, analyses rapidly become exceedingly difficult, if not impossible, for these methods for all but a very few uncorrelated random variables. These matters and others have been ameliorated by Rosenblueth's point estimate method.1 Many problems are encountered that require numerous correlated random variables. For such systems all of the above methods become untractable. This paper presents a simple procedure that accomodates the analyses of such systems. \textcopyright{} 1989.},
  keywords = {eigenvalues,mathematical model,point distributions,probability},
  file = {D\:\\Zotero\\storage\\QF6BLXLM\\Harr - 1989 - Probabilistic estimates for multivariate analyses.pdf}
}

@book{hartshornHypothesisTestingVisual,
  title = {Hypothesis {{Testing}} : {{A Visual Introduction To Statistical Significance}}},
  author = {Hartshorn, Scott},
  langid = {english},
  file = {D\:\\Zotero\\storage\\UH5CVU9P\\Hartshorn - Hypothesis Testing  A Visual Introduction To Stat.pdf}
}

@book{HastieEA2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, T and Tibshirani, R and Friedman, J},
  year = {2009},
  journal = {Springer Series in Statistics},
  volume = {27},
  issn = {03436993},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
  isbn = {978-0-387-84857-0},
  pmid = {15512507},
  file = {D\:\\Zotero\\storage\\9CN4CEIX\\m-api-7216ce53-2dfc-d0f3-3e20-9d37d44228ad.pdf}
}

@book{heardIntroductionBayesianInference2021,
  title = {An {{Introduction}} to {{Bayesian Inference}}, {{Methods}} and {{Computation}}},
  author = {Heard, Nick},
  year = {2021},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-82808-0},
  urldate = {2023-06-19},
  isbn = {978-3-030-82807-3 978-3-030-82808-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\MHAQUVCP\\Heard - 2021 - An Introduction to Bayesian Inference, Methods and.pdf}
}

@techreport{Heintz2010,
  title = {{{FEMA P-695 Quantification}} of {{Building Seismic Performance Factors}}},
  author = {a Heintz, Jon and Council, Applied Technology},
  year = {2010},
  journal = {ATC-63},
  file = {D\:\\Zotero\\storage\\PGVNGJ3N\\Heintz, Council - 2010 - LATBSDC Annual Meeting May 7, 2010.pdf}
}

@article{Helsel1990a,
  title = {Less than Obvious: {{Statistical}} Treatment of Data below the Detection Limit},
  author = {Helsel, Dennis R.},
  year = {1990},
  journal = {Environmental Science and Technology},
  volume = {24},
  number = {12},
  pages = {1766--1774},
  issn = {15205851},
  doi = {10.1021/es00082a001},
  file = {D\:\\Zotero\\storage\\LIAX2A4X\\Helsel - 1990 - Less than obvious Statistical treatment of data below the detection limit.pdf}
}

@book{Helsel2012a,
  title = {{{STATISTICS FOR CENSORED ENVIRONMENTAL DATA USING MINITAB\`O AND R}}},
  author = {Helsel, Denise R.},
  year = {2012},
  publisher = {{Wiley}},
  isbn = {978-0-470-47988-9},
  file = {D\:\\Zotero\\storage\\JWLH5RJR\\Helsel - 2012 - STATISTICS FOR CENSORED ENVIRONMENTAL DATA USING MINITABÒ AND R.pdf}
}

@book{Hoel,
  title = {Introduction to {{Mathematical Statistics}} 6th Ed. - {{P}}. {{Hoel}}.Pdf},
  author = {{Hoel}},
  file = {D\:\\Zotero\\storage\\FHGN8IQE\\Unknown - Unknown - Introduction to Mathematical Statistics 6th ed. - P. Hoel.pdf.pdf}
}

@book{Hofer2018a,
  title = {The {{Uncertainty Analysis}} of {{Model Results}}},
  author = {Hofer, Eduard},
  year = {2018},
  journal = {The Uncertainty Analysis of Model Results},
  doi = {10.1007/978-3-319-76297-5},
  isbn = {978-3-319-76296-8},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\5XNX7T44\\Hofer - 2018 - The Uncertainty Analysis of Model Results.pdf}
}

@article{Hong1998a,
  title = {An Efficient Point Estimate Method for Probabilistic Analysis},
  author = {Hong, H. P.},
  year = {1998},
  journal = {Reliability Engineering and System Safety},
  volume = {59},
  number = {3},
  pages = {261--267},
  issn = {09518320},
  doi = {10.1016/S0951-8320(97)00071-9},
  abstract = {A new and efficient point estimate method is developed to calculate the statistical moments of a random quantity, Z, that is a function of n random variables, X. The method is an extension of Rosenblueth's two-point concentration method. The method uses m X n concentrations matching up to the first m X n non-crossed moments of each random variable and crossed second order moments of the random variables. The kth moment of Z is calculated by weighting the value of Z to the power of k evaluated at n X m locations. Simple to use formulas are provided for two special cases of the method, i.e. 2n-concentration scheme and 2n + 1-concentration scheme. This 2n-concentration scheme considers the skewness of probability density function. The 2n + 1-concentration scheme considers the skewness and kurtosis of probability density function. The correlations between the random variables are considered by using a rotational transformation based on the eigenvector of covariance matrix. Illustrative examples are presented. \textcopyright{} 1998 Published by Elsevier Science Limited.},
  file = {D\:\\Zotero\\storage\\7VBHNNKI\\Hong - 1998 - An efficient point estimate method for probabilistic analysis.pdf}
}

@book{Hooten2007a,
  title = {Statistical {{Analysis}} of {{Environmental Space-Time Processes}}},
  author = {Le, Nhu D. and Zidek, James V.},
  issn = {0162-1459},
  doi = {10.1198/jasa.2007.s237},
  abstract = {This book provides a broad introduction to the subject of environmental space-time processes, addressing the role of uncertainty. It covers a spectrum of technical matters from measurement to environmental epidemiology to risk assessment. It showcases non-stationary vector-valued processes, while treating stationarity as a special case. In particular, with members of their research group the authors developed within a hierarchical Bayesian framework, the new statistical approaches presented in the book for analyzing, modeling, and monitoring environmental spatio-temporal processes. Furthermore they indicate new directions for development.},
  isbn = {978-0-387-26209-3},
  file = {D\:\\Zotero\\storage\\N23Y9YCB\\Hooten - 2007 - Statistical Analysis of Environmental Space-Time Processes.pdf}
}

@article{Huang2011,
  title = {Scaling Earthquake Ground Motions for Performance-Based Assessment of Buildings},
  author = {Huang, Yin Nan and Whittaker, Andrew S. and Luco, Nicolas and Hamburger, Ronald O.},
  year = {2011},
  journal = {Journal of Structural Engineering},
  volume = {137},
  number = {3},
  pages = {311--321},
  issn = {07339445},
  doi = {10.1061/(ASCE)ST.1943-541X.0000155},
  abstract = {The impact of alternate ground-motion scaling procedures on the distribution of displacement responses in simplified structural systems is investigated. Recommendations are provided for selecting and scaling ground motions for performance-based assessment of buildings. Four scaling methods are studied, namely, (1)geometric-mean scaling of pairs of ground motions, (2)spectrum matching of ground motions, (3)first-mode-period scaling to a target spectral acceleration, and (4)scaling of ground motions per the distribution of spectral demands. Data were developed by nonlinear response-history analysis of a large family of nonlinear single degree-of-freedom (SDOF) oscillators that could represent fixed-base and base-isolated structures. The advantages and disadvantages of each scaling method are discussed. The relationship between spectral shape and a ground-motion randomness parameter, is presented. A scaling procedure that explicitly considers spectral shape is proposed. \textcopyright{} 2011 American Society of Civil Engineers.},
  keywords = {Ground motion,Response spectra,Scale,Seismic design,Time-series analysis},
  file = {D\:\\Zotero\\storage\\3V3H4B74\\Huang et al. - 2011 - Scaling earthquake ground motions for performance-based assessment of buildings.pdf}
}

@book{huberRobustStatistics2009,
  title = {Robust Statistics},
  author = {Huber, Peter J. and Ronchetti, Elvezio M.},
  year = {2009},
  series = {Wiley Series in Probability and Statistics},
  edition = {2nd ed},
  publisher = {{Wiley}},
  address = {{Hoboken (N.J.)}},
  abstract = {"Robust Statistics, Second Edition includes four new chapters on the following topics: robust tests; small sample asymptotics; breakdown point; and Bayesian robustness. A new section on time series has also been included. The first edition of this book was the first systematic, book-length treatment of robust statistics. The book begins with a general introduction and the formal mathematical background behind qualitative and quantitative robustness. A solid foundation of robust statistics for both the theoretical and the applied statistician is provided. The book successfully reorganizes, summarizes, and extends information that has been available in part thus far. Concepts are stressed throughout rather than mathematical completeness, and selected numerical algorithms for computing robust estimates, as well as convergence proofs, are provided. Quantitative robustness information for a variety of estimates is contained within tables throughout."--Jacket},
  isbn = {978-0-470-12990-6},
  langid = {english},
  lccn = {519.5},
  file = {D\:\\Zotero\\storage\\5LRVG7L3\\Huber and Ronchetti - 2009 - Robust statistics.pdf}
}

@article{hustonmccullochPreciseTabulationMaximallyskewed1997,
  title = {Precise Tabulation of the Maximally-Skewed Stable Distributions and Densities},
  author = {Huston McCulloch, J. and Panton, Don B.},
  year = {1997},
  month = jan,
  journal = {Computational Statistics \& Data Analysis},
  volume = {23},
  number = {3},
  pages = {307--320},
  issn = {01679473},
  doi = {10.1016/S0167-9473(96)00039-4},
  urldate = {2023-04-05},
  abstract = {The cdf and pdf of the maximally skewed (/\textasciitilde{} = 1) stable distributions are tabulated to high precision, by means of Zolotarev's integral representation, for ct = 0.50 (0.02) 2.00, at fractiles corresponding to p = 0.0001, 0.001, 0.005, 0.01 (0.01) 0.99, 0.995, 0.999, 0.9999. This tabulation is intended to be suitable for developing and calibrating a numerical approximation to these distributions. The probability at the tabulated fractiles is estimated to be accurate to within 4.1 \texttimes{} 10- \textasciitilde o. The densities have an absolute precision of 2.0 x 10-13 and a relative precision of 1.6 x 10-12. Zolotarev's correction of the discontinuity at ct = 1 is graphically illustrated. The full tabulation, documented here, is available by anonymous FTP.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\VL5ADNK4\\Huston McCulloch and Panton - 1997 - Precise tabulation of the maximally-skewed stable .pdf}
}

@article{hwangGeneralizedStructuredComponent,
  title = {Generalized {{Structured Component Analysis}}: {{A Component-Based Approach}} to {{Structural Equation Modeling}}},
  author = {Hwang, Heungsun},
  langid = {english},
  file = {D\:\\Zotero\\storage\\9L3SP85M\\Hwang - Generalized Structured Component Analysis A Compo.pdf}
}

@book{HypothesisTestingIntuitive,
  title = {Hypothesis {{Testing}}: {{An Intuitive Guide}}},
  langid = {english},
  file = {D\:\\Zotero\\storage\\YZV4EMEI\\Hypothesis Testing An Intuitive Guide.pdf}
}

@book{Idier,
  title = {Bayseian {{Approach}} to {{Inverse Problems}}},
  author = {Idier, Jerome},
  year = {2008},
  publisher = {{Wiley}},
  isbn = {978-1-84821-032-5},
  file = {D\:\\Zotero\\storage\\EZJTD5IJ\\Idier - 2008 - Bayseian Approach to Inverse Problems.pdf}
}

@article{Iervolino2017,
  title = {The Effect of Spatial Dependence on Hazard Validation},
  author = {Iervolino, Iunio and Giorgio, Massimiliano and Cito, Pasquale},
  year = {2017},
  journal = {Geophysical Journal International},
  volume = {209},
  number = {3},
  pages = {1363--1368},
  issn = {1365246X},
  doi = {10.1093/gji/ggx090},
  abstract = {In countries where best-practice probabilistic hazard studies and seismic monitoring networks are available, there is increasing interest in direct validation of hazard maps. It usually means trying to quantitatively understand whether probabilities estimated via hazard analysis are consistent with observed frequencies of exceedance of ground motion intensity thresholds. Because the exceedance events of interest are typically rare with respect to the time span covered by data from seismic networks, a common approach underlying these studies is to pool observations from different sites. The main reason for this is to collect a sample large enough to convincingly performa statistical analysis. However, this requires accounting for the dependence among the stochastic processes counting exceedances of ground motion intensity measures thresholds at different sites. Neglecting this dependence may lead to potentially fallacious conclusions about inadequateness of probabilistic seismic hazard. This study addresses this issue revisiting a hazard validation exercise for Italy, showing that accounting for this kind of spatial dependence can change the results of formal testing.},
  keywords = {Probabilistic forecasting,Probability distributions,Seismic attenuation,Statistical methods},
  file = {D\:\\Zotero\\storage\\ZPYWFN6Q\\Iervolino, Giorgio, Cito - 2017 - The effect of spatial dependence on hazard validation.pdf}
}

@book{IntegralsBesselFunctions1962,
  title = {Integrals of {{Bessel}} Functions},
  year = {1962},
  volume = {274},
  file = {D\:\\Zotero\\storage\\H4YHAGA4\\Unknown - 1962 - Integrals of Bessel functions.pdf}
}

@book{Janosik2005bd,
  title = {Sensitivity and {{Uncertainty Analysis}} - {{Vol II}}},
  author = {Cacuci, Dan G and {Ionescu-bujor}, Mihaela and Navon, Ionel Michael},
  year = {2005},
  volume = {42},
  keywords = {icle},
  file = {D\:\\Zotero\\storage\\TRL7XELE\\Cacuci, Ionescu-bujor, Navon - 2005 - Sensitivity and Uncertainty Analysis - Vol II.pdf}
}

@book{Janosik2005r,
  title = {Statistical {{Analysis}} of {{Extreme Values}}},
  author = {Reiss, R. and Thomas, M.},
  year = {2007},
  volume = {42},
  eprint = {1011.1669v3},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {Learning how to recognize and anticipate the legal risks associated with student affairs practice is a crucial skill all successful administrators must develop. This can be done by developing a sense for scanning the broad legal environment and being aware of legal issues in other parts of the education enterprise. Good professionals make a considerable effort to remain current in their career fields. Professional associations assist their members in this task by developing training and professional development programs that address the critical skills that professionals need to do their jobs. In higher education and student affairs, many practitioners acknowledge the importance of knowing how the law affects what they do. Constitutional law affects what kinds of rules and regulations public institutions promulgate. Contract law affects the type of business relationship administrators have with students and other constituents. Tort law affects how managers maintain facilities and supervise student events. As a result, professional associations have been created to focus attention solely on legal issues in higher education (e.g., Education Law Association and the Association for Interdisciplinary Initiatives in Higher Education Law and Policy), programs on a wide variety of legal topics appear on almost every national conference schedule, many professional associations devote part of their Web sites to law and legislation (e.g., American College Personnel Association, National Association of Student Personnel Administrators, and the Association for Student Judicial Affairs), and private companies publish newsletters designed to inform their readers about the latest court rulings (e.g., The College Student and the Courts by Gehring and Letzring, Synfax weekly report by Pavela). Some of these resources examine events that may be several years old since litigation takes time and initial decisions may be appealed. Many of the authors of these publications restate the facts of the particular case and give some guidance on appropriate administrative practice. These resources, however, may not always be able to identify what administrators might face on their own campuses in the near future or define decision-making processes that might help administrators avoid legal pitfalls. The purpose of this paper is to identify two important mechanisms that college administrators can use to more actively anticipate the legal issues that may occur on their own campuses. First, practitioners should scan the broad legal environment. Secondly, they should be aware of legal issues in other parts of the education enterprise. Anticipating},
  archiveprefix = {arxiv},
  isbn = {978-85-7811-079-6},
  pmid = {25246403},
  keywords = {icle},
  file = {D\:\\Zotero\\storage\\4JBT6LM7\\Fallis - 2013 - Begining Perl for Bioinformatic.pdf;D\:\\Zotero\\storage\\C2Y4UFE3\\Tesfamarian, Goda - 2013 - Handbook of seismic risk analysis and management of civil ifrastructure systems.pdf;D\:\\Zotero\\storage\\DHIU2QIR\\Unknown - Unknown - No Title(59).pdf;D\:\\Zotero\\storage\\EMKGYFQQ\\Setiyawan - 2013 - 済無No Title No Title(4).pdf;D\:\\Zotero\\storage\\ETD8GUJS\\Brillinger et al. - 1981 - Specifying Statistical Models.pdf;D\:\\Zotero\\storage\\GIELTTIF\\Shroder, Wyss - 2014 - Earthquake Hazard, Risk, and Disasters.pdf;D\:\\Zotero\\storage\\ZEBGJDW3\\Reiss, Thomas - 2007 - Statistical Analysis of Extreme Values.pdf}
}

@misc{Jeffreys1950,
  title = {Methodos of {{Mathematical Physics}}},
  author = {Jeffreys, H. and Jeffreys, B. S.},
  year = {1950},
  publisher = {{Cambridge University Press}},
  file = {D\:\\Zotero\\storage\\3FEPV9NQ\\Jeffreys, Jeffreys - 1950 - Methodos of Mathematical Physics.pdf}
}

@book{JohnLaw1387,
  title = {Design and {{Analysis}} of {{Experiments}} with {{R}}},
  author = {Lawson, John},
  year = {2014},
  journal = {Design and Analysis of Experiments with R},
  doi = {10.1201/b17883},
  abstract = {After studying experimental design a researcher or statistician should be able to: (1) choose an experimental design that is appropriate for the research problem at hand; (2) construct the design (including performing proper ran- domization and determining the required number of replicates); (3) execute the plan to collect the data (or advise a colleague on how to do it); (4) deter- mine the model appropriate for the data; (5) fit the model to the data; and (6) interpret the data and present the results in a meaningful way to answer the research question. The purpose of this book is to focus on connecting the objectives of research to the type of experimental design required, describing the actual process of creating the design and collecting the data, showing how to perform the proper analysis of the data, and illustrating the interpreta- tion of results. Exposition on the mechanics of computation is minimized by relying on a statistical software package.},
  isbn = {978-1-4987-2848-5},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\XD5D4YFX\\Lawson - 2014 - Design and Analysis of Experiments with R.pdf}
}

@book{Jones2018,
  title = {Volume {{I}} - {{Principles}}, {{Process}} and {{Practice}} of {{Professional Number Juggling}}},
  author = {Jones, Alan R.},
  year = {2018},
  journal = {Principles, Process and Practice of Professional Number Juggling},
  doi = {10.4324/9781315160054},
  abstract = {Provides an overview of the main processes, methods and techniques that are used for building project and engineering estimates. Alan Jones outlines the principles of Best Practice Spreadsheet Modelling and discusses the concepts of Spreadsheet risks and Estimate Maturity. The text then considers the difference and relative relevance of Accuracy and Precision and how you should interpret these in relation to Primary and Secondary Estimate Drivers. As a general rule you want estimates to be accurate not precise. Introduction and objectives -- Methods, approaches, techniques and related terms -- Estimate TRACEability and health checks -- Primary and secondary drivers; accuracy and precision -- Factors, rates, ratios amd estimating by analogy -- Data normalisation-levelling the playing field -- Pseudo-quantitative qualitative estimating techniques -- Benford's Law as a potential measure of cost bias.},
  file = {D\:\\Zotero\\storage\\IN2PY3GA\\Jones - 2018 - Volume I - Principles, Process and Practice of Professional Number Juggling.epub}
}

@book{Jordaan2005a,
  title = {Decisions under Uncertainty: {{Probabilistic}} Analysis for Engineering Decisions},
  author = {Jordaan, Ian},
  year = {2005},
  journal = {Decisions Under Uncertainty: Probabilistic Analysis for Engineering Decisions},
  volume = {9780521782},
  doi = {10.1017/CBO9780511804861},
  abstract = {To better understand the core concepts of probability and to see how they affect real-world decisions about design and system performance, engineers and scientists might want to ask themselves the following questions: {$\cdot$} What exactly is meant by probability? {$\cdot$} What is the precise definition of the 100-year load and how is it calculated? {$\cdot$} What is an ``extremal'' probability distribution? {$\cdot$} What is the Bayesian approach? {$\cdot$} How is utility defined? {$\cdot$} How do games fit into probability theory? {$\cdot$} What is entropy? {$\cdot$} How do I apply these ideas in risk analysis? Starting from the most basic assumptions, this book develops a coherent theory of probability and broadens it into applications in decision theory, design, and risk analysis. This book is written for engineers and scientists interested in probability and risk. It can be used by undergraduates, graduate students, or practicing engineers.},
  isbn = {978-0-511-80486-1},
  file = {D\:\\Zotero\\storage\\G4N8GUVW\\Jordaan - 2005 - Decisions under uncertainty Probabilistic analysis for engineering decisions.pdf}
}

@article{Kabir2015,
  title = {Integrating {{Bayesian Linear Regression}} with {{Ordered Weighted Averaging}}: {{Uncertainty Analysis}} for {{Predicting Water Main Failures}}},
  author = {Kabir, Golam and Tesfamariam, Solomon and Loeppky, Jason and Sadiq, Rehan},
  year = {2015},
  journal = {ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems},
  volume = {1},
  number = {3},
  pages = {04015007},
  issn = {2376-7642},
  doi = {10.1061/ajrua6.0000820},
  abstract = {\textcopyright{} 2015 American Society of Civil Engineers. Water distribution networks (WDNs) are among the most important and expensive municipal infrastructure assets that are vital to public health. Municipal authorities strive for implementing preventive (or proactive) programs rather than corrective (or reactive) programs. The ability to predict the failure of pipes in WDNs is vital in the proactive investment planning of replacement and rehabilitation strategies. However, due to inherent uncertainties in data and modeling, WDN failure prediction is challenging. To improve understanding of water main failure processes, accurate quantification of uncertainty is necessary. The research reported in this paper presents a comparative evaluation of the prediction accuracy of normal multiple linear regression and Bayesian regression models using water mains failure data/information from the City of Calgary. Results indicate that Bayesian regression models provide better predicted response and handle the uncertainty more accurately than normal regression model.},
  file = {D\:\\Zotero\\storage\\3PP3KLCG\\Kabir et al. - 2015 - Integrating Bayesian Linear Regression with Ordered Weighted Averaging Uncertainty Analysis for Predicting Water M.pdf}
}

@book{Kassambara2017a,
  title = {Practical {{Guide To Cluster Analysis}} in {{R}} - {{Unsupervised Machine Learning}}},
  author = {Kassambara, Alboukadel},
  year = {2017},
  abstract = {Large amounts of data are collected every day from satellite images, bio-medical, security, marketing, web search, geo-spatial or other automatic equipment. Mining knowledge from these big data far exceeds human's abilities. Clustering is one of the important data mining methods for discovering knowledge in multidimensional data. The goal of clustering is to identify pattern or groups of similar objects within a data set of interest. In the litterature, it is referred as ``pattern recognition'' or ``unsupervised machine learning'' - ``unsupervised'' because we are not guided by a priori ideas of which variables or samples belong in which clusters. ``Learning'' because the machine algorithm ``learns'' how to cluster.},
  isbn = {1-5424-6270-3},
  file = {D\:\\Zotero\\storage\\J4GEH8RM\\Kassambara - 2017 - Practical Guide To Cluster Analysis in R - Unsupervised MAchine Learning.pdf}
}

@book{Kassambara2017b,
  title = {Practical {{Guide To Principal Component Methods}} in {{R}}: {{PCA}}, {{M}} ({{CA}}), {{FAMD}}, {{MFA}}, {{HCPC}}, Factoextra.},
  author = {Kassambara, Alboukadel},
  year = {2017},
  volume = {2},
  abstract = {This tutorial explains the concept of principal component analysis used for extracting important variables from a data set in R and Python},
  isbn = {9781975721138},
  file = {D\:\\Zotero\\storage\\DECVARZ5\\Kassambara - 2017 - Practical Guide To Principal Component Methods in R PCA, M (CA), FAMD, MFA, HCPC, factoextra.pdf}
}

@article{keyhaniClosedLoopPredictive2000,
  title = {Closed {{Loop Predictive}} Optimal {{Control Algorithm Using ARMA Models}}},
  author = {Keyhani and Allam},
  year = {2000},
  number = {June},
  file = {D\:\\Zotero\\storage\\XJTUV94G\\Odels - 2000 - Arma m.pdf}
}

@book{Kianifard1995,
  title = {Logistic {{Regression}}: {{A Self-Learning Text}}},
  author = {Kianifard, Farid},
  year = {1995},
  journal = {Technometrics},
  volume = {37},
  issn = {0040-1706},
  doi = {10.1080/00401706.1995.10485899},
  abstract = {Springer},
  isbn = {978-1-4419-1741-6},
  file = {D\:\\Zotero\\storage\\2S8P866A\\Kianifard - 1995 - Logistic Regression A Self-Learning Text.pdf}
}

@book{klemel2014,
  title = {Multivariate {{Nonparametric Regression}} and {{Visualization}}},
  author = {Klemel, Jussi},
  langid = {english},
  file = {D\:\\Zotero\\storage\\9ZLVDIUJ\\Klemel - Multivariate Nonparametric Regression and Visualiz.pdf}
}

@book{Kloke2014,
  title = {The {{R Series Statistics Nonparametric Statistical Methods Using R}}},
  author = {Kloke, John and Mckean, Joseph W},
  year = {2014},
  journal = {The R Series},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {Nonparametric Statistical Methods Using R covers traditional nonparamet-ric methods and rank-based analyses, including estimation and inference for models ranging from simple location models to general linear and nonlinear models for uncorrelated and correlated responses. The authors emphasize applications and statistical computation. They illustrate the methods with many real and simulated data examples using R, including the packages Rfit and npsm. The book first gives an overview of the R language and basic statistical concepts before discussing nonparametrics. It presents rank-based methods for one-and two-sample problems, procedures for regression models, computation for general fixed-effects ANOVA and ANCOVA models, and time-to-event analyses. The last two chapters cover more advanced material, including high breakdown fits for general regression models and rank-based inference for cluster correlated data. Features \textbullet{} Explains how to apply and compute nonparametric methods, such as Wilcoxon procedures and bootstrap methods \textbullet{} Describes various types of rank-based estimates, including linear, nonlinear, time series, and basic mixed effects models \textbullet{} Illustrates the use of diagnostic procedures, including studentized residuals and difference in fits \textbullet{} Provides the R packages on CRAN, enabling you to reproduce all of the analyses \textbullet{} Includes exercises at the end of each chapter for self-study and classroom use John Kloke is a biostatistician and assistant scientist at the University of Wis-consin-Madison. He has held faculty positions at the University of Pittsburgh, Bucknell University, and Pomona College. An R user for more than 15 years, he is an author and maintainer of numerous R packages, including Rfit and npsm. Joseph W. McKean is a professor of statistics at Western Michigan University. He has co-authored several books and published many papers on nonpara-metric and robust statistical procedures. He is a fellow of the American Statistical Association.},
  file = {D\:\\Zotero\\storage\\XIUPL6CJ\\Kloke, Mckean - 2014 - The R Series Statistics Nonparametric Statistical Methods Using R.pdf}
}

@article{Klugel2007,
  title = {Error Inflation in {{Probabilistic Seismic Hazard Analysis}}},
  author = {Kl{\"u}gel, Jens Uwe},
  year = {2007},
  journal = {Engineering Geology},
  volume = {90},
  number = {3-4},
  pages = {186--192},
  issn = {00137952},
  doi = {10.1016/j.enggeo.2007.01.003},
  abstract = {Based on a consistent interpretation of earthquake occurrence as a stochastic process I demonstrate that the mathematical model of Probabilistic Seismic Hazard Analysis (PSHA) as it is in use today is inaccurate and leads to systematic errors in the calculation process. These mathematical errors may be regarded as an important contributor to the unrealistic results obtained by traditional PSHA for low probabilities of exceedance in recent projects. \textcopyright{} 2007 Elsevier B.V. All rights reserved.},
  keywords = {Error,Ground motion prediction,Probabilistic Seismic Hazard Analysis},
  file = {D\:\\Zotero\\storage\\RW255T5S\\Klügel - 2007 - Error inflation in Probabilistic Seismic Hazard Analysis.pdf}
}

@article{Klugel2007a,
  title = {How {{To Eliminate Non-Damaging Earthquakes From}} the {{Results}} of a {{Probabilistic Seismic Hazard Analysis}} ({{Psha}})},
  author = {Kl{\"u}gel, Jens-Uwe},
  year = {2007},
  journal = {SMiRT 19},
  pages = {1--8},
  abstract = {From 2000 to 2004 the Swiss nuclear power plants sponsored a large scale probabilistic seismic hazard analysis following the SSHAC-procedures at its most elaborated level 4. This research project was intended to be used as a support of the update of the Goesgen plant-specific PSA. A detailed review based on the development of site-specific controlling earthquakes indicated that the obtained results included the effects of non-damaging earthquakes, which may potentially cause significant accelerations at the plant site not exceeding the acknowledged limits of an operational earthquake as defined in NRC RG 1.166 by a threshold value of 0.16 gs for the cumulative absolute velocity (CAV). Therefore, a detailed procedure was developed to eliminate non-damaging earthquakes from the results of a SSHAC- level 4 PSHA. The procedure allows to calculate the probability of exceedance of the damaging threshold of an operational earthquake. A key element of the procedure consists in the use of a correlated bivariate Monte Carlo bootstrap technique to address the epistemic uncertainties associated with the definition of scenario earthquakes and the use of multiple equations for the calculation of CAV. The implementation of the procedure in the site specific seismic PSA leads to a significant reduction of the calculated core damage frequency. It was also demonstrated that the deterministic design basis Safe Shutdown Earthquake (SSE) for Goesgen matches the theoretical Safe Shutdown Earthquake derived from PEGASOS after application of the procedure to a reasonable extent.},
  file = {D\:\\Zotero\\storage\\DU2X53LI\\Klügel - 2007 - How To Eliminate Non-Damaging Earthquakes From the Results of a Probabilistic Seismic Hazard Analysis (Psha).pdf}
}

@article{Kozin1988,
  title = {Autoregressive Moving Average Models of Earthquake Records},
  author = {Kozin, F.},
  year = {1988},
  journal = {Probabilistic Engineering Mechanics},
  volume = {3},
  number = {2},
  pages = {58--63},
  issn = {02668920},
  doi = {10.1016/0266-8920(88)90016-1},
  abstract = {In this paper, we present a brief survey of literature concerned with fitting ARMA models to earthquake record data. This is, in a sense, relatively new for the earthquake record problem. The major conceptual difficulty is due to the nonstationary character of the data, and the fact that only single records are available from which to estimate the various coefficients. Thus, proper statistical methods must be developed and applied to account for this inherent problem. We briefly discuss a number of approaches that have been taken to generate statistically reliable and useful nonstationary models for earthquake engineering studies, and state a few problems of importance in the further development of this method. \textcopyright{} 1988.},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\HZ5E59M9\\Kozin - 1988 - Autoregressive moving average models of earthquake records.pdf}
}

@book{Kreiss2012,
  title = {Bootstrap {{Methods}} for {{Time Series}}},
  author = {Kreiss, Jens Peter and Lahiri, Soumendra Nath},
  year = {2012},
  journal = {Handbook of Statistics},
  volume = {30},
  publisher = {{Elsevier B.V.}},
  issn = {01697161},
  doi = {10.1016/B978-0-444-53858-1.00001-6},
  abstract = {The chapter gives a review of the literature on bootstrap methods for time series data. It describes various possibilities on how the bootstrap method, initially introduced for independent random variables, can be extended to a wide range of dependent variables in discrete time, including parametric or nonparametric time series models, autoregressive and Markov processes, long range dependent time series and nonlinear time series, among others. Relevant bootstrap approaches, namely the intuitive residual bootstrap and Markovian bootstrap methods, the prominent block bootstrap methods as well as frequency domain resampling procedures, are described. Further, conditions for consistent approximations of distributions of parameters of interest by these methods are presented. The presentation is deliberately kept non-technical in order to allow for an easy understanding of the topic, indicating which bootstrap scheme is advantageous under a specific dependence situation and for a given class of parameters of interest. Moreover, the chapter contains an extensive list of relevant references for bootstrap methods for time series. \textcopyright{} 2012 Elsevier B.V.},
  isbn = {978-0-444-53858-1},
  keywords = {Bootstrap methods,Discrete Fourier transform,Linear and nonlinear time series,Long range dependence,Markov chains,Resampling,Second order correctness,Stochastic processes},
  file = {D\:\\Zotero\\storage\\PVQYLNX4\\Kreiss, Lahiri - 2012 - Bootstrap Methods for Time Series.pdf}
}

@book{kuhnAppliedPredictiveModeling2013,
  title = {Applied {{Predictive Modeling}}},
  author = {Kuhn, Max and Johnson, Kjell},
  year = {2013},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-6849-3},
  urldate = {2023-07-24},
  isbn = {978-1-4614-6848-6 978-1-4614-6849-3},
  langid = {english},
  file = {D\:\\Zotero\\storage\\F85CZ3L6\\Kuhn and Johnson - 2013 - Applied Predictive Modeling.pdf}
}

@book{kumarDataMiningKnowledge2008,
  title = {Data {{Mining}} and {{Knowledge Discovery Series}}},
  author = {Kumar, Vipin},
  year = {2008},
  publisher = {{Chapman \& Hall/CRC}},
  langid = {english},
  file = {D\:\\Zotero\\storage\\QFQL5QGX\\Kumar - 2008 - Chapman & HallCRC Data Mining and Knowledge Disco.pdf}
}

@book{Kuonen2004,
  title = {Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis},
  author = {Harrell, Frank},
  year = {2004},
  doi = {10.1007/978-3-319-19425-7},
  abstract = {There are many books that are excellent sources of knowledge about individual stastical tools (survival models, general linear models, etc.), but the art of data analysis is about choosing and using multiple tools. In the words of Chatfield "...students typically know the technical details of regressin for example, but not necessarily when and how to apply it. This argues the need for a better balance in the literature and in statistical teaching between techniques and problem solving strategies." Whether analyzing risk factors, adjusting for biases in observational studies, or developing predictive models, there are common problems that few regression texts address. For example, there are missing data in the majority of datasets one is likely to encounter (other than those used in textbooks!) but most regression texts do not include methods for dealing with such data effectively, and texts on missing data do not cover regression modeling.},
  isbn = {978-3-319-19424-0},
  file = {D\:\\Zotero\\storage\\W8HU87KL\\Harrell - 2004 - Regression modeling strategies with applications to linear models, logistic regression, and survival analysis.pdf}
}

@article{Kuwada2009a,
  title = {Laplace Approximation for Stochastic Line Integrals},
  author = {Kuwada, Kazumasa},
  year = {2009},
  journal = {Probability Theory and Related Fields},
  volume = {144},
  number = {1-2},
  pages = {1--51},
  issn = {01788051},
  doi = {10.1007/s00440-008-0140-3},
  abstract = {We prove a precision of large deviation principle for current-valued processes such as shown in Bolthausen et al. (Ann Probab 23(1):236-267, 1995) for mean empirical measures. The class of processes we consider is determined by the martingale part of stochastic line integrals of 1-forms on a compact Riemannian manifold. For the pair of the current-valued process and mean empirical measures, we give an asymptotic evaluation of a nonlinear Laplace transform under a nondegeneracy assumption on the Hessian of the exponent at equilibrium states. As a direct consequence, our result implies the Laplace approximation for stochastic line integrals or periodic diffusions. In particular, we recover a result in Bolthausen et al. (Ann Probab 23(1):236-267, 1995) in our framework. \textcopyright{} 2008 Springer-Verlag.},
  keywords = {Empirical measure,Laplace approximation,Large deviation,Stochastic line integral},
  file = {D\:\\Zotero\\storage\\CUSQ43T9\\Kuwada - 2009 - Laplace approximation for stochastic line integrals.pdf}
}

@article{L.F.Richardson1910,
  title = {The Approximate Arithmetical Solution by Finite Differences with an Application to Stresses in Masonry Dams},
  author = {Richardson, L F},
  year = {1911},
  journal = {Philosophical Transactions of the Royal Society of America},
  volume = {210},
  pages = {307--357},
  file = {D\:\\Zotero\\storage\\CZZXSL5X\\Richardson - 1911 - The approximate arithmetical solution by finite differences with an application to stresses in masonry dams.pdf}
}

@book{Learning,
  title = {New Introduction to Multiple Time Series Analysis},
  author = {L{\"u}tkepohl, Helmut},
  year = {2005},
  journal = {New introduction to Multiple Time Series Analysis},
  doi = {10.1007/978-3-540-27752-1},
  abstract = {This is the new and totally revised edition of Ltkepohl's classic 1991 work. It provides a detailed introduction to the main steps of analyzing multiple time series, model specification, estimation, model checking, and for using the models for economic analysis and forecasting. The book now includes new chapters on cointegration analysis, structural vector autoregressions, cointegrated VARMA processes and multivariate ARCH models. Different procedures for model selection and model specification are treated and a wide range of tests and criteria for model checking are introduced. Causality analysis, impulse response analysis and innovation accounting are presented as tools for structural analysis. It bridges the gap to the difficult technical literature on the topic as it is a highly accessible and user-friendly work. In addition, multiple time series courses in other fields such as statistics and engineering may be based on it. \textcopyright{} Springer-Verlag Berlin Heidelberg 2005.},
  isbn = {3-540-40172-5},
  file = {D\:\\Zotero\\storage\\TIPQK9YZ\\Lütkepohl - 2005 - New introduction to multiple time series analysis.pdf}
}

@book{Learninga,
  title = {Nonlinear Time Series Analysis},
  author = {Tsay, Ruey S. and Chen, Rong},
  year = {2018},
  journal = {Nonlinear Time Series Analysis},
  issn = {0218-348X},
  doi = {10.1002/9781119514312},
  abstract = {A comprehensive resource that draws a balance between theory and applications of nonlinear time series analysis Nonlinear Time Series Analysis offers an important guide to both parametric and nonparametric methods, nonlinear state-space models, and Bayesian as well as classical approaches to nonlinear time series analysis. The authors-noted experts in the field-explore the advantages and limitations of the nonlinear models and methods and review the improvements upon linear time series models. The need for this book is based on the recent developments in nonlinear time series analysis, statistical learning, dynamic systems and advanced computational methods. Parametric and nonparametric methods and nonlinear and non-Gaussian state space models provide a much wider range of tools for time series analysis. In addition, advances in computing and data collection have made available large data sets and high-frequency data. These new data make it not only feasible, but also necessary to take into consideration the nonlinearity embedded in most real-world time series. This vital guide: \textbullet{} Offers research developed by leading scholars of time series analysis \textbullet{} Presents R commands making it possible to reproduce all the analyses included in the text \textbullet{} Contains real-world examples throughout the book \textbullet{} Recommends exercises to test understanding of material presented \textbullet{} Includes an instructor solutions manual and companion website Written for students, researchers, and practitioners who are interested in exploring nonlinearity in time series, Nonlinear Time Series Analysis offers a comprehensive text that explores the advantages and limitations of the nonlinear models and methods and demonstrates the improvements upon linear time series models.},
  isbn = {978-1-119-51431-2},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\CXCT338W\\Tsay, Chen - 2018 - Nonlinear time series analysis.pdf}
}

@article{Lee1999a,
  title = {The {{Normal}} and {{Lognormal Distributions}}},
  author = {Lee, Cheng F and Lee, John C and Lee, Alice C},
  year = {1999},
  journal = {Statistics for Business and Financial Economics},
  pages = {246--297},
  doi = {10.1142/9789812816214_0007},
  abstract = {The basic properties of the normal and lognormal distributions, with full proofs. We assume familiarity with elementary probability theory and with college-level calculus.},
  file = {D\:\\Zotero\\storage\\5MAYC2X2\\Lee, Lee, Lee - 1999 - The Normal and Lognormal Distributions.pdf}
}

@book{Lehmann1998a,
  title = {Theory of {{Point Estimation}} , {{Second Edition Springer Texts}} in {{Statistics}}},
  author = {Lehmann, E L and Casella, G},
  year = {1998},
  journal = {Design},
  volume = {41},
  issn = {00401706},
  doi = {10.2307/1270597},
  abstract = {This second, much enlarged edition by Lehmann and Casella of Lehmann's classic text on point estimation maintains the outlook and general style of the first edition. All of the topics are updated. An entirely new chapter on Bayesian and hierarchical Bayesian approaches is provided, and there is much new material on simultaneous estimation. Each chapter concludes with a Notes section which contains suggestions for further study. The book is a companion volume to the second edition of Lehmann's "Testing Statistical Hypotheses". E.L. Lehmann is Professor Emeritus at the University of California, Berkeley. He is a member of the National Academy of Sciences and the American Academy of Arts and Sciences, and the recipient of honorary degrees from the University of Leiden, The Netherlands, and the University of Chicago. George Casella is the Liberty Hyde Bailey Professor of Biological Statistics in The College of Agriculture and Life Sciences at Cornell University. Casella has served as associate editor of The American Statistician, Statistical Science and JASA. He is currently the Theory and Methods Editor of JASA. Casella has authored two other textbooks (Statistical Inference, 1990, with Roger Berger and Variance Components, 1992, with Shayle A. Searle and Charles McCulloch). He is a fellow of the IMS and ASA, and an elected fellow of the ISI.Also available:E.L. Lehmann, Testing Statistical Hypotheses Second Edition, Springer-Verlag New York, Inc., ISBN 0-387-949194.},
  isbn = {0-387-98502-6},
  pmid = {3087590},
  file = {D\:\\Zotero\\storage\\7FAJLB68\\Lehmann, Casella - 1998 - Theory of Point Estimation , Second Edition Springer Texts in Statistics.pdf}
}

@book{lehmannTestingStatisticalHypotheses2008,
  title = {Testing Statistical Hypotheses},
  author = {Lehmann, Erich L. and Romano, Joseph P.},
  year = {2008},
  series = {Springer Texts in Statistics},
  edition = {3. ed., correct. at 4. print},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-98864-1},
  langid = {english},
  file = {D\:\\Zotero\\storage\\KE42T2EZ\\Lehmann and Romano - 2008 - Testing statistical hypotheses.pdf}
}

@misc{LermaAnaM.QuesadaJoseM.ySanchez,
  title = {Introducci\'on a Las Funciones de Varias Variables},
  author = {{Lerma, Ana M. , Quesada, Jos{\'e} M. y S{\'a}nchez}, R.},
  pages = {4--7},
  abstract = {La asignatura de Ampliaci\'on de Matem\'aticas para el grado de ingenier\'ia, estudia entre otros apartados, la integraci\'on m\'ultiple (integrales dobles e integrales triples), Geometr\'ia Diferencial (estudio de curvas y superficies) y las integrales de linea y de superficie. Para una correcta comprensi\'on de estos temas es necesario poseer un conocimiento, si no profundo, s\'i escogido, de la teor\'ia de funciones de varias variables. Para trabajar con los dominios de este tipo de funciones necesitaremos una peque\~na iniciaci\'on a la topolog\'ia del espacio eucl\'ideo que nos permita conocer los conceptos de conjunto abierto, conjunto cerrado, interior de un conjunto, ..., que tanto aparecen en toda la bibliograf\'ia que el alumno va a encontrar de la asignatura. A lo largo de estos temas ser\'an muy frecuentes los casos en que sea necesario derivar funciones de varias variables y, m\'as precisamente, derivar la composici\'on de funciones de este tipo. Problemas tan sencillos como por ejemplo "Dada una curva C de ecuaciones impl\'icitas F(x, y, z) = 0, G(x, y, z) = 0, encontrar la ecuaci\'on de la recta tangente a la curva en un punto no singular de dicha curva" o el de calcular un jacobiano para un cambio de variables que no venga dado de forma expl\'icita, necesitan del uso del teorema de la funci\'on impl\'icita (o de una consecuencia de \'el como es el teorema de la funci\'on inversa). Por tanto, dedicaremos un breve espacio en este primer tema a ver los enunciados de estos teoremas as\'i como a realizar algunos ejercicios sobre este tema.},
  file = {D\:\\Zotero\\storage\\L8W6KMCI\\Lerma, Ana M. , Quesada, José M. y Sánchez - Unknown - Introducción a las funciones de varias variables.pdf}
}

@book{lewis100StatisticalTests,
  title = {100 {{Statistical Tests}} in {{R}}},
  author = {Lewis, N D},
  langid = {english},
  file = {D\:\\Zotero\\storage\\BP2ZUM78\\Lewis - 100 Statistical Tests in R.pdf}
}

@article{leysDetectingOutliersNot2013,
  title = {Detecting Outliers: {{Do}} Not Use Standard Deviation around the Mean, Use Absolute Deviation around the Median},
  shorttitle = {Detecting Outliers},
  author = {Leys, Christophe and Ley, Christophe and Klein, Olivier and Bernard, Philippe and Licata, Laurent},
  year = {2013},
  month = jul,
  journal = {Journal of Experimental Social Psychology},
  volume = {49},
  number = {4},
  pages = {764--766},
  issn = {00221031},
  doi = {10.1016/j.jesp.2013.03.013},
  urldate = {2023-07-24},
  abstract = {A survey revealed that researchers still seem to encounter difficulties to cope with outliers. Detecting outliers by determining an interval spanning over the mean plus/minus three standard deviations remains a common practice. However, since both the mean and the standard deviation are particularly sensitive to outliers, this method is problematic. We highlight the disadvantages of this method and present the median absolute deviation, an alternative and more robust measure of dispersion that is easy to implement. We also explain the procedures for calculating this indicator in SPSS and R software.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\632LAIX7\\Leys et al. - 2013 - Detecting outliers Do not use standard deviation .pdf}
}

@article{Li2020,
  title = {A Novel Method of Seismic Signal Detection Using Waveform Features},
  author = {Li, Jian and He, Mengmin and Cui, Gaofeng and Wang, Xiaoming and Wang, Weidong and Wang, Juan},
  year = {2020},
  journal = {Applied Sciences (Switzerland)},
  volume = {10},
  number = {8},
  issn = {20763417},
  doi = {10.3390/APP10082919},
  abstract = {The detection of seismic signals is vital in seismic data processing and analysis. Many algorithms have been proposed to resolve this issue, such as the ratio of short-term and long-term power averages (STA/LTA), F detector, Generalize F, and etc. However, the detection performance will be affected by the noise signals severely. In this paper, we propose a novel seismic signal detection method based on the historical waveform features to improve the seismic signals detection performance and reduce the affection from the noise signals. We use the historical events location information in a specific area and waveform features information to build the joint probability model. For the new signal from this area, we can determine whether it is the seismic signal according to the value of the joint probability. The waveform features used to construct the model include the average spectral energy on a specific frequency band, the energy of the component obtained by decomposing the signal through empirical mode decomposition (EMD), and the peak and the ratio of STA/LTA trace. We use the Gaussian process (GP) to build each feature model and finally get a multi-features joint probability model. The historical events location information is used as the kernel of the GP, and the historical waveform features are used to train the hyperparameters of GP. The beamforming data of the seismic array KSRS of International Monitoring System are used to train and test the model. The testing results show the effectiveness of the proposed method.},
  keywords = {\ding{72},Detection performance,Gaussian process,Joint probability model,STA/LTA,Waveform features},
  file = {D\:\\Zotero\\storage\\SZ8EZDDW\\Li et al. - 2020 - A novel method of seismic signal detection using waveform features.pdf}
}

@article{Lieblein1953,
  title = {On the {{Exact Evaluation}} of the {{Variances}} and {{Covariances}} of {{Order Statistics}} in {{Samples}} from the {{Extreme-Value Distribution}}},
  author = {Lieblein, J.},
  year = {1953},
  journal = {The Annals of Mathematical Statistics},
  volume = {24},
  number = {2},
  pages = {282--287},
  keywords = {\ding{72},icle},
  file = {D\:\\Zotero\\storage\\W4WTVHM9\\Lieblein - 1953 - On the Exact Evaluation of the Variances and Covariances of Order Statistics in Samples from the Extreme-Value Distrib.pdf}
}

@article{Lieblein1957a,
  title = {Table of the First Moment of Ranked Extremes},
  author = {Lieblein, J. and Salzer, H.E.},
  year = {1957},
  journal = {Journal of Research of the National Bureau of Standards},
  volume = {59},
  number = {3},
  pages = {203},
  issn = {0091-0635},
  doi = {10.6028/jres.059.020},
  file = {D\:\\Zotero\\storage\\GZQEWKGH\\Lieblein, Salzer - 1957 - Table of the first moment of ranked extremes.pdf}
}

@article{Liel2009,
  title = {Incorporating Modeling Uncertainties in the Assessment of Seismic Collapse Risk of Buildings},
  author = {Liel, Abbie B. and Haselton, Curt B. and Deierlein, Gregory G. and Baker, Jack W.},
  year = {2009},
  journal = {Structural Safety},
  volume = {31},
  number = {2},
  pages = {197--211},
  publisher = {{Elsevier Ltd}},
  issn = {01674730},
  doi = {10.1016/j.strusafe.2008.06.002},
  abstract = {The primary goal of seismic provisions in building codes is to protect life safety through the prevention of structural collapse. To evaluate the extent to which current and past building code provisions meet this objective, the authors have conducted detailed assessments of collapse risk of reinforced-concrete moment frame buildings, including both 'ductile' frames that conform to current building code requirements, and 'non-ductile' frames that are designed according to out-dated (pre-1975) building codes. Many aspects of the assessment process can have a significant impact on the evaluated collapse performance; this study focuses on methods of representing modeling parameter uncertainties in the collapse assessment process. Uncertainties in structural component strength, stiffness, deformation capacity, and cyclic deterioration are considered for non-ductile and ductile frame structures of varying heights. To practically incorporate these uncertainties in the face of the computationally intensive nonlinear response analyses needed to simulate collapse, the modeling uncertainties are assessed through a response surface, which describes the median collapse capacity as a function of the model random variables. The response surface is then used in conjunction with Monte Carlo methods to quantify the effect of these modeling uncertainties on the calculated collapse fragilities. Comparisons of the response surface based approach and a simpler approach, namely the first-order second-moment (FOSM) method, indicate that FOSM can lead to inaccurate results in some cases, particularly when the modeling uncertainties cause a shift in the prediction of the median collapse point. An alternate simplified procedure is proposed that combines aspects of the response surface and FOSM methods, providing an efficient yet accurate technique to characterize model uncertainties, accounting for the shift in median response. The methodology for incorporating uncertainties is presented here with emphasis on the collapse limit state, but is also appropriate for examining the effects of modeling uncertainties on other structural limit states. \textcopyright{} 2008 Elsevier Ltd. All rights reserved.},
  keywords = {Collapse,Modeling uncertainties,Monte Carlo,Performance-based earthquake engineering,Response surface,Seismic reliability},
  file = {D\:\\Zotero\\storage\\NBSQF4FV\\Liel et al. - 2009 - Incorporating modeling uncertainties in the assessment of seismic collapse risk of buildings.pdf}
}

@misc{LiMaddalaBootstrapping,
  title = {Li \& {{Maddala}} - {{Bootstrapping}} Time Series Models},
  file = {D\:\\Zotero\\storage\\Z9LW6ASB\\Unknown - Unknown - Li & Maddala - Bootstrapping time series models.pptx}
}

@article{Lind1983a,
  title = {Modelling of Uncertainty in Discrete Dynamical Systems},
  author = {Lind, N. C.},
  year = {1983},
  journal = {Applied Mathematical Modelling},
  volume = {7},
  number = {3},
  pages = {146--152},
  issn = {0307904X},
  doi = {10.1016/0307-904X(83)90001-X},
  abstract = {Dynamic systems that are not Gaussian, stationary and linear are difficult to model by full probabilistic analysis. Sufficient information for practical application can often be obtained by second moment analysis, described in the paper. Alternatively, second moment analysis can be performed using point distributions. Two new methods in this class, one exact for linear systems and one approximate, are described. Examples show the application and illustrate the accuracy. \textcopyright{} 1983.},
  keywords = {dynamic,mathematical model,point distributions,probability,random processes,second moment,stochastic,systems,uncertainty},
  file = {D\:\\Zotero\\storage\\VFZDSRH7\\Lind - 1983 - Modelling of uncertainty in discrete dynamical systems.pdf}
}

@article{Lineales1995,
  title = {M {{ETODOS SECANTES PARA SISTEMAS DE ECUACIONES NO LINEALES Referencias}}},
  author = {Lineales, C O N Restricciones},
  year = {1995},
  number = {2},
  pages = {18--19},
  file = {D\:\\Zotero\\storage\\GK3YK97T\\Lineales - 1995 - M ETODOS SECANTES PARA SISTEMAS DE ECUACIONES NO LINEALES Referencias.pdf}
}

@book{LinShang2011,
  title = {Dynamic Linear Models with {{R}}},
  author = {Lin Shang, Han},
  year = {2011},
  journal = {Journal of Applied Statistics},
  volume = {38},
  issn = {0266-4763},
  doi = {10.1080/02664763.2010.517938},
  abstract = {-},
  isbn = {978-0-387-77237-0},
  file = {D\:\\Zotero\\storage\\7RIB8MD9\\Lin Shang - 2011 - Dynamic linear models with R.pdf}
}

@article{Liu2017a,
  title = {Reassessment of {{Dam Safety Using Bayesian Network}}},
  author = {Liu, Z Q and Ph, D and Nadim, F and Sc, D and Eidsvig, U K and Lacasse, S and Sc, D},
  year = {2017},
  number = {1996},
  pages = {168--177},
  file = {D\:\\Zotero\\storage\\Y2X6TNF4\\Liu et al. - 2017 - Reassessment of Dam Safety Using Bayesian Network.pdf}
}

@book{M.Last,
  title = {Data {{Mining}} in {{Time Series Databases}}},
  author = {Last, M. and Kandel, A. and Bunke, H.},
  isbn = {981-238-290-9},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\FTCV9MMQ\\Unknown - Unknown - No Title(63).pdf}
}

@book{maathuisHandbookGraphicalModels,
  title = {Handbook of {{Graphical Models}}},
  author = {Maathuis, Marloes and Drton, Mathias and Lauritzen, Steffen and Wainwright, Martin},
  langid = {english},
  file = {D\:\\Zotero\\storage\\XL9EBRD8\\Maathuis et al. - Handbook of Graphical Models.pdf}
}

@book{Maharaj2019,
  title = {Time {{Series Clustering}} and {{Classification}}},
  author = {Maharaj, Elizabeth Ann and D'Urso, Pierpaolo and Caiado, Jorge},
  year = {2019},
  doi = {10.1201/9780429058264},
  abstract = {The beginning of the age of artificial intelligence and machine learning has created new challenges and opportunities for data analysts, statisticians, mathematicians, econometricians, computer scientists and many others. At the root of these techniques are algorithms and methods for clustering and classifying different types of large datasets, including time series data. Time Series Clustering and Classification includes relevant developments on observation-based, feature-based and model-based traditional and fuzzy clustering methods, feature-based and model-based classification methods, and machine learning methods. It presents a broad and self-contained overview of techniques for both researchers and students. Features Provides an overview of the methods and applications of pattern recognition of time series Covers a wide range of techniques, including unsupervised and supervised approaches Includes a range of real examples from medicine, finance, environmental science, and more R and MATLAB code, and relevant data sets are available on a supplementary website Introduction -- Time series features and models -- Traditional cluster analysis -- Fuzzy clustering -- Observation-based clustering -- Feature-based clustering -- Model-based clustering -- Other time series clustering approaches -- Feature-based approaches -- Other time series classification approaches -- Software and data sets.},
  isbn = {978-1-4987-7321-8},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\A5GBXHA3\\Maharaj, D'Urso, Caiado - 2019 - Time Series Clustering and Classification.pdf;D\:\\Zotero\\storage\\TID5TV5A\\Maharaj, D'Urso, Caiado - 2019 - Time Series Clustering and Classification.pdf}
}

@book{maimonDataMiningKnowledge2010,
  title = {Data {{Mining}} and {{Knowledge Discovery Handbook}}},
  editor = {Maimon, Oded and Rokach, Lior},
  year = {2010},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-0-387-09823-4},
  urldate = {2023-06-27},
  isbn = {978-0-387-09822-7 978-0-387-09823-4},
  langid = {english},
  file = {D\:\\Zotero\\storage\\V2NQVFNU\\Maimon and Rokach - 2010 - Data Mining and Knowledge Discovery Handbook.pdf}
}

@book{Maiti1981b,
  title = {Applied {{Nonparametric Statistical Methods}}},
  author = {Sprent, P. and Smeeton, N.C.},
  year = {2007},
  keywords = {icle},
  file = {D\:\\Zotero\\storage\\7VDYYRM2\\Sprent, Smeeton - 2007 - Applied Nonparametric Statistical Methods.pdf}
}

@article{MarcA.,
  title = {Ignorance {{Factors Using Model Expansion}}},
  author = {Maes, Marc A.},
  journal = {Journal of Engineering Mechanics},
  file = {D\:\\Zotero\\storage\\69JANFK8\\Unknown - Unknown - ASCE-IJEM-1996-01-05.pdf.pdf}
}

@book{Markushevich1970,
  title = {Teoria de Las {{Funciones Analiticas}}},
  author = {Markushevich, A},
  year = {1970},
  volume = {2},
  pages = {646},
  abstract = {V. 1. El objeto de la teor\'ia. Los n\'umeros complejos. La derivabilidad y su significado geom\'etrico. Las funciones elementales. Integrales y series de potencias. Curvas rectificables integrales.Diversas series, residuos, funciones inversas e impl\'icitas},
  file = {D\:\\Zotero\\storage\\DAEHMYNP\\Markushevich - 1970 - Teoria de las Funciones Analiticas.pdf}
}

@article{Martelli2011,
  title = {Seismic Safety of High Risk Plants},
  author = {Martelli, A},
  year = {2011},
  pages = {1--5},
  keywords = {chemical plants,components,energy dissipation,nuclear plants,seismic isolation},
  file = {D\:\\Zotero\\storage\\FZ849HPP\\Martelli - 2011 - Seismic safety of high risk plants.pdf}
}

@article{Martin2010a,
  title = {Econometric Modelling with Time Series: {{Specification}}, Estimation and Testing},
  author = {Martin, Vance and Hurn, Stan and Harris, David},
  year = {2010},
  journal = {Econometric Modelling with Time Series: Specification, Estimation and Testing},
  pages = {1--887},
  doi = {10.1017/CBO9781139043205},
  abstract = {This book is a complete guide to the current state of geometric algebra with early chapters providing a self-contained introduction. Topics range from new techniques for handling rotations in arbitrary dimensions, the links between rotations, bivectors, the structure of the Lie groups, non-Euclidean geometry, quantum entanglement, and gauge theories. Applications such as black holes and cosmic strings are also explored.},
  isbn = {9781139043205},
  file = {D\:\\Zotero\\storage\\4D4TCL4R\\Martin, Hurn, Harris - 2010 - Econometric modelling with time series Specification, estimation and testing.pdf}
}

@book{Marwala2010,
  title = {Computational {{Intelligence}} for {{Missing Data Imputation}}, {{Estimation}}, and {{Management}}},
  author = {Marwala, Tshilidzi},
  year = {2010},
  journal = {Computational Intelligence for Missing Data Imputation, Estimation, and Management},
  doi = {10.4018/978-1-60566-336-4},
  abstract = {"This book is for those who use data analysis to build decision support systems, particularly engineers, scientists and statisticians"--Provided by publisher.},
  isbn = {978-1-60566-336-4},
  file = {D\:\\Zotero\\storage\\HJMMNSGQ\\Marwala - 2010 - Computational Intelligence for Missing Data Imputation, Estimation, and Management.pdf}
}

@book{maSupportVectorMachines2014,
  title = {Support {{Vector Machines Applications}}},
  editor = {Ma, Yunqian and Guo, Guodong},
  year = {2014},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-02300-7},
  urldate = {2023-07-23},
  isbn = {978-3-319-02299-4 978-3-319-02300-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\2IBQYZJD\\Ma and Guo - 2014 - Support Vector Machines Applications.pdf}
}

@article{mccullochSimpleConsistentEstimators1986,
  title = {Simple Consistent Estimators of Stable Distribution Parameters},
  author = {McCulloch, J. Huston},
  year = {1986},
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {15},
  number = {4},
  pages = {1109--1136},
  issn = {0361-0918},
  doi = {10.1080/03610918608812563},
  urldate = {2023-04-05},
  langid = {english},
  file = {D\:\\Zotero\\storage\\5PWXTD4X\\McCulloch - 1986 - Simple consistent estimators of stable distributio.pdf}
}

@book{Mensik2019,
  title = {Machine {{Learning Using R}}},
  author = {Ramasubramanian, Karthik and Singh, Abhishek},
  year = {2019},
  journal = {Frontiers in Artificial Intelligence and Applications},
  volume = {321},
  issn = {09226389},
  doi = {10.3233/FAIA200024},
  abstract = {In this paper we deal with machine learning methods and algorithms applied to the area of geographic data. First, we briefly introduce learning with a supervisor that is applied in our case. Then we describe the algorithm 'Framework' together with heuristic methods used in it. Definitions of particular geographic objects, i.e. their concepts, are formulated in our background theory Transparent Intensional Logic (TIL) as TIL constructions. These concepts serve as general hypotheses. Basic principles of supervised machine learning are generalization and specialization. Given a positive example, the learner generalizes, while after a nearmiss example specialization is applied. Heuristic methods deal with the way generalization and specialization are applied.},
  isbn = {978-1-64368-044-6},
  keywords = {Generalization,Heuristics,Hypothesis,Machine learning,Specialization,Til,Transparent intensional logic},
  file = {D\:\\Zotero\\storage\\XH7SNUDC\\Ramasubramanian, Singh - 2019 - Machine Learning Using R.pdf}
}

@article{Mentaschi2016b,
  title = {Non-Stationary {{Extreme Value Analysis}}: A Simplified Approach for {{Earth}} Science Applications},
  author = {Mentaschi, Lorenzo and Vousdoukas, Michalis and Voukouvalas, Evangelos and Sartini, Ludovica and Feyen, Luc and Besio, Giovanni and Alfieri, Lorenzo},
  year = {2016},
  journal = {Hydrology and Earth System Sciences Discussions},
  number = {February},
  pages = {1--38},
  issn = {1812-2116},
  doi = {10.5194/hess-2016-65},
  abstract = {Statistical approaches to study extreme events require by definition long time series of data. The climate is subject to natural and anthropogenic variations at different temporal scales, leaving their footprint on the frequency and intensity of climatic and hydrological extremes, therefore assumption of stationarity is violated and alternative methods to conventional stationary Extreme Value Analysis (EVA) need to be adopted. In this study we introduce the Transformed-Stationary (TS) methodology for non-stationary EVA. This approach consists in (i) transforming a non-stationary time series into a stationary one to which the stationary EVA theory can be applied; and (ii) reverse-transforming the result into a non-stationary extreme value distribution. As a transformation we propose and discuss a simple time-varying normalization of the signal and show that it allows a comprehensive formulation of non stationary GEV/GPD models with constant shape parameter. A validation of the methodology is carried out on time series of significant wave height, residual water level, and river discharge, which show varying degrees of long-term and seasonal variability. The results from the proposed approach are comparable with the ones from (a) a stationary EVA on quasi-stationary slices of non stationary series and (b) the previously applied non stationary EVA approach. However, the proposed technique comes with advantages in both cases, as in contrast to (a) it uses the whole time horizon of the series for the estimation of the extremes, allowing for a more accurate estimation of large return levels; and with respect to (b) it decouples the detection of non-stationary patterns from the fitting of the extreme values distribution. As a result the steps of the analysis are simplified and intermediate diagnostics are possible. In particular the transformation can be carried out by means of simple statistical techniques such as low-pass filters based on running mean and standard deviation, and the fitting procedure is a stationary one with a few degrees of freedom and easy to implement and control. An open-source MATLAB toolbox has been developed to cover this methodology, available at https://bitbucket.org/menta78/tseva.},
  file = {D\:\\Zotero\\storage\\Q8YCQM3X\\Mentaschi et al. - 2016 - Non-stationary Extreme Value Analysis a simplified approach for Earth science applications.pdf}
}

@article{Mentaschi2016c,
  title = {The Transformed-Stationary Approach: {{A}} Generic and Simplified Methodology for Non-Stationary Extreme Value Analysis},
  author = {Mentaschi, Lorenzo and Vousdoukas, Michalis and Voukouvalas, Evangelos and Sartini, Ludovica and Feyen, Luc and Besio, Giovanni and Alfieri, Lorenzo},
  year = {2016},
  journal = {Hydrology and Earth System Sciences},
  volume = {20},
  number = {9},
  pages = {3527--3547},
  issn = {16077938},
  doi = {10.5194/hess-20-3527-2016},
  abstract = {Statistical approaches to study extreme events require, by definition, long time series of data. In many scientific disciplines, these series are often subject to variations at different temporal scales that affect the frequency and intensity of their extremes. Therefore, the assumption of stationarity is violated and alternative methods to conventional stationary extreme value analysis (EVA) must be adopted. Using the example of environmental variables subject to climate change, in this study we introduce the transformed-stationary (TS) methodology for non-stationary EVA. This approach consists of (i) transforming a non-stationary time series into a stationary one, to which the stationary EVA theory can be applied, and (ii) reverse transforming the result into a non-stationary extreme value distribution. As a transformation, we propose and discuss a simple time-varying normalization of the signal and show that it enables a comprehensive formulation of non-stationary generalized extreme value (GEV) and generalized Pareto distribution (GPD) models with a constant shape parameter. A validation of the methodology is carried out on time series of significant wave height, residual water level, and river discharge, which show varying degrees of long-term and seasonal variability. The results from the proposed approach are comparable with the results from (a) a stationary EVA on quasi-stationary slices of non-stationary series and (b) the established method for non-stationary EVA. However, the proposed technique comes with advantages in both cases. For example, in contrast to (a), the proposed technique uses the whole time horizon of the series for the estimation of the extremes, allowing for a more accurate estimation of large return levels. Furthermore, with respect to (b), it decouples the detection of non-stationary patterns from the fitting of the extreme value distribution. As a result, the steps of the analysis are simplified and intermediate diagnostics are possible. In particular, the transformation can be carried out by means of simple statistical techniques such as low-pass filters based on the running mean and the standard deviation, and the fitting procedure is a stationary one with a few degrees of freedom and is easy to implement and control. An open-source MATLAB toolbox has been developed to cover this methodology, which is available at {$<$}a hrefCombining double low line"https://github.com/menta78/tsEva/" targetCombining double low line"-blank"{$>$}https://github.com/menta78/tsEva/{$<$}/a{$>$} (Mentaschi et al., 2016).},
  file = {D\:\\Zotero\\storage\\APQGW9VY\\Mentaschi et al. - 2016 - The transformed-stationary approach A generic and simplified methodology for non-stationary extreme value anal.pdf}
}

@article{Method1992a,
  title = {{{POINT-ESTIMATE METHOD FOR CALCULATING STATISTICAL MOMENTS}}},
  author = {Li, K.S.},
  year = {1992},
  volume = {118},
  number = {7},
  pages = {1506--1511},
  file = {D\:\\Zotero\\storage\\BEC2DRWR\\Li - 1992 - POINT-ESTIMATE METHOD FOR CALCULATING STATISTICAL MOMENTS.pdf}
}

@book{Millard,
  title = {Environmental {{Statistics}} with {{S-Plus}}},
  author = {Millard, S. and Neerchal, N.},
  keywords = {icle},
  file = {D\:\\Zotero\\storage\\MNA2RBPK\\Millard, Neerchal - Unknown - Environmental Statistics with S-Plus.pdf}
}

@article{Mills1975,
  title = {The {{New Math}} of {{Computer Programming}}},
  author = {Mills, Harlan D.},
  year = {1975},
  journal = {Communications of the ACM},
  volume = {18},
  number = {1},
  pages = {43--48},
  issn = {15577317},
  doi = {10.1145/360569.360659},
  abstract = {Structured programming has proved to be an important methodology for systematic program design and development. Structured programs are identified as compound function expressions in the algebra of functions. The algebraic properties of these function expressions permit the reformulation (expansion as well as reduction) of a nested subexpression independently of its environment, thus modeling what is known as stepwise program refinement as well as program execution. Finally, structured programming is characterized in terms of the selection and solution of certain elementary equations defined in the algebra of functions. These solutions can be given in general formulas, each involving a single parameter, which display the entire freedom available in creating correct structured programs. \textcopyright{} 1975, ACM. All rights reserved.},
  keywords = {algebra of functions,program correctness,stepwise refinement,structured programming},
  file = {D\:\\Zotero\\storage\\EG7U4JJH\\Mills - 1975 - The New Math of Computer Programming.pdf}
}

@article{Mishra2000a,
  title = {Uncertainty Propagation Using the Point Estimate Method},
  author = {Mishra, S.},
  year = {2000},
  journal = {IAHS-AISH Publication},
  number = {265},
  pages = {292--296},
  issn = {01447815},
  abstract = {The point estimate method is presented as an alternative to Monte Carlo simulation and the first-order second-moment method for propagating uncertainty with models containing a limited number of uncertain inputs. In this method, the model is evaluated at a discrete set of points in the uncertain parameter space, with the mean/variance of model predictions computed using a weighted average of these functional evaluations. The methodology is illustrated using an analytical model of health risk arising from water-borne radionuclide migration from a repository. Estimates of mean and variance for model-predicted risk, as obtained with the point estimate method, are found to compare well with those from a detailed Monte Carlo simulation study.},
  file = {D\:\\Zotero\\storage\\SQC4FF74\\Mishra - 2000 - Uncertainty propagation using the point estimate method.pdf}
}

@article{Montero2014a,
  title = {{{TSclust}}: {{An R}} Package for Time Series Clustering},
  author = {Montero, Pablo and Vilar, Jos{\'e} A.},
  year = {2014},
  journal = {Journal of Statistical Software},
  volume = {62},
  number = {1},
  pages = {1--43},
  issn = {15487660},
  doi = {10.18637/jss.v062.i01},
  abstract = {Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity measure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to implement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.},
  keywords = {\ding{72},Clustering,Dissimilarity measure,Time series data,Validation indices},
  file = {D\:\\Zotero\\storage\\N3B8VAYC\\Montero, Vilar - 2014 - TSclust An R package for time series clustering.pdf}
}

@book{Montgomery1994a,
  title = {Applied {{Statistics}} and {{Probability}} for {{Engineers}}},
  author = {Montgomery, Douglas C. and Runger, George C.},
  year = {1994},
  journal = {European Journal of Engineering Education},
  volume = {19},
  issn = {14695898},
  doi = {10.1080/03043799408928333},
  isbn = {0-471-20454-4},
  file = {D\:\\Zotero\\storage\\7BSBLDI8\\Montgomery, Runger - 1994 - Applied Statistics and Probability for Engineers.pdf;D\:\\Zotero\\storage\\Q7RCF83S\\Snell, Montgomery, Runger - 1995 - Applied Statistics and Probability for Engineers.pdf}
}

@incollection{Montgomery2004,
  title = {Design and {{Analysis}} of {{Experiments}}: {{Solution Manual}}},
  booktitle = {Design and {{Analysis}} of {{Experiments}}},
  author = {Montgomery, Douglas},
  year = {2004},
  volume = {800},
  pages = {1--2},
  abstract = {Now in its 6th edition, this bestselling professional reference has helped over 100,000 engineers and scientists with the success of their experiments. Douglas Montgomery arms readers with the most effective approach for learning how to design, conduct, and analyze experiments that optimize performance in products and processes. He shows how to use statistically designed experiments to obtain information for characterization and optimization of systems, improve manufacturing processes, and design and develop new processes and products. You will also learn how to evaluate material alternatives in product design, improve the field performance, reliability, and manufacturing aspects of products, and conduct experiments effectively and efficiently.},
  isbn = {978-0-471-48735-7},
  file = {D\:\\Zotero\\storage\\4NRF52C8\\Montgomery - 2004 - Design and Analysis of Experiments Solution Manual.pdf}
}

@article{Mosegaard2002,
  title = {Monte {{Carlo}} Analysis of Inverse Problems},
  author = {Mosegaard, Klaus and Sambridge, Malcolm},
  year = {2002},
  journal = {Inverse Problems},
  volume = {18},
  number = {3},
  pages = {29--54},
  issn = {02665611},
  doi = {10.1088/0266-5611/18/3/201},
  abstract = {Monte Carlo methods have become important in analysis of nonlinear inverse problems where no analytical expression for the forward relation between data and model parameters is available, and where linearization is unsuccessful. In such cases a direct mathematical treatment is impossible, but the forward relation materializes itself as an algorithm allowing data to be calculated for any given model. Monte Carlo methods can be divided into two categories: the sampling methods and the optimization methods. Monte Carlo sampling is useful when the space of feasible solutions is to be explored, and measures of resolution and uncertainty of solution are needed. The Metropolis algorithm and the Gibbs sampler are the most widely used Monte Carlo samplers for this purpose, but these methods can be refined and supplemented in various ways of which the neighbourhood algorithm is a notable example. Monte Carlo optimization methods are powerful tools when searching for globally optimal solutions amongst numerous local optima. Simulated annealing and genetic algorithms have shown their strength in this respect, but they suffer from the same fundamental problem as the Monte Carlo sampling methods: no provably optimal strategy for tuning these methods to a given problem has been found, only a number of approximate methods.},
  file = {D\:\\Zotero\\storage\\NRGLR5IC\\Mosegaard, Sambridge - 2002 - Monte Carlo analysis of inverse problems.pdf}
}

@article{Muhanna2001,
  title = {{{UNCERTAINTY IN MECHANICS PROBLEMS}}\textemdash{{INTERVAL}}\textendash{{BASED APPROACH}}},
  author = {Muhanna, By Rafi L and Mullen, Robert L},
  year = {2001},
  number = {June},
  pages = {557--566},
  file = {D\:\\Zotero\\storage\\APRPDT4C\\Muhanna, Mullen - 2001 - U NCERTAINTY IN M ECHANICS P ROBLEMS — I NTERVAL – B ASED A PPROACH.pdf}
}

@book{Murtagh2015,
  title = {Handbook of {{Cluster Analysis}}},
  author = {Henning, Christian and Meila, Marina and Murtagh, Fionn and Rocci, Roberto},
  year = {2015},
  isbn = {978-1-4665-5189-3},
  file = {D\:\\Zotero\\storage\\T9U2RC5U\\Henning et al. - 2015 - Handbook of Cluster Analysis.pdf}
}

@article{Myung2003a,
  title = {Tutorial on Maximum Likelihood Estimation},
  author = {Myung, In Jae},
  year = {2003},
  journal = {Journal of Mathematical Psychology},
  volume = {47},
  number = {1},
  pages = {90--100},
  issn = {00222496},
  doi = {10.1016/S0022-2496(02)00028-7},
  abstract = {In this paper, I provide a tutorial exposition on maximum likelihood estimation (MLE). The intended audience of this tutorial are researchers who practice mathematical modeling of cognition but are unfamiliar with the estimation method. Unlike least-squares estimation which is primarily a descriptive tool, MLE is a preferred method of parameter estimation in statistics and is an indispensable tool for many statistical modeling techniques, in particular in non-linear modeling with non-normal data. The purpose of this paper is to provide a good conceptual explanation of the method with illustrative examples so the reader can have a grasp of some of the basic principles. \textcopyright{} 2003 Elsevier Science (USA). All rights reserved.},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\R8EAWBKZ\\Myung - 2003 - Tutorial on maximum likelihood estimation.pdf}
}

@article{Nau1982,
  title = {Simulating and Analyzing Artifical Non-Stationary Earthquake Ground Motions.},
  author = {Nau, R. F. and Oliver, R. M. and Pister, K. S.},
  year = {1980},
  journal = {Bull. Seismol. Soc. Am.},
  volume = {72},
  number = {2},
  pages = {615--636},
  issn = {0037-1106},
  doi = {10.1785/bssa0720020615},
  abstract = {This report describes models used to simulate earthquake accelerograms and analyses of these artifical accelerogram records for use in structural response studies. The artifical accelerogram records are generated by a class of linear difference equations which have been previously identified as suitable for describing strong ground motions. The major contributions of the report are the use of Kalman filters for estimating time-varying model parameters, and the development of an effective non-parametric method for estimating the variance envelopes of the accelerogram records. - from US Govt Reports Announcements, 12, 1981},
  isbn = {0080286283},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\BDA37JW9\\Nau, Oliver, Pister - 1980 - Simulating and analyzing artifical non-stationary earthquake ground motions.pdf}
}

@book{Nielsen2001,
  title = {Practical {{Time Series Analysis}}. {{Prediction}} with {{Statistics}} \& {{Machine Learning}}},
  author = {Nielsen, Aileen},
  year = {2001},
  volume = {21},
  abstract = {A time series is a set of ordered observations of an individual or event taken at different points in time. The study of time-ordered data is an important branch of statistics with applications as variable as medicine, engineering and economics. This text offers a practical hands-on look at how the analysis works, rather than a mere discussion of theory. A disk accompanying the book contains data which students can use in their analysis.},
  isbn = {978-1-4920-4165-8},
  file = {D\:\\Zotero\\storage\\QX9FV967\\Nielsen - 2001 - Practical Time Series Analysis. Prediction with Statistics & Machine Learning.epub}
}

@book{nolanUnivariateStableDistributions2020,
  title = {Univariate {{Stable Distributions}}: {{Models}} for {{Heavy Tailed Data}}},
  shorttitle = {Univariate {{Stable Distributions}}},
  author = {Nolan, John P.},
  year = {2020},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-52915-4},
  urldate = {2023-04-05},
  isbn = {978-3-030-52914-7 978-3-030-52915-4},
  langid = {english},
  file = {D\:\\Zotero\\storage\\QPDSVZKP\\Nolan - 2020 - Univariate Stable Distributions Models for Heavy .pdf}
}

@book{NonParametricStatisticalDiagnosis,
  title = {Non-{{Parametric Statistical Diagnosis}}: {{Problems}} and {{Methods}} ({{Mathematics}} and {{Its Applications}}): {{E}}. {{Brodsky}}, {{B}}.{{S}}. {{Darkhovsky}}: 9780792363286: {{Amazon}}.Com: {{Books}}},
  keywords = {0792363280,B.S. Darkhovsky,Business \& Economics / Econometrics,Change-point problems,Cybernetics \& systems theory,E. Brodsky,Econometrics,Family medicine,General,General practice,Mathematical Statistics,Mathematics,Mathematics : Probability \& Statistics - General,Mathematics / Applied,Mathematics / Probability \& Statistics / General,Mathematik / Wahrscheinlichkeitstheorie,Mathematische Statistik,Maths for scientists,Medical / Family \& General Practice,Medical / General,Non-Parametric Statistical Diagnosis: Problems and,Nonparametric statistics,Probability \& statistics,Probability \& Statistics - General,Science / System Theory,Science/Mathematics,Springer,Statistics,Stochastik,Systems theory},
  file = {D\:\\Zotero\\storage\\H6A6SCS5\\Unknown - Unknown - Non-Parametric Statistical Diagnosis Problems and Methods (Mathematics and Its Applications) E. Brodsky, B.S. Darkh.part}
}

@book{NumericalMethodsLaplace2007,
  title = {Numerical {{Methods}} for {{Laplace Transform Inversion}}},
  year = {2007},
  journal = {Numerical Methods for Laplace Transform Inversion},
  doi = {10.1007/978-0-387-68855-8},
  abstract = {The Laplace transform, as its name implies, can be traced back to the work of the Marquis Pierre-Simon de Laplace (1749-1827). Strange as it may seem no reference is made to Laplace transforms in Rouse Ball's ``A Short Account of the History of Mathematics''. Rouse Ball does refer to Laplace's contribution to Probability Theory and his use of the generating function.},
  isbn = {978-0-387-28261-9},
  file = {D\:\\Zotero\\storage\\5RCHI4MB\\Unknown - 2007 - Numerical Methods for Laplace Transform Inversion.pdf}
}

@book{Offline,
  title = {Extending the Linear Model with {{R}} Generalized Linear, Mixed Effects and Nonparametric Regression Models},
  author = {Faraway, Julian J.},
  year = {2016},
  publisher = {{CRC Press}},
  isbn = {978-1-4987-2098-4},
  file = {D\:\\Zotero\\storage\\JJHA7UJE\\Faraway - 2016 - Extending the linear model with R generalized linear, mixed effects and nonparametric regression models.pdf}
}

@techreport{Oliver,
  title = {Class of {{Models}} for {{Identification}} and {{Simulation}} of {{Earthquake Ground Motions}}.},
  author = {Oliver, R. M. and Pister, K. S.},
  year = {1979},
  journal = {Transactions of the International Conference on Structural Mechanics in Reactor Technology},
  volume = {K(a).},
  abstract = {This paper outlines the use of discrete, autoregressive/moving-average (ARMA) models for identification and estimation of parameters in models derived from analysis of uniformly digitized earthquake ground motion acceleration data. Such models are of equal generality as compared to continuous-time models and have a number of significant advantages for purposes of digital analysis and simulation. The structure of ARMA models is briefly described, their relation to continuous models noted, and results of their application to a number of recorded accelerograms summarized.},
  file = {D\:\\Zotero\\storage\\LYKKNCCW\\Oliver, Pister - 1979 - Class of Models for Identification and Simulation of Earthquake Ground Motions.pdf}
}

@article{OnderCetin2002,
  title = {Probabilistic Models for the Initiation of Seismic Soil Liquefaction},
  author = {Onder Cetin, K. and Der Kiureghian, Armen and Seed, Raymond B.},
  year = {2002},
  journal = {Structural Safety},
  volume = {24},
  number = {1},
  pages = {67--82},
  issn = {01674730},
  doi = {10.1016/S0167-4730(02)00036-X},
  abstract = {A Bayesian framework for probabilistic assessment of the initiation of seismic soil liquefaction is described. A database, consisting of post-earthquake field observations of soil performance, in conjunction with in situ "index" test results is used for the development of probabilistically-based seismic soil liquefaction initiation correlations. The proposed stochastic model allows full and consistent representation of all relevant uncertainties, including (a) measurement/estimation errors, (b) model imperfection, (c) statistical uncertainty, and (d) inherent variabilities. Different sets of probabilistic liquefaction boundary curves are developed for the seismic soil liquefaction initiation hazard problem, representing various sources of uncertainty that are intrinsic to the problem. The resulting correlations represent a significant improvement over prior efforts, producing predictive relationships with enhanced accuracy and greatly reduced overall model uncertainty. \textcopyright{} 2002 Elsevier Science Ltd. All rights reserved.},
  keywords = {Bayesian updating,Choice-based sampling,Limit state models,Liquefaction,Liquefaction hazard,Model error,Parameter uncertainty,Statistical uncertainty},
  file = {D\:\\Zotero\\storage\\8BQK75D2\\Onder Cetin, Der Kiureghian, Seed - 2002 - Probabilistic models for the initiation of seismic soil liquefaction.pdf}
}

@article{pakinComprehensiveLaTeXSymbol,
  title = {The {{Comprehensive LaTeX Symbol List}}},
  author = {Pakin, Scott},
  abstract = {This document lists 18150 symbols and the corresponding LATEX commands that produce them. Some of these symbols are guaranteed to be available in every LATEX 2{$\mathsl{E}$} system; others require fonts and packages that may not accompany a given distribution and that therefore need to be installed. All of the fonts and packages used to prepare this document\textemdash as well as this document itself\textemdash are freely available from the Comprehensive TEX Archive Network (http://www.ctan.org/).},
  langid = {english},
  file = {D\:\\Zotero\\storage\\AP63F6YF\\Pakin - The Comprehensive LaTeX Symbol List.pdf}
}

@book{Pao1998,
  title = {Analysis {{Project Editor}} : {{Cover}} Design},
  author = {Pao, Yen-Ching},
  year = {1998},
  journal = {eLibraryM},
  volume = {127},
  doi = {10.1007/s00244-002-0133-7},
  abstract = {This book provides a concise introduction to numerical concepts in engineering analysis, using FORTRAN, QuickBASIC, MATLAB, and Mathematica to illustrate the examples.},
  isbn = {0-8493-2016-X},
  file = {D\:\\Zotero\\storage\\9CREK57R\\Pao - 1998 - Analysis Project Editor Cover design.pdf}
}

@book{Paolella2019,
  title = {Linear {{Models}} and {{Time-Series Analysis}}},
  author = {Paolella, Marc S.},
  year = {2019},
  journal = {Wiley Series in Probability and Statistics},
  publisher = {{Wiley}},
  pmid = {25246403},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\JYCDHYAN\\Paolella - 2019 - Linear Models and Time-Series Analysis.pdf;D\:\\Zotero\\storage\\TS8F34HJ\\Paolella - 2019 - Linear Models and Time-Series Analysis.pdf}
}

@article{Pappas2008,
  title = {Modeling of the Grounding Resistance Variation Using {{ARMA}} Models},
  author = {Pappas, S. Sp and Ekonomou, L. and Karampelas, P. and Katsikas, S. K. and Liatsis, P.},
  year = {2008},
  journal = {Simulation Modelling Practice and Theory},
  volume = {16},
  number = {5},
  pages = {560--570},
  issn = {1569190X},
  doi = {10.1016/j.simpat.2008.02.009},
  abstract = {This study addresses the problem of modeling the variation of the grounding resistance during the year. An AutoRegressive Moving Average (ARMA) model is fitted (off-line) on the provided actual data using the Corrected Akaike Information Criterion (AICC). The developed model is shown to fit the data in a successful manner. Difficulties occur when the provided data includes noise or errors and also when an on line/adaptive modeling is required. In both cases, and under the assumption that the provided data can be represented by an ARMA model, simultaneous order and parameter estimation of ARMA models under the presence of noise is necessary. In this paper, a new method based on the multi-model partitioning theory which is also applicable to on line/adaptive operation, is used for the solution of the above mentioned problem. The simulations show that the proposed method succeeds in selecting the correct ARMA model order and estimates the parameters accurately in very few steps and even with a small sample size. For validation purposes the method introduced is compared with three other established order selection criteria presenting very good results. The proposed method can be extremely useful in the studies of electrical engineer designers, since the variation of the grounding resistance during the year affects significantly power systems performance and must be definitely considered. \textcopyright{} 2008 Elsevier B.V. All rights reserved.},
  keywords = {Adaptive multi-model,ARMA,Filtering,Grounding resistance,Kalman,Order selection,Parameter estimation},
  file = {D\:\\Zotero\\storage\\LAWCZKN7\\Pappas et al. - 2008 - Modeling of the grounding resistance variation using ARMA models.pdf}
}

@incollection{Patil2006,
  title = {Environmental and {{Ecological Statistics}}},
  booktitle = {Encyclopedia of {{Environmetrics}}},
  author = {Patil, G.P.},
  editor = {Patil, Ganapati P. and {For}},
  year = {2006},
  publisher = {{Springer}},
  doi = {10.1002/9780470057339.vae002},
  abstract = {Published: Norwell, MA : Kluwer Academic Publishers, {$<$}1999-{$>$}},
  isbn = {978-1-4614-3121-3},
  file = {D\:\\Zotero\\storage\\BG769LS6\\Gregoire et al. - Unknown - Series Editor.pdf}
}

@book{pearlCausalInferenceStatistics,
  title = {Causal {{Inference}} in {{Statistics}}},
  author = {Pearl, Judea},
  langid = {english},
  file = {D\:\\Zotero\\storage\\TCTHKJW9\\Pearl - Causal Inference in Statistics.pdf}
}

@article{Percival1992,
  title = {Simulating {{Gaussian Random Processes}} with {{Specified Spectra}}},
  author = {Percival, Donald B.},
  year = {1992},
  journal = {Computing Science and Statistics},
  volume = {24},
  pages = {534--538},
  abstract = {We discuss the problem of generating realizations of length N from a Gaussian stationary process \{Y t \} with a specified spectral density function S Y (\^A{$\cdot$}). We review three methods for generating the required realizations and consider their relative merits. In particular, we discuss an approximate frequency domain technique that is evidently used frequently in practice, but that has some potential pitfalls. We discuss extensions to this technique that allow it to be used to generate realizations from a power-law process with spectral density function similar to S(f) = |f | \# for \# {$<$} 0. I. Introduction Let \{Y t \} be a real-valued Gaussian stationary process with spectral density function (sdf) S Y (\^A{$\cdot$}), autocorrelation sequence (acvs) \{s \#,Y \} and zero mean. If we define the sampling time between observations Y t and Y t+1 to be unity so that the Nyquist frequency is 1 2 , then the acvs is related to the sdf via the usual relationship s \#,Y = Z 1 2 - 1 2...},
  file = {D\:\\Zotero\\storage\\XQBRKZBR\\Percival - 1992 - Simulating Gaussian Random Processes with Specified Spectra.pdf}
}

@book{Percival2020,
  title = {Spectral {{Analysis}} for {{Univariate Time Series}}},
  author = {Percival, Donald B. and Walden, Andrew T.},
  year = {2020},
  journal = {Spectral Analysis for Univariate Time Series},
  doi = {10.1017/9781139235723},
  abstract = {"This chapter provides a quick introduction to the subject of spectral analysis. Except for some later references to the exercises of Section 1.6, this material is independent of the rest of the book and can be skipped without loss of continuity. Our intent is to use some simple examples to motivate the key ideas. Since our purpose is to view the forest before we get lost in the trees, the particular analysis techniques we use here have been chosen for their simplicity rather than their appropriateness"-- CN - QA280},
  isbn = {978-1-107-02814-2},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\UNRKTBPH\\Percival, Walden - 2020 - Spectral Analysis for Univariate Time Series.pdf}
}

@book{PerezRuiz2016,
  title = {Bayesian {{Nonparametric Data Analysis}}},
  author = {Perez Ruiz, Diego Andres},
  year = {2016},
  journal = {International Statistical Review},
  volume = {84},
  doi = {10.1111/insr.12168},
  abstract = {In this book, we review nonparametric Baye sian methods and models. The orga- nization of the book follows a data analysis perspective. Rather than focusing on specific models, chapters are organized by traditional data analysis problems. For each problem, we introduce suitable nonpa rametric Bayesian models and show how they are used to implement inference in the given data analysis problem. In selecting specific nonparametric models, we favor simpler and traditional models over specialized ones. The organizatio n by inferential problem leads to some repetition in the discussion of specific m odels when the same nonparametric prior is used in different contexts.},
  isbn = {978-3-319-18967-3},
  file = {D\:\\Zotero\\storage\\R22CMA7T\\Perez Ruiz - 2016 - Bayesian Nonparametric Data Analysis.pdf}
}

@article{piteraGoodnessoffitTestAstable,
  title = {Goodness-of-Fit Test for a-Stable Distribution Based on the Quantile Conditional Variance Statistics},
  author = {Pitera},
  journal = {Statistical Methods \& Applications},
  doi = {10.1007/s10260-021-00571-9},
  file = {D\:\\Zotero\\storage\\263JFJ7T\\Pitera2021_Article_Goodness-of-fitTestForAlphaΑ-s.pdf}
}

@book{Plouffe,
  title = {Advanced {{Combinatorics}}},
  author = {Comtet, Louis},
  year = {1974},
  journal = {Advanced Combinatorics},
  doi = {10.1007/978-94-010-2196-8},
  file = {D\:\\Zotero\\storage\\Y29M2XAF\\Plouffe - Unknown - Advanced Combinatorics COMTET.djvu}
}

@article{Popescu1990,
  title = {Analysis and Simulation of Strong Earthquake Ground Motions Using {{ARMA}} Models},
  author = {Popescu, Th D. and Demetriu, S.},
  year = {1990},
  journal = {Automatica},
  volume = {26},
  number = {4},
  pages = {721--737},
  issn = {00051098},
  doi = {10.1016/0005-1098(90)90049-N},
  abstract = {The acceleration record of an earthquake ground motion is a nonstationary process with both amplitude and frequency content varying in time. The paper presents a general procedure for the analysis and simulation of strong earthquake ground motions based on parametric ARMA models to be used in computing structural response. Some computational results obtained in the analysis and simulation of ground acceleration, recorded during the Romanian Earthquake of 4 March 1977, are also included. \textcopyright{} 1990.},
  keywords = {\ding{72},ARMA models,earthquake ground motion,geophysics,nonstationarity analysis,parameter estimation,signal processing,simulation,Time-series analysis}
}

@book{Press2008,
  title = {Integral Equations and Their Applications},
  author = {Press, W I T},
  year = {2008},
  journal = {Choice Reviews Online},
  volume = {45},
  issn = {0009-4978},
  doi = {10.5860/choice.45-4442},
  abstract = {The polymeric semiconducting carbon films are grown on silicon and quartz substrates by excimer (XeCl) pulsed laser deposition (PLD) technique using fullerene C-60 precursor. The substrate temperature is varied up to 300 degrees C. The structure and optical properties of the films strongly depend on the substrate temperature. The grain size is increased and uniform polymeric film with improved morphology at higher temperature is observed. The Tauc gap is about 1.35 eV for the film deposited at 100 degrees C and with temperature the gap is decreased upto 1.1 eV for the film deposited at 250 degrees C and increased to about 1.4 eV for the film deposited at 300 degrees C. The optical absorption properties are improved with substrate temperature. Raman spectra show the presence of both G peak and D peak and are peaked at about 1590 cm(-1) and 1360 cm(-1), respectively for the film deposited at 100 degrees C. The G peak position remains almost unchanged while D peak has changed only a little with temperature might be due to its better crystalline structure compared to the typical amorphous carbon films and might show interesting in device such as, optoelectronic applications. (C) 2008 Elsevier B.V. All rights reserved.},
  isbn = {978-1-84564-101-6},
  file = {D\:\\Zotero\\storage\\V8Q7K9GP\\Press - 2008 - Integral equations and their applications.pdf}
}

@misc{Qcqpi,
  title = {Qcqpi},
  file = {D\:\\Zotero\\storage\\7C4X7HZB\\Unknown - Unknown - qcqpi.epub}
}

@book{Quarteroni1994,
  title = {Numerical {{Mathematics}}},
  author = {Quarteroni, Alfio and Sacco, Riccardo and Saleri, Fausto},
  journal = {Texts in Applied Mathematics},
  publisher = {{Springer}},
  issn = {00255572},
  doi = {10.2307/3619466},
  isbn = {0-387-98959-5},
  file = {D\:\\Zotero\\storage\\8WBT2VHB\\Quarteroni, Sacco, Saleri - Unknown - Numerical Mathematics.pdf}
}

@article{Rankin1969,
  title = {Calculus. {{Vol}}. {{I}}. {{One-Variable Calculus}}, with an {{Introduction}} to {{Linear Algebra}}},
  author = {Rankin, R. A. and Apostol, T. M.},
  year = {1969},
  journal = {The Mathematical Gazette},
  volume = {53},
  number = {384},
  pages = {218},
  issn = {00255572},
  doi = {10.2307/3614609},
  abstract = {The initial velocities of enzyme catalyzed reactions are usually determined by measuring the slope near the origin of a product concentration vs. time plot. However, experimentally, it may be difficult to measure the product concentration accurately in the region of linearity. An integrated rate equation has been derived which can be used to calculate initial velocities from product concentration data taken in the time range from zero until the reaction has nearly reached completion. Using the partial enzymatic mechanism and the steady-state approximation, an equation,In(1-P/Peq)=-vt/Peq has been derived in which Pis the product concentration at time t, Peq is the product concentration at equilibrium, and v is the initial velocity. If ln (1 -P/Peq) is plotted vs. t, v can be calculated from the slope. The advantage of this method is that measurements of Pare not restricted to less than 10\% of Peq to maintain linearity. In most cases ln (1-P/Peq) vs. t plots are linear up to P/Peq equal to 09.},
  file = {D\:\\Zotero\\storage\\UMUF2XMD\\Rankin, Apostol - 1969 - Calculus. Vol. I. One-Variable Calculus, with an Introduction to Linear Algebra.pdf}
}

@book{RatonLondonNewYorkBrianEverittTorstenHothorn2006,
  title = {Statistical {{Analyses Using R}}},
  author = {Raton London New York Brian Everitt Torsten Hothorn, Boca S},
  year = {2006},
  isbn = {1-58488-539-4},
  file = {D\:\\Zotero\\storage\\878JBU2Y\\Raton London New York Brian Everitt Torsten Hothorn - 2006 - Statistical Analyses Using R.pdf}
}

@techreport{RegressionDiagnostics,
  title = {Regression {{Diagnostics}}},
  file = {D\:\\Zotero\\storage\\WLZ6K42X\\Regression Diagnostics - Identifying Influential Data and Sources of Collinearity (David A. Belsley, Edwin Kuh, Roy E. Welsch) (z-lib.org).pdf}
}

@article{Reiter2004,
  title = {Technical {{Review Board Uncertainties}} and {{Conservatism}} in {{Seismic Hazard Assessment}}},
  author = {Reiter, Leon and Staff, Senior},
  year = {2004},
  file = {D\:\\Zotero\\storage\\PLXPR85A\\Reiter, Staff - 2004 - Technical Review Board Uncertainties and Conservatism in Seismic Hazard Assessment.pdf}
}

@article{Ribo2000,
  title = {1 {{Introducci}} on 2 {{Espacio Eucl Ideo}}},
  author = {Ribo, Ramon},
  year = {2000},
  file = {D\:\\Zotero\\storage\\W89DAPWP\\Ribo - 2000 - 1 Introducci on 2 Espacio Eucl Ideo.pdf}
}

@book{Rivadulla1991,
  title = {Mathematical Statistics and Data Analysis},
  author = {Rivadulla, Andr{\'e}s},
  year = {1991},
  journal = {Erkenntnis},
  volume = {34},
  issn = {01650106},
  doi = {10.1007/BF00385721},
  abstract = {This paper deals with meta-statistical questions concerning frequentist statistics. In Sections 2 to 4 I analyse the dispute between Fisher and Neyman on the so called logic of statistical inference, a polemic that has been concomitant of the development of mathematical statistics. My conclusion is that, whenever mathematical statistics makes it possible to draw inferences, it only uses deductive reasoning. Therefore I reject Fisher's inductive approach to the statistical estimation theory and adhere to Neyman's deductive one. On the other hand, I assert that Neyman-Pearson's testing theory, as well as Fisher's tests of significance, properly belong to decision theory, not to logic, neither deductive nor inductive. I then also disagree with Costantini's view of Fisher's testing model as a theory of hypothetico-deductive inferences. In Section 5 I disapprove Hacking1's evidentialists criticisms of the Neyman-Pearson's theory of statistics (NPT), as well as Hacking2's interpretation of NPT as a theory of probable inference. In both cases Hacking misses the point. I conclude, by claiming that Mayo's conception of the Neyman-Pearson's testing theory, as a model of learning from experience, does not purport any advantages over Neyman's behavioristic model. \textcopyright{} 1991 Kluwer Academic Publishers.},
  isbn = {0-534-39942-8},
  file = {D\:\\Zotero\\storage\\VNP5ELUT\\Rivadulla - 1991 - Mathematical statistics and metastatistical analysis.pdf}
}

@article{Robin1981,
  title = {Statistical Uncertainties in Seismic Hazard Evaluations in the {{United States}}},
  author = {McGuire, Robin K and Shedlock, Kaye M.},
  year = {1981},
  journal = {Bulletin of the Seismological Society of America},
  volume = {71},
  number = {4},
  pages = {1287--1308},
  abstract = {Efficient and accurate methods of estimating the sensitivity of seismic hazard calculations to statistical uncertainties in models and parameters are demonstrated. These models require knowledge of the earthquake magnitude and distance that contribute most to the probability of exceedence of a chosen acceleration level; the methods estimate sensitivities using point-source seismic-hazard approximations for which closed-form solutions are available. An additional result is that the use of Bayesian estimates for seismicity and ground motion parameters in the hazard analysis produces unbiased Bayesian estimates of the seismic ground motion hazard, due to the almost linear relationship between ground motion amplitudes at a given probability level, and parameter uncertainties. Application of these methods to the San Francisco, California, Bay area indicates a coefficient of variation (cov) of the 500-yr acceleration of about 0.4 at sites close to major faults, and a cov of about 0.2 at sites 50 km to the east of the major east bay faults. These cov's result from statistical uncertainty in the depth of energy release, the activity rate and Richter b value for each fault, and the mean acceleration-attenuation relationship. A similar analysis in the central Mississippi Valley area indicates a cov in 500-yr acceleration of 0.4 near the major faults, with a value of about 0.3 at distances greater than 50 km. The sources of statistical uncertainty in this region are the depth of energy release as well as its location, the activity rate and Richter b value for each fault, and the mean acceleration-attenuation function.},
  isbn = {0037-1106},
  file = {D\:\\Zotero\\storage\\W4SKPDLJ\\Unknown - 1981 - STATISTICAL UNCERTAINTIES IN SEISMIC HAZARD EVALUATIONS The primary purpose of the present study is to determine the st.pdf}
}

@book{Robinson2005,
  title = {Linear {{Models With R}}},
  author = {Robinson, Timothy J},
  year = {2005},
  journal = {Technometrics},
  volume = {47},
  issn = {0040-1706},
  doi = {10.1198/tech.2005.s287},
  abstract = {Physical, Chemical \& Earth Sciences},
  isbn = {1-58488-425-8},
  file = {D\:\\Zotero\\storage\\V4JDYU3A\\Robinson - 2005 - Linear Models With R.pdf}
}

@article{Rosenblueth1975a,
  title = {Point Estimates for Probability Moments},
  author = {Rosenblueth, E.},
  year = {1975},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {72},
  number = {10},
  pages = {3812--3814},
  issn = {00278424},
  doi = {10.1073/pnas.72.10.3812},
  file = {D\:\\Zotero\\storage\\J79PUIBI\\Rosenblueth - 1975 - Point estimates for probability moments.pdf}
}

@article{Rosner1983a,
  title = {Percentage Points for a Generalized Esd Many-Outlier Procedure},
  author = {Rosner, Bernard},
  year = {1983},
  journal = {Technometrics},
  volume = {25},
  number = {2},
  pages = {165--172},
  issn = {15372723},
  doi = {10.1080/00401706.1983.10487848},
  abstract = {A generalized (extreme Studentized deviate) ESD many-outlier procedure is given for detecting from 1 to k outliers in a data set. This procedure has an advantage over the original ESD many-outlier procedure (Rosner 1975) in that it controls the type I error both under the hypothesis of no outliers and under the alternative hypotheses of 1, 2, \ldots. k-l outliers. A method is given for approximating percentiles for this procedure based on the t distribution. This method is shown to be adequately accurate using Monte Carlo simulation, for detecting up to 10 outliers in samples as small as 25. Tables are given for implementing this method for n = 25(1)50(10)100(50)500; k = 10, {$\alpha$} =.05,.Ol,.005. \textcopyright{} 1983 Taylor \& Francis Group, LLC.},
  keywords = {Detection of outliers,Extreme Studentized deviate,Multiple outliers,Outliers,T distribution},
  file = {D\:\\Zotero\\storage\\7CAK28Y2\\Rosner - 1983 - Percentage points for a generalized esd many-outlier procedure.pdf}
}

@article{RuizEspejo2017a,
  title = {Estimaci\'on de La Desviaci\'on Est\'andar},
  author = {Ruiz Espejo, Mariano},
  year = {2017},
  journal = {Estad\'istica Espa\~nola},
  volume = {59},
  pages = {37--44},
  abstract = {Resumen En el presente art\'iculo estudiamos las propiedades del estimador "cuasidesviaci\'on est\'andar muestral" como estimador de la "desviaci\'on est\'andar poblacional" cuando el dise\~no muestral es el muestreo aleatorio simple con reemplazamiento de tama\~no fijo, as\'i como cuando este tama\~no muestral tiende a infinito. Palabras clave: cuasidesviaci\'on est\'andar muestral, desviaci\'on est\'andar poblacional, muestreo aleatorio simple con reemplazamiento. Clasificaci\'on AMS: 62D05, 62E20, 62Pxx. Estimation of the standard deviation Abstract In the present article we study the properties of the estimator "sample standard quasideviation" as estimator of the "population standard deviation" when the sampling design is simple random sampling with replacement of fixed size, as well as when this sample size tends to infinite. Keywords: sample standard quasideviation, population standard deviation, simple random sampling with replacement.},
  keywords = {62E20,62Pxx,AMS classification: 62D05},
  file = {D\:\\Zotero\\storage\\289TU3GN\\m-api-01e1347d-7391-2b75-76d3-630cab11b285.pdf}
}

@article{S.2015,
  title = {Measuring Bias in Structural Response Caused by Ground Motion Scaling},
  author = {S., Mosalam M.K Gunay},
  year = {2015},
  journal = {International Association for Earthquake Engineering},
  volume = {44},
  number = {056},
  pages = {657--675},
  issn = {00988847},
  isbn = {6507252573},
  file = {D\:\\Zotero\\storage\\JR58WHU4\\S. - 2015 - Measuring bias in structural response caused by ground motion scaling.pdf}
}

@article{Sambridge1999,
  title = {Geophysical Inversion with a Neighbourhood Algorithm--{{II}}. {{Appraising}} the Ensemble},
  author = {Sambridge, Malcolm},
  year = {1999},
  journal = {Geophysical Journal International},
  volume = {138},
  number = {3},
  pages = {727--746},
  issn = {0956540X},
  doi = {10.1046/j.1365-246X.1999.00900.x},
  abstract = {Monte Carlo direct search methods, such as genetic algorithms, simulated annealing, etc., are often used to explore a finite-dimensional parameter space. They require the solving of the forward problem many times, that is, making predictions of observables from an earth model. The resulting ensemble of earth models represents all 'information' collected in the search process. Search techniques have been the subject of much study in geophysics; less attention is given to the appraisal of the ensemble. Often inferences are based on only a small subset of the ensemble, and sometimes a single member. This paper presents a new approach to the appraisal problem. To our knowledge this is the first time the general case has been addressed, that is, how to infer information from a complete ensemble, previously generated by any search method. The essence of the new approach is to use the information in the available ensemble to guide a resampling of the parameter space. This requires no further solving of the forward problem, but from the new 'resampled' ensemble we are able to obtain measures of resolution and trade-off in the model parameters, or any combinations of them. The new ensemble inference algorithm is illustrated on a highly non-linear waveform inversion problem. It is shown how the computation time and memory requirements scale with the dimension of the parameter space and size of the ensemble. The method is highly parallel, and may easily be distributed across several computers. Since little is assumed about the initial ensemble of earth models, the technique is applicable to a wide variety of situations. For example, it may be applied to perform 'error analysis' using the ensemble generated by a genetic algorithm, or any other direct search method.},
  keywords = {Numerical techniques,Receiver functions,Waveform inversion},
  file = {D\:\\Zotero\\storage\\TG3YY545\\Sambridge - 1999 - Geophysical inversion with a neighbourhood algorithm--II. Appraising the ensemble.pdf}
}

@article{Sambridge1999a,
  title = {Sambridge-1999-{{Geophysical}}\_{{Journal}}\_{{International}}},
  author = {Sambridge, Malcolm},
  year = {1999},
  journal = {Geophysical journal international},
  volume = {138},
  number = {2},
  pages = {479--494},
  issn = {1365-246X},
  keywords = {numerical techniques,receiver functions,waveform inversion},
  file = {D\:\\Zotero\\storage\\T4AQW9NV\\Sambridge - 1999 - Sambridge-1999-Geophysical_Journal_International.pdf}
}

@article{Sambridge2001,
  title = {Finding Acceptable Models in Nonlinear Inverse Problems Using a Neighbourhood Algorithm},
  author = {Sambridge, Malcolm},
  year = {2001},
  journal = {Inverse Problems},
  volume = {17},
  number = {3},
  pages = {387--403},
  issn = {02665611},
  doi = {10.1088/0266-5611/17/3/302},
  abstract = {A recently proposed new class of direct search method is applied to the problem of mapping out the region of data-acceptable models (sets of unknowns) in a finite-dimensional nonlinear inverse problem. A model is defined to be data acceptable if its fit to the observed data is better than some prescribed level. The neighbourhood algorithm (NA) can be used to generate ensembles of models which preferentially sample the data-acceptable regions of parameter space. Simple transformations of a data misfit criterion are proposed to assist in this task. Some numerical experiments are presented which are motivated by highly nonlinear geophysical inverse problems. In these cases it is shown how the NA can be used to map out the main features of data-acceptable regions in both high-and low-dimensional problems. It is also shown how the NA can concentrate sampling in multiple acceptable regions simultaneously.},
  file = {D\:\\Zotero\\storage\\BF3F7ZFS\\Sambridge - 2001 - Finding acceptable models in nonlinear inverse problems using a neighbourhood algorithm.pdf}
}

@book{Sataloff,
  title = {Rules of {{Thumb}} in {{Engineering Practice}}},
  author = {Woods, Donald R.},
  year = {2015},
  publisher = {{Wiley}},
  isbn = {978-1-62623-977-7},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\3IX6QXIP\\Jones - Unknown - Volume V - Risk, Opportunity, Uncrtainty and Pther Random Models.pdf;D\:\\Zotero\\storage\\69A8NZ7X\\Jones - Unknown - Volume II - Probability, Statistics and Other Frightening Stuff.pdf;D\:\\Zotero\\storage\\7GFBH4KP\\Elnashai, Sarno - Unknown - Fundamentals of Earthquake Engineering. From Source to Fragility.pdf;D\:\\Zotero\\storage\\83V2BAK7\\Han - 2015 - Principles and Practice of Ground Improvement.pdf;D\:\\Zotero\\storage\\BAM9J4MM\\Jones - Unknown - Volume III - Best Fit Lines & Curves and some mathe-magical trnasformations.pdf;D\:\\Zotero\\storage\\BQBZ66S4\\Hollander, Wolfe, Chicken - Unknown - Nonparametric Statistical Methods.pdf;D\:\\Zotero\\storage\\HCRTN3GS\\Geisser - Unknown - Models of Parametric Statistical Inference.pdf;D\:\\Zotero\\storage\\MSJUUHQU\\Kaufman, Rousseeuw - 2005 - Finding Groups in Data - An Introduction to Cluster Analysis.pdf;D\:\\Zotero\\storage\\PZGSWFP9\\Deshpande, Naik-Nimbalkar, Dewan - Unknown - Nonparametric Statistics.pdf;D\:\\Zotero\\storage\\VH8CX83P\\Woods - Unknown - Rules of Thumb in Engineering Practice.pdf;D\:\\Zotero\\storage\\WEBHA7GH\\Ewald - Unknown - Writing in the Technical Fields - A Practical Guida.pdf}
}

@book{schapireBoostingFoundationsAlgorithms2012,
  title = {Boosting: Foundations and Algorithms},
  shorttitle = {Boosting},
  author = {Schapire, Robert E. and Freund, Yoav},
  year = {2012},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  isbn = {978-0-262-01718-3},
  langid = {english},
  lccn = {Q325.75 .S33 2012},
  keywords = {Boosting (Algorithms),Supervised learning (Machine learning)},
  file = {D\:\\Zotero\\storage\\I6C8DNPK\\Schapire and Freund - 2012 - Boosting foundations and algorithms.pdf}
}

@book{Schmee1986,
  title = {An {{Introduction}} to {{Multivariate Statistical Analysis}}},
  author = {Schmee, Josef and Anderson, T. W.},
  year = {1986},
  journal = {Technometrics},
  volume = {28},
  issn = {00401706},
  doi = {10.2307/1270458},
  isbn = {3-17-572399-3},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\2885JX6H\\Schmee, Anderson - 1986 - An Introduction to Multivariate Statistical Analysis.pdf;D\:\\Zotero\\storage\\FBU6SEXH\\Fisher, Fisher - 2003 - Candlesticks, fibonacci, and chart pattern trading tools.pdf;D\:\\Zotero\\storage\\MUSWU55C\\Montgomery, Jennings, Kulahci - 2008 - Introduction to Time Series Analysis and Forecasting.pdf}
}

@book{scholkopfLearningKernelsSupport2002,
  title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
  shorttitle = {Learning with Kernels},
  author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander Johannes},
  year = {2002},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Reprint.},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass.}},
  isbn = {978-0-262-19475-4 978-0-262-53657-8},
  langid = {english},
  file = {D\:\\Zotero\\storage\\R7BKDQYB\\Schölkopf and Smola - 2002 - Learning with kernels support vector machines, re.pdf}
}

@article{Schreiber1998,
  title = {Constrained Randomization of Time Series Data},
  author = {Schreiber, Thomas},
  year = {1998},
  journal = {Physical Review Letters},
  volume = {80},
  number = {10},
  eprint = {chao-dyn/9909042},
  pages = {2105--2108},
  issn = {10797114},
  doi = {10.1103/PhysRevLett.80.2105},
  abstract = {A new method is introduced to create artificial time sequences that fulfil given constraints but are random otherwise. Constraints are usually derived from a measured signal for which surrogate data are to be generated. They are fulfilled by minimizing a suitable cost function using simulated annealing. A wide variety of structures can be imposed on the surrogate series, including multivariate, nonlinear, and nonstationary properties. When the linear correlation structure is to be preserved, the new approach avoids certain artifacts generated by Fourier-based randomization schemes. \textcopyright{} 1998 The American Physical Society.},
  archiveprefix = {arxiv},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\NUTZ2AII\\Schreiber - 1998 - Constrained randomization of time series data.pdf}
}

@book{Schreiber2000,
  title = {Surrogate Time Series},
  author = {Schreiber, Thomas and Schmitz, Andreas},
  year = {2000},
  volume = {142},
  eprint = {chao-dyn/9909037},
  abstract = {Before we apply nonlinear techniques, e.g. those inspired by chaos theory, to dynamical phenomena occurring in nature, it is necessary to first ask if the use of such advanced techniques is justified by the data. While many processes in nature seem very unlikely a priori to be linear, the possible nonlinear nature might not be evident in specific aspects of their dynamics. The method of surrogate data has become a very popular tool to address such a question. However, while it was meant to provide a statistically rigorous, foolproof framework, some limitations and caveats have shown up in its practical use. In this paper, recent efforts to understand the caveats, avoid the pitfalls, and to overcome some of the limitations, are reviewed and augmented by new material. In particular, we will discuss specific as well as more general approaches to constrained randomisation, providing a full range of examples. New algorithms will be introduced for unevenly sampled and multivariate data and for surrogate spike trains. The main limitation, which lies in the interpretability of the test results, will be illustrated through instructive case studies. We will also discuss some implementational aspects of the realisation of these methods in the TISEAN software package. \textcopyright{} 2000 Elsevier Science B.V. All rights reserved.},
  archiveprefix = {arxiv},
  keywords = {Nonlinearity,Surrogate data,Time series},
  file = {D\:\\Zotero\\storage\\FE94UNAC\\Schreiber, Schmitz - 2000 - Surrogate time series.pdf}
}

@article{Schweckendiek2021,
  title = {{{STATE-OF-THE-ART REVIEW OF INHERENT VARIABILITY AND UNCERTAINTY IN Technical Committee}} of {{Engineering Practice}} of {{Risk Assessment}} \& {{Jianye Ching}} ( {{Chair}} of {{TC304}} , {{ISSMGE}} )},
  author = {Schweckendiek, Timo},
  year = {2021},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\22VYLQFR\\Schweckendiek - 2021 - STATE-OF-THE-ART REVIEW OF INHERENT VARIABILITY AND UNCERTAINTY IN Technical Committee of Engineering Practice of.pdf}
}

@book{SchwederT;HjortN.2016,
  title = {Confidence , {{Likelihood}} , {{Probability}}: {{Statistical Inference}} with {{Confidence Distributions}}},
  author = {Schweder, T;Hjort, N., L},
  year = {2016},
  isbn = {978-0-521-86160-1},
  file = {D\:\\Zotero\\storage\\4YBFJJMX\\Schweder, THjort, N. - 2016 - Confidence , Likelihood , Probability Statistical Inference with Confidence Distributions.pdf}
}

@article{scutariBayesianNetworks,
  title = {Bayesian {{Networks}}},
  author = {Scutari, Marco and Denis, Jean-Baptiste},
  langid = {english},
  file = {D\:\\Zotero\\storage\\Y2XMVJ22\\Scutari and Denis - Bayesian Networks.pdf}
}

@book{scutariBayesianNetworksExamples2021,
  title = {Bayesian {{Networks}}: {{With Examples}} in {{R}}},
  shorttitle = {Bayesian {{Networks}}},
  author = {Scutari, Marco and Denis, Jean-Baptiste},
  year = {2021},
  month = may,
  edition = {2},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  doi = {10.1201/9780429347436},
  urldate = {2023-06-19},
  isbn = {978-0-429-34743-6},
  langid = {english},
  file = {D\:\\Zotero\\storage\\LN6SE7AW\\Scutari and Denis - 2021 - Bayesian Networks With Examples in R.pdf}
}

@book{Seber1984,
  title = {Multivariate {{Observations}}},
  author = {Seber, G. A. F.},
  year = {1984},
  issn = {0471691216},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\MSPHEYBM\\Seber - 1984 - Multivariate Observations.pdf}
}

@article{Shin2016,
  title = {A {{Time-Domain Method}} to {{Generate Artificial Time History}} from a {{Given Reference Response Spectrum}}},
  author = {Shin, Gangsig and Song, Ohseop},
  year = {2016},
  journal = {Nuclear Engineering and Technology},
  volume = {48},
  number = {3},
  pages = {831--839},
  publisher = {{Elsevier B.V}},
  issn = {2234358X},
  doi = {10.1016/j.net.2016.01.023},
  abstract = {Seismic qualification by test is widely used as a way to show the integrity and functionality of equipment that is related to the overall safety of nuclear power plants. Another means of seismic qualification is by direct integration analysis. Both approaches require a series of time histories as an input. However, in most cases, the possibility of using real earthquake data is limited. Thus, artificial time histories are widely used instead. In many cases, however, response spectra are given. Thus, most of the artificial time histories are generated from the given response spectra. Obtaining the response spectrum from a given time history is straightforward. However, the procedure for generating artificial time histories from a given response spectrum is difficult and complex to understand. Thus, this paper presents a simple time-domain method for generating a time history from a given response spectrum; the method was shown to satisfy conditions derived from nuclear regulatory guidance.},
  keywords = {Artificial Time History,Response Spectrum,Seismic Qualification,Time-Domain Peak Reduction},
  file = {D\:\\Zotero\\storage\\8J9DQ2ID\\Shin, Song - 2016 - A Time-Domain Method to Generate Artificial Time History from a Given Reference Response Spectrum.pdf}
}

@article{Siffer2017c,
  title = {Anomaly Detection in Streams with Extreme Value Theory},
  author = {Siffer, Alban and Fouque, Pierre Alain and Termier, Alexandre and Largouet, Christine},
  year = {2017},
  journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  volume = {Part F1296},
  pages = {1067--1075},
  doi = {10.1145/3097983.3098144},
  abstract = {Anomaly detection in time series has attracted considerable attention due to its importance in many real-world applications including intrusion detection, energy management and finance. Most approaches for detecting outliers rely on either manually set thresholds or assumptions on the distribution of data according to Chandola, Banerjee and Kumar. Here, we propose a new approach to detect outliers in streaming univariate time series based on Extreme Value Theory that does not require to hand-set thresholds and makes no assumption on the distribution: the main parameter is only the risk, controlling the number of false positives. Our approach can be used for outlier detection, but more generally for automatically setting thresholds, making it useful in wide number of situations. We also experiment our algorithms on various real-world datasets which confirm its soundness and efficiency.},
  isbn = {9781450348874},
  keywords = {Extreme value theory,Outliers in time series,Streaming},
  file = {D\:\\Zotero\\storage\\NQVQLCNZ\\Siffer et al. - 2017 - Anomaly detection in streams with extreme value theory.pdf;D\:\\Zotero\\storage\\WBPH9I4T\\Siffer et al. - 2017 - Anomaly detection in streams with extreme value theory(2).pdf}
}

@article{Sitaram1999a,
  title = {Uncertainty Principles and Fourier Analysis},
  author = {Sitaram, Alladi},
  year = {1999},
  journal = {Resonance},
  volume = {4},
  number = {2},
  pages = {20--23},
  issn = {0971-8044},
  doi = {10.1007/bf02838759},
  file = {D\:\\Zotero\\storage\\Y8FF8YLM\\Sitaram - 1999 - Uncertainty principles and fourier analysis.pdf}
}

@book{Sjoquista,
  title = {Understanding {{Regression Analysis}}: {{An Introductory Guide Quantitative Applications}} in the {{Social Sciences No}}. 57.},
  author = {Sparks, T. and Schroeder, Larry D. and Sjoquist, David L. and Stephan, Paula E.},
  year = {1987},
  volume = {36},
  file = {D\:\\Zotero\\storage\\QJRD82LR\\Sjoquist - Unknown - Understanding Regression Analysis.pdf}
}

@book{skansiIntroductionDeepLearning2018,
  title = {Introduction to {{Deep Learning}}: {{From Logical Calculus}} to {{Artificial Intelligence}}},
  shorttitle = {Introduction to {{Deep Learning}}},
  author = {Skansi, Sandro},
  year = {2018},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-73004-2},
  urldate = {2023-07-25},
  isbn = {978-3-319-73003-5 978-3-319-73004-2},
  langid = {english},
  file = {D\:\\Zotero\\storage\\DMM2LSPV\\Skansi - 2018 - Introduction to Deep Learning From Logical Calcul.pdf}
}

@article{Smith2003a,
  title = {{{STATISTICS OF EXTREMES}} , {{WITH APPLICATIONS IN ENVIRONMENT}} , {{INSURANCE AND FINANCE}}},
  author = {Smith, Richard L.},
  year = {2003},
  journal = {Time},
  number = {March},
  pages = {1--62},
  file = {D\:\\Zotero\\storage\\G6TXL7DV\\Smith - 2003 - STATISTICS OF EXTREMES , WITH APPLICATIONS IN ENVIRONMENT , INSURANCE AND FINANCE.pdf}
}

@article{Source1986,
  title = {Bulletin of the {{Seismological Society}} of {{America MODELING SOURCE ZONE BOUNDARY UNCERTAINTY IN SEISMIC}}},
  author = {Source, Modeling and Boundary, Zone and In, Uncertainty and Analysis, Hazard and Bender, B Y Bernice},
  year = {1986},
  volume = {76},
  number = {2},
  pages = {329--341},
  file = {D\:\\Zotero\\storage\\GIIF4L7E\\Source et al. - 1986 - Bulletin of the Seismological Society of America MODELING SOURCE ZONE BOUNDARY UNCERTAINTY IN SEISMIC.pdf}
}

@article{Spiegel1968,
  title = {Schaum's {{Outline}} - {{Mathematical Formulas}}},
  author = {Spiegel, Murray R.},
  year = {1968},
  file = {D\:\\Zotero\\storage\\NIY6ADH6\\Spiegel - 1968 - Schaum's Outline - Mathematical Formulas.pdf}
}

@book{Spiegel1968a,
  title = {Schaum's {{Outlines}}: {{Mathematical Handbook}} of {{Formulas}} and {{Tables}}},
  author = {Spiegel, Muyrray R. and Liu, John M.},
  year = {1968},
  file = {D\:\\Zotero\\storage\\CEQPL7F9\\Spiegel, Liu - 1968 - Schaum's Outlines Mathematical Handbook of Formulas and Tables.pdf}
}

@book{Spiegel2000,
  title = {Theory and {{Problems}} in {{Advanced Calculus}} 2th},
  author = {Spiegel, Murray R.},
  year = {2000},
  isbn = {0-07-139834-1},
  keywords = {Calculus,Mathematics,Science,Vector Calculus},
  file = {D\:\\Zotero\\storage\\Q76T6DLZ\\Spiegel - 2000 - Theory and Problems in Advanced Calculus 2th.pdf}
}

@article{Spiegel2015,
  title = {Time {{Series Distance Measures}}: {{Segmentation}}, {{Classification}}, and {{Clustering}} of {{Temporal Data}}},
  author = {Spiegel, Stephan},
  year = {2015},
  pages = {211},
  abstract = {Time series can be found in domains as diverse as medicine, astronomy, geophysics, engineering, and quantitative finance. In general, a time series is a sequence of data points, measured at successive points in time and spaced at uniform time intervals. This thesis is concerned with time series mining, including segmentation, classification, and clustering of temporal data. Many algorithms for these tasks depend upon pairwise (dis)similarity comparisons of (sub)sequences, which accounts for the continued research on time series distance measures as an important subroutine. In the course of this work we introduce several novel distance measures, which describe time series characteristics that may distinguish the individual classes contained in the data. Our proposed time series distance measures address frequently encountered issues, such as the processing of multivariate data, the computational complexity of pairwise (dis)similarity comparisons, the invariance required for temporal data with distortions, the separation of mixed signals, and the analysis of nonlinear systems. Our work contributes to the time series community by introducing novel approaches to pattern recognition in temporal data, presenting miscellaneous sensor fusion techniques for multivariate measurements, offering efficient and robust distance measures for fast time series classification, introducing previously disregarded invariance and proposing corresponding distance measures, comparing various machine learning algorithms for signal separation, and providing nonlinear models for time series mining. In addition to our theoretical contributions, we furthermore demonstrate that our proposed time series distance measures are beneficial in real-world applications, including the optimization of vehicle engines with regard to exhaust emission and the optimization of heating control in terms of energy efficiency. Furthermore, we present several specifically developed time series mining tools, which implement our introduced distance measures and provide graphical user interfaces for straightforward parameter setting as well as exploratory data analysis.},
  keywords = {\ding{72},data mining,distance measures,Distanzma\ss e,machine learning,maschinelles Lernen,Mustererkennung,pattern recognition,time series,Wissensextraktion,Zeitreihen},
  file = {D\:\\Zotero\\storage\\ZIM2Y3NV\\Spiegel - 2015 - Time Series Distance Measures Segmentation, Classification, and Clustering of Temporal Data.pdf}
}

@article{Spouge1999,
  title = {A {{Guide}} to {{Quantitative Risk Assessment}} for {{Offshore Installations}}},
  author = {Spouge, John},
  year = {1999},
  journal = {DNV Technica},
  doi = {ISBN I 870553 365},
  abstract = {FOREWORD The need for guidance on risk assessment was identified as an industry requirement as a result of regulations, initially promulgated in the UK and Norway, requiring quantitative risk assessments of new and existing installations as part of their safety case. At that time, no standard reference works existed, most expertise was held by individual operators and consultants and little reached the public domain. The project leading to this Guide was initiated by MTD Ltd, and is now published by The Centre for Marine and Petroleum Technology (CMPT), in order to assist engineers involved in commissioning, performing and evaluating risk assessments specifically for the offshore industry.},
  file = {D\:\\Zotero\\storage\\F76DDEQW\\Spouge - 1999 - A Guide to Quantitative Risk Assessment for Offshore Installations.pdf}
}

@misc{Stafforda,
  title = {Uncertainities in {{Ground Motion}} Prediction},
  author = {Stafford, Peter J.},
  file = {D\:\\Zotero\\storage\\W2VZHD5Z\\Unknown - Unknown - Stafford - Uncertainities in Ground Motion prediction.pdf.pdf}
}

@book{Stearns2005a,
  title = {Digital {{Signal Processing}} with {{Examples}} in {{MATLAB}} [{{Book Review}}]},
  author = {Stearns, Samuel D and Hush, Don R},
  year = {2005},
  journal = {IEEE Signal Processing Magazine},
  volume = {22},
  issn = {1053-5888},
  doi = {10.1109/msp.2005.1458309},
  abstract = {In a field as rapidly expanding as digital signal processing, even the topics relevant to the basics change over time both in their nature and their relative importance. It is important, therefore, to have an up-to-date text that not only covers the fundamentals, but that also follows a logical development that leaves no gaps readers must somehow bridge by themselves.Digital Signal Processing with Examples in MATLAB? is just such a text. The presentation does not focus on DSP in isolation, but relates it to continuous signal processing and treats digital signals as samples of physical phenomena. The author also takes care to introduce important topics not usually addressed in signal processing texts, including the discrete cosine and wavelet transforms, multirate signal processing, signal coding and compression, least squares systems design, and adaptive signal processing. He also uses the industry-standard software MATLAB to provide examples of signal processing, system design, spectral analysis, filtering, coding and compression, and exercise solutions. All of the examples and functions used in the text are available online at www.crcpress.com.Designed for a one-semester upper-level course but also ideal for self-study and reference, Digital Signal Processing with Examples in MATLAB is complete, self-contained, and rigorous. For basic DSP, it is quite simply the only book you need.},
  isbn = {978-1-4398-3783-2},
  file = {D\:\\Zotero\\storage\\T8ECBZNQ\\Stearns, Hush - 2005 - Digital Signal Processing with Examples in MATLAB Book Review.pdf}
}

@book{Suri2019a,
  title = {Intelligent {{Systems Reference Library}} 155 {{Outlier Detection}}: {{Techniques}} and {{Applications A Data Mining Perspective}}},
  author = {Suri, N N R Ranga and Murty, Narasimha and Athithan, M G},
  year = {2019},
  abstract = {Book},
  isbn = {978-3-030-05125-9},
  file = {D\:\\Zotero\\storage\\XRQNWI5W\\Suri, Murty, Athithan - 2019 - Intelligent Systems Reference Library 155 Outlier Detection Techniques and Applications A Data Mining Per.pdf}
}

@book{Talwar1990,
  title = {The Analysis of Time Series: {{An}} Introduction},
  author = {Chatfield, Chris and Xing, Haipeng},
  year = {2019},
  journal = {Text in Statistical Science},
  issn = {01692070},
  doi = {10.1016/0169-2070(90)90041-9},
  isbn = {978-1-138-06613-7},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\M59CQIJB\\Talwar - 2019 - The analysis of time series An introduction.pdf}
}

@misc{TeoriaFuncionesAnaliticas,
  title = {Teoria\_{{De}}\_{{Las}}\_{{Funciones}}\_{{Analiticas}}\_{{Archivo3}}.{{Pdf}}},
  file = {D\:\\Zotero\\storage\\JSXJUT5K\\Unknown - Unknown - Teoria_De_Las_Funciones_Analiticas_Archivo3.Pdf.pdf}
}

@book{thompsonSampling2012,
  title = {Sampling},
  author = {Thompson, Steven K.},
  year = {2012},
  series = {Wiley Series in Probability and Statistics},
  edition = {3. ed},
  publisher = {{Wiley}},
  address = {{Hoboken, NJ}},
  abstract = {"The Third Edition retains the general organization of the prior two editions, but it incorporates new material throughout the text. The book is organized into six parts: Part I covers basic sampling from simple random sampling to unequal probability sampling; Part II treats the use of auxiliary data with ratio and regression estimation and looks at the ideas of sufficient data, model, and design in practical sampling; Part III covers major useful designs such as stratified, cluster and systematic, multistage, and double and network sampling; Part IV examines detectability methods for elusive populations, and basic problems in detectability, visibility, and catchability are discussed; Part V concerns spatial sampling with the prediction methods of geostatistics, considerations of efficient spatial designs, and comparisons of different observational methods including plot shapes and detection aspects; and Part VI introduces adaptive sampling designs in which the sampling procedure depends on what is observed during the survey. For this new edition, the author has focused on thoroughly updating the book with a special emphasis on the first 14 chapters since these topics are invariably covered in basic sampling courses. The author has also implemented new approaches to explain the various techniques in the book, and as a result, new examples and explanations have been added throughout. In an effort to improve the presentation and visualization of the book, new figures as well as replacement figures for previously existing figures have been added. This book has continuously stood out from other sampling texts since the figures evoke the idea of each sampling design. The new figures will help readers to better visualize and understand the underlying concepts such as the different sampling strategies"--},
  isbn = {978-0-470-40231-3},
  langid = {english},
  file = {D\:\\Zotero\\storage\\SYLTZT2K\\Thompson - 2012 - Sampling.pdf}
}

@article{Trefethen2000,
  title = {Spectral {{Methods}} in {{MATLAB}}},
  author = {Trefethen, Lloyd N.},
  year = {2000},
  journal = {Spectral Methods in MATLAB},
  doi = {10.1137/1.9780898719598},
  abstract = {This is the only book on spectral methods built around MATLAB programs. Along with finite differences and finite elements, spectral methods are one of the three main technologies for solving partial differential equations on computers. Since spectral methods involve significant linear algebra and graphics they are very suitable for the high level programming of MATLAB. This hands-on introduction is built around forty short and powerful MATLAB programs, which the reader can download from the World Wide Web. This book presents the key ideas along with many figures, examples, and short, elegant MATLAB programs for readers to adapt to their own needs. It covers ODE and PDE boundary value problems, eigenvalues and pseudospectra, linear and nonlinear waves, and numerical quadrature.},
  file = {D\:\\Zotero\\storage\\K3AZT3KF\\Trefethen - 2000 - Spectral Methods in MATLAB.pdf}
}

@book{triantafyllopoulosBayesianInferenceState2021,
  title = {Bayesian {{Inference}} of {{State Space Models}}: {{Kalman Filtering}} and {{Beyond}}},
  shorttitle = {Bayesian {{Inference}} of {{State Space Models}}},
  author = {Triantafyllopoulos, Kostas},
  year = {2021},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-76124-0},
  urldate = {2023-06-19},
  isbn = {978-3-030-76123-3 978-3-030-76124-0},
  langid = {english},
  file = {D\:\\Zotero\\storage\\KCXYQ7K3\\Triantafyllopoulos - 2021 - Bayesian Inference of State Space Models Kalman F.pdf}
}

@book{tsybakovIntroductionNonparametricEstimation2009,
  title = {Introduction to Nonparametric Estimation},
  author = {Tsybakov, Alexandre B. and Zaiats, Vladimir},
  year = {2009},
  series = {Springer Series in Statistics},
  edition = {English ed.},
  publisher = {{Springer}},
  address = {{New York}},
  abstract = {"Methods of nonparametric estimation are located at the core of modern statistical science. The aim of this book is to give a short but mathematically self-contained introduction to the theory of nonparametric estimation. The emphasis is on the construction of optimal estimators; therefore the concepts of minimax optimality and adaptivity, as well as the oracle approach, occupy the central place in the book." "This is a concise text developed from lecture notes and ready to be used for a course on the graduate level. The main idea is to introduce the fundamental concepts of the theory while maintaining the exposition suitable for a first approach in the field. Therefore, the results are not always given in the most general form but rather under assumptions that lead to shorter or more elegant proofs." "The book has three chapters. Chapter 1 presents basic nonparametric regression and density estimators and analyzes their properties. Chapter 2 is devoted to a detailed treatment of minimax lower bounds. Chapter 3 develops more advanced topics: Pinsker's theorem, oracle inequalities, Stein shrinkage, and sharp minimax adaptivity."--Jacket},
  isbn = {978-0-387-79052-7},
  langid = {english},
  lccn = {519.5},
  file = {D\:\\Zotero\\storage\\ERWTUUZT\\Tsybakov and Zaiats - 2009 - Introduction to nonparametric estimation.pdf}
}

@book{Tu2007,
  title = {Basic Principles of Statistical Inference.},
  author = {Tu, Wanzhu},
  year = {2007},
  journal = {Methods in molecular biology (Clifton, N.J.)},
  volume = {404},
  issn = {10643745},
  doi = {10.1007/978-1-59745-530-5_4},
  abstract = {In this chapter, we discuss the fundamental principles behind two of the most frequently used statistical inference procedures: confidence interval estimation and hypothesis testing, both procedures are constructed on the sampling distributions that we have learned in previous chapters. To better understand these inference procedures, we focus on the logic of statistical decision making and the role that experimental data play in the decision process. Numerical examples are used to illustrate the implementation of the discussed procedures. This chapter also introduces some of the most important concepts associated with confidence interval estimation and hypothesis testing, including P values, significance level, power, sample size, and two types of errors. We conclude the chapter with a brief discussion on statistical and practical significance of test results.},
  isbn = {978-0-521-86673-6},
  pmid = {18450045},
  file = {D\:\\Zotero\\storage\\UK58CC7S\\Tu - 2007 - Basic principles of statistical inference.pdf}
}

@article{Tuan1984,
  title = {A {{Note}} on {{Some Statistics Useful}} in {{Identifying}} the {{Order}} of {{Autoregressive Moving Average Model}}},
  author = {Tuan, Pham Dinh},
  year = {1984},
  journal = {Journal of Time Series Analysis},
  volume = {5},
  number = {4},
  pages = {273--279},
  issn = {14679892},
  doi = {10.1111/j.1467-9892.1984.tb00393.x},
  abstract = {Abstract. Several recursive relations concerning some statistics useful in identifying the order of autoregressive moving average are derived and the asymptotic behaviour of these statistics are studied. Copyright \textcopyright{} 1984, Wiley Blackwell. All rights reserved},
  keywords = {Autocorrelations,Autoregressive moving average model,Partial autocorrelations},
  file = {D\:\\Zotero\\storage\\8YSY8XBB\\Tuan - 1984 - a Note on Some Statistics Useful in Identifying the Order of Autoregressive Moving Average Model.pdf}
}

@book{uchaikinChanceStabilityStable1999,
  title = {Chance and {{Stability}}: {{Stable Distributions}} and Their {{Applications}}},
  shorttitle = {Chance and {{Stability}}},
  author = {Uchaikin, Vladimir V. and Zolotarev, Vladimir M.},
  year = {1999},
  month = dec,
  publisher = {{DE GRUYTER}},
  doi = {10.1515/9783110935974},
  urldate = {2023-04-05},
  isbn = {978-90-6764-301-6},
  langid = {english},
  file = {D\:\\Zotero\\storage\\BURVVW5R\\Uchaikin and Zolotarev - 1999 - Chance and Stability Stable Distributions and the.pdf}
}

@book{Unknowna,
  title = {Boostrapping {{Stationary ARMA-GARCH Models}}},
  author = {Shimizu, Kenichi},
  journal = {ثبثبثب},
  isbn = {978-3-8348-0992-6},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\7NEAQYPG\\Shimizu - Unknown - Bootstrapping Stationary ARMA-GARCH Models.pdf;D\:\\Zotero\\storage\\XKZJZUAV\\運動器分科会 - Unknown - No Title超高齢社会における運動器の健康بیبیب.pdf}
}

@book{Unknowncg,
  title = {Probability, {{Statistics}} and {{Decision}} for {{Civil Engineers}}},
  author = {Benjamin, J.R. and Cornell, A.C.},
  year = {1970},
  isbn = {978-0-486-31570-6},
  file = {D\:\\Zotero\\storage\\W9S4N8TB\\Benjamin, Cornell - 1970 - Probability, Statistics and Decision for Civil Engineers.pdf}
}

@book{Unknowndn,
  title = {Robust {{Regression}} and {{Outlier Detection}}},
  author = {Rousseeuw, P and Leroy, A.},
  isbn = {0-471-85233-3},
  file = {D\:\\Zotero\\storage\\MIHQID4H\\Rousseeuw, Leroy - Unknown - Robust Regression and Outlier Detection.pdf}
}

@book{Unknownee,
  title = {Introducci\'on al {{An\'alisis}} de {{Regresi\'on Lineal}}},
  author = {{Montgomery} and {PEck} and {Vinining}},
  year = {2006},
  isbn = {0-471-31565-6},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\9ME74V2H\\Montgomery, PEck, Vinining - 2006 - Introducción al Análisis de Regresión Lineal.pdf}
}

@book{Unknownef,
  title = {Hydrosystems {{Engineering U}}\_ncertainty {{Analysis}}},
  author = {Tung, Y-K and Yen, B.C.},
  isbn = {0-07-146708-4},
  file = {D\:\\Zotero\\storage\\747829LV\\Tung, Yen - Unknown - Hydrosystems Engineering U_ncertainty Analysis.pdf}
}

@book{Unknownfw,
  title = {Uncertainty {{Analysis}} of {{Experimental DAta}} with {{R}}},
  author = {Shaw, B.},
  year = {2017},
  file = {D\:\\Zotero\\storage\\2EUC4CHA\\Shaw - 2017 - Uncertainty Analysis of Experimental DAta with R.pdf}
}

@book{Unknownga,
  title = {Applied {{Research}} in {{Uncertainty Modeling}} and {{Analysis}}},
  author = {{Attoh-Okin} and Ayyub, Bilal M.},
  isbn = {0-387-23550-7},
  file = {D\:\\Zotero\\storage\\FJURF6VA\\Attoh-Okin, Ayyub - Unknown - Applied Research in Uncertainty Modeling and Analysis.pdf}
}

@book{Unknowngb,
  title = {Uncertainty {{Analysis}} with {{High Dimensional Dependence Modeliling}}},
  author = {Kurowicka, D. and Cooke, R.},
  isbn = {978-0-470-86306-0},
  file = {D\:\\Zotero\\storage\\NA5JFUVH\\Kurowicka, Cooke - Unknown - Uncertainty Analysis with High Dimensional Dependence Modeliling.pdf}
}

@article{Variability1984,
  title = {Incorporating Acceleratin Variability into Seismic Hazard Analysis},
  author = {Bender, Bernice},
  year = {1984},
  journal = {Bulletin - Seismological Society of America},
  volume = {74},
  number = {4},
  pages = {1451--1462},
  file = {D\:\\Zotero\\storage\\HIFGZK22\\Variability et al. - 1984 - = cl c2rn f ( R ). loge ( a ), exp ( ~ ) is the median of a and exp ( ~ a--2 ) is the mean of a .pdf}
}

@article{Wand2002a,
  title = {Vector Differential Calculus in Statistics},
  author = {Wand, M. P.},
  year = {2002},
  journal = {American Statistician},
  volume = {56},
  number = {1},
  pages = {55--62},
  issn = {00031305},
  doi = {10.1198/000313002753631376},
  abstract = {Many statistical operations benefit from differential calculus. Examples include optimization of likelihood functions and calculation of information matrices. For multiparameter models differential calculus suited to vector argument functions is usually the most efficient means of performing the required calculations. We present a primer on vector differential calculus and demonstrate its application to statistics through several worked examples.},
  keywords = {Best linear prediction,Generalized linear mixed model,Generalized linear model,Information matrix,Matrix differential calculus,Maximum likelihood estimation,Penalized quasi-likelihood,Score equation},
  file = {D\:\\Zotero\\storage\\PXIU4G3R\\Wand - 2002 - Vector differential calculus in statistics.pdf}
}

@article{Wang2015b,
  title = {Major {{Earthquakes}} around {{Taipei}} and a {{Seismic Hazard Assessment}} with {{Monte Carlo Simulation}}},
  author = {Wang, J. P. and Wu, Yih-Min and Huang, Duruo},
  year = {2015},
  journal = {Natural Hazards Review},
  volume = {16},
  number = {4},
  pages = {04015003},
  issn = {1527-6988},
  doi = {10.1061/(asce)nh.1527-6996.0000176},
  abstract = {\textcopyright{} 2015 American Society of Civil Engineers. The region around Taiwan is known for high seismicity. Under the circumstances, seismic hazard assessments and proper earthquake-resistant designs are essential to the region. From a well-studied earthquake catalog, this paper presents the statistics of major earthquakes (i.e., moment magnitude Mw{$\geq$}5.5 and distance {$\leq$}200km) around the most important city in Taiwan (i.e., Taipei). The analysis shows that the mean annual rate of the major earthquakes is 2.79 around the city, with mean magnitude and SD equal to Mw 6.12 and 0.68, respectively, and mean source-to-site distance and SD equal to 129 and 39 km, respectively. With the earthquake statistics and local ground motion models, a seismic hazard assessment was conducted with Monte Carlo simulations (MCSs). The analysis shows the mean rate for peak ground acceleration {$\geq$}0.23g (where g is gravitational acceleration) should be around 0.013 per year, indicating that the current design peak ground acceleration in Taipei. From this paper, the current seismic design in Taipei might not be as conservative as expected, and a review and revision could be needed for assuring the city's seismic safety against high seismicity in the region.},
  file = {D\:\\Zotero\\storage\\A7NGSA9L\\Wang, Wu, Huang - 2015 - Major Earthquakes around Taipei and a Seismic Hazard Assessment with Monte Carlo Simulation(2).pdf;D\:\\Zotero\\storage\\BZ2DVC68\\Wang, Wu, Huang - 2015 - Major Earthquakes around Taipei and a Seismic Hazard Assessment with Monte Carlo Simulation(2).pdf}
}

@book{Watson1945,
  title = {A {{Treatise}} on the {{Theory}} of {{Bessel Functions}}},
  author = {Watson, G. N.},
  year = {1945},
  volume = {29},
  abstract = {An Unabridged, Digitally Enlarged Printing, To Include: The Tabulation Of Bessel Functions - Bibliography - Index Of Symbols - List Of Authors Quoted, And A Comprehensive Index},
  file = {D\:\\Zotero\\storage\\NWCLLQGD\\B., Watson - 1945 - A Treatise on the Theory of Bessel Functions.pdf}
}

@book{Weller2012,
  title = {Multiple {{Correspondence Analysis}}},
  author = {Weller, Susan and Romney, A.},
  year = {2012},
  journal = {Metric Scaling},
  doi = {10.4135/9781412985048.n8},
  abstract = {This work deals with the use of multiple correspondence analysis (MCA) and a weighted Euclidean distance (the tolerance distance) as an exploratory tool in developing predictive logistic models. The method was applied to a living-donor kidney transplant data set with 109 cases and 13 predictors. This approach, followed by backward and forward selection procedures, yielded two models, one with four and another with two predictors. These models were compared to two other models, ordinarily built by backward and forward stepwise selection, which yielded, respectively, five and two predictors. After internal validation, the models performance statistics showed similar results. Likelihood ratio tests suggested that backward approach achieved a better fit than the forward modelling in both methods and the Vuong's non-nested test between backward-built models suggested that these were undistinguishable. We conclude that the tolerance distance, in combination with MCA, could be a feasible method for variable selection in logistic modelling, when there are several categorical predictors.},
  isbn = {978-1-4129-6897-3},
  file = {D\:\\Zotero\\storage\\ANR5QMWP\\Weller, Romney - 2012 - Multiple Correspondence Analysis.pdf}
}

@book{Wiggins,
  title = {Interdisciplinary {{Applied Mathematics}}},
  author = {Wiggins, S and Sirovich, L},
  volume = {7},
  isbn = {0-387-97520-9},
  file = {D\:\\Zotero\\storage\\QZW23FMV\\Wiggins, Sirovich - Unknown - Interdisciplinary Applied Mathematics.pdf}
}

@book{Wilks2011,
  title = {Generalized {{Principal Component Analysis}}},
  author = {Wilks, D. S.},
  year = {2011},
  journal = {International Geophysics},
  volume = {100},
  issn = {00746142},
  doi = {10.1016/B978-0-12-385022-5.00012-9},
  abstract = {Principal component analysis is a linear data transformation defined in terms of the eigenvectors of their covariance matrix, which provides maximal data compression. Often useful insights into multivariate data arise from interpretation of these transformations. In meteorology and climatology this method is most often applied to time series of spatial fields, although it is also applicable in other settings. \textcopyright{} 2011 Elsevier Inc.},
  isbn = {978-0-387-87810-2},
  keywords = {Empirical orthogonal functions,PCA truncation,Rotated PCA,Singular spectrum analysis},
  file = {D\:\\Zotero\\storage\\2EVTUNCZ\\Wilks - 2011 - Principal Component (EOF) Analysis.pdf}
}

@book{Witte2017,
  title = {Statistics},
  author = {Witte, Robert S and Witte, John S},
  year = {2017},
  isbn = {978-1-119-25451-5},
  file = {D\:\\Zotero\\storage\\WYGYZUTT\\Witte, Witte - 2017 - Statistics.pdf}
}

@article{Woehlke1994a,
  title = {Introduction to {{Hierarchical Linear Models}}},
  author = {Woehlke, Paula L.},
  year = {1994},
  journal = {Contemporary Psychology: A Journal of Reviews},
  volume = {39},
  number = {5},
  pages = {475--476},
  issn = {00107549},
  doi = {10.1037/034309},
  file = {D\:\\Zotero\\storage\\RMY4LIB5\\Woehlke - 1994 - Introduction to Hierarchical Linear Models.pdf}
}

@book{Wolfenden1991a,
  title = {Experimentation and {{Uncertainty Analysis}} for {{Engineers}}},
  author = {Wolfenden, A and Jacobs, {\relax TL}},
  year = {1991},
  journal = {Journal of Testing and Evaluation},
  volume = {19},
  issn = {00903973},
  doi = {10.1520/jte12616j},
  abstract = {The dramatic developments in the field of experimental uncertainty analysis over the last ten years have led to sweeping changes in applications, resulting in a new international experimental uncertainty standard. Now, in the only manual available with direct applications to the design and analysis of engineering experiments, respected authors Hugh Coleman and Glenn Steele have thoroughly updated their bestselling title to include the new methodologies being used by the United States and international standards committee groups. Along with several new examples, this latest edition includes new material on: * The utilization of Uncertainty Magnification Factors (UMFs) and Uncertainty Percentage Contributions (UPCs) in the planning and early design phases of experiments * Refined procedures for accounting for the effects of correlated bias errors * Improved methods for accounting for the effects of asymmetric systematic uncertainties * The importance of (previously ignored) correlated random errors with an example illustrating how to account for them * Uncertainties in comparative testing * Uncertainties in the comparison of data and predictions (code validation) * Uncertainty analysis by direct Monte Carlo simulation * A new method to determine regression uncertainties that properly accounts for both random and systematic uncertaintiesWith a step-by-step approach, engineering students as well as practicing professional engineers who analyze or design experiments will find Experimentation and Uncertainty Analysis for Engineers, Second Edition to be an invaluable reference tool.},
  isbn = {978-0-470-16888-2},
  file = {D\:\\Zotero\\storage\\8R4QGBR2\\Wolfenden, Jacobs - 1991 - Experimentation and Uncertainty Analysis for Engineers.pdf}
}

@book{wuNonparametricRegressionMethods,
  title = {Nonparametric {{Regression Methods}} for {{Longitudinal Data Analysis}}},
  author = {Wu, Hulin and Zhang, Jin-Ting},
  langid = {english},
  file = {D\:\\Zotero\\storage\\P8RAQ7VA\\Wu and Zhang - Nonparametric Regression Methods for Longitudinal .pdf}
}

@article{Yoe2011b,
  title = {Probability {{Review}}},
  author = {Yoe, Charles and Yoe, Charles},
  year = {2011},
  journal = {Principles of Risk Analysis},
  pages = {303--317},
  doi = {10.1201/b11256-11},
  abstract = {Review of probability. Random variables, distributions, joint and conditional distribution, marginalization, baye's rule, conditional independence, expectation, MLE},
  keywords = {\ding{72}},
  file = {D\:\\Zotero\\storage\\HPQKM54H\\Yoe, Yoe - 2011 - Probability Review.pdf}
}

@article{Zareian2007a,
  title = {A {{Simplified Procedure}} for {{Performance-Based Design}}},
  author = {Zareian, Farzin and Krawinkler, Helmut},
  year = {2007},
  journal = {Journal of the Earthquake Engineering Society of Korea},
  volume = {11},
  number = {4},
  pages = {13--23},
  issn = {1226-525X},
  doi = {10.5000/eesk.2007.11.4.013},
  keywords = {collapse fragility curves,design decision support system,hazard curves,mean loss curves,structural response curves},
  file = {D\:\\Zotero\\storage\\F33B6W3X\\Zareian, Krawinkler - 2007 - A Simplified Procedure for Performance-Based Design.pdf}
}

@article{zeileisExtendedModelFormulas2010,
  title = {Extended {{Model Formulas}} in {{{\emph{R}}}} : {{Multiple Parts}} and {{Multiple Responses}}},
  shorttitle = {Extended {{Model Formulas}} in {{{\emph{R}}}}},
  author = {Zeileis, Achim and Croissant, Yves},
  year = {2010},
  journal = {Journal of Statistical Software},
  volume = {34},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v034.i01},
  urldate = {2023-08-20},
  abstract = {This introduction to the R package Formula is a (slightly) modified version of Zeileis and Croissant (2010), published in the Journal of Statistical Software. Model formulas are the standard approach for specifying the variables in statistical models in the S language. Although being eminently useful in an extremely wide class of applications, they have certain limitations including being confined to single responses and not providing convenient support for processing formulas with multiple parts. The latter is relevant for models with two or more sets of variables, e.g., different equations for different model parameters (such as mean and dispersion), regressors and instruments in instrumental variable regressions, two-part models such as hurdle models, or alternative-specific and individual-specific variables in choice models among many others. The R package Formula addresses these two problems by providing a new class ``Formula'' (inheriting from ``formula'') that accepts an additional formula operator | separating multiple parts and by allowing all formula operators (including the new |) on the left-hand side to support multiple responses.},
  langid = {english},
  file = {D\:\\Zotero\\storage\\ZYUXFMYS\\Zeileis and Croissant - 2010 - Extended Model Formulas in R  Multiple Par.pdf}
}

@book{zhangEnsembleMachineLearning2012,
  title = {Ensemble {{Machine Learning}}: {{Methods}} and {{Applications}}},
  shorttitle = {Ensemble {{Machine Learning}}},
  editor = {Zhang, Cha and Ma, Yunqian},
  year = {2012},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4419-9326-7},
  urldate = {2023-07-23},
  isbn = {978-1-4419-9325-0 978-1-4419-9326-7},
  langid = {english},
  file = {D\:\\Zotero\\storage\\SUY4WYWD\\Zhang and Ma - 2012 - Ensemble Machine Learning Methods and Application.pdf}
}

@article{Zhao2000,
  title = {New Point Estimates for Probability Moments},
  author = {Zhao, Yan Gang and Ono, Tetsuro},
  year = {2000},
  journal = {Journal of Engineering Mechanics},
  volume = {126},
  number = {4},
  pages = {433--436},
  issn = {07339399},
  doi = {10.1061/(ASCE)0733-9399(2000)126:4(433)},
  abstract = {There are many areas of structural safety and structural dynamics in which it is often desirable to compute the first few statistical moments of a function of random variables. The usual approximation is by the Taylor expansion method. This approach requires the computation of derivatives. In order to avoid the computation of derivatives, point estimates for probability moments have been proposed. However, the accuracy is quite low, and sometimes, the estimating points may be outside the region in which the random variable is defined. In the present paper, new point estimates for probability moments are proposed, in which increasing the number of estimating points is easier because the estimating points are independent of the random variable in its original space and the use of high-order moments of the random variables is not required. By using this approximation, the practicability and accuracy of point estimates can be much improved.},
  file = {D\:\\Zotero\\storage\\BDWAMCSL\\Zhao, Ono - 2000 - New point estimates for probability moments.pdf}
}

@book{zhouEnsembleMethodsFoundations2012,
  title = {Ensemble {{Methods}}: {{Foundations}} and {{Algorithms}}},
  shorttitle = {Ensemble {{Methods}}},
  author = {Zhou, Zhi-Hua},
  year = {2012},
  month = jun,
  edition = {0},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/b12207},
  urldate = {2023-08-09},
  isbn = {978-0-429-15109-5},
  langid = {english},
  file = {D\:\\Zotero\\storage\\5CUH9SPS\\Zhou - 2012 - Ensemble Methods Foundations and Algorithms.pdf}
}
